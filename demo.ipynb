{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "from gridworld.env import GridWorldEnv\n",
    "from tictactoe.env import TicTacToeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    (Synchronous) Deep Q-Learning Network agent class\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_actions):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(n_features, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DQN(10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4112,  0.3504, -0.5346,  0.4519, -0.3472], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.forward(torch.Tensor([1,2,3,5,5,6,7,8,9,10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.AsyncVectorEnv([\n",
    "    lambda: TicTacToeEnv(), \n",
    "    lambda: TicTacToeEnv(),\n",
    "    lambda: TicTacToeEnv(),\n",
    "    lambda: TicTacToeEnv(),\n",
    "    lambda: TicTacToeEnv()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# envs = gym.make_vec(\"LunarLander-v2\", num_envs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('deck': Box(-1, 1, (3, 3), int64))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs.single_observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs_shape = envs.single_observation_space.shape[0]\n",
    "action_shape = envs.single_action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('agent',\n",
       "               array([[0, 1],\n",
       "                      [3, 2],\n",
       "                      [2, 1],\n",
       "                      [3, 2],\n",
       "                      [4, 1]])),\n",
       "              ('target',\n",
       "               array([[2, 3],\n",
       "                      [4, 3],\n",
       "                      [4, 1],\n",
       "                      [1, 3],\n",
       "                      [0, 2]]))]),\n",
       " {'distance': array([4., 2., 2., 3., 5.]),\n",
       "  '_distance': array([ True,  True,  True,  True,  True])})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Till Zemann\n",
    "# License: MIT License\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(nn.Module):\n",
    "    \"\"\"\n",
    "    (Synchronous) Advantage Actor-Critic agent class\n",
    "\n",
    "    Args:\n",
    "        n_features: The number of features of the input state.\n",
    "        n_actions: The number of actions the agent can take.\n",
    "        device: The device to run the computations on (running on a GPU might be quicker for larger Neural Nets,\n",
    "                for this code CPU is totally fine).\n",
    "        critic_lr: The learning rate for the critic network (should usually be larger than the actor_lr).\n",
    "        actor_lr: The learning rate for the actor network.\n",
    "        n_envs: The number of environments that run in parallel (on multiple CPUs) to collect experiences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        n_actions: int,\n",
    "        device: torch.device,\n",
    "        critic_lr: float,\n",
    "        actor_lr: float,\n",
    "        n_envs: int,\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes the actor and critic networks and their respective optimizers.\"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_envs = n_envs\n",
    "\n",
    "        critic_layers = [\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),  # estimate V(s)\n",
    "        ]\n",
    "\n",
    "        actor_layers = [\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                32, n_actions\n",
    "            ),  # estimate action logits (will be fed into a softmax later)\n",
    "        ]\n",
    "\n",
    "        # define actor and critic networks\n",
    "        self.critic = nn.Sequential(*critic_layers).to(self.device)\n",
    "        self.actor = nn.Sequential(*actor_layers).to(self.device)\n",
    "\n",
    "        # define optimizers for actor and critic\n",
    "        self.critic_optim = optim.RMSprop(self.critic.parameters(), lr=critic_lr)\n",
    "        self.actor_optim = optim.RMSprop(self.actor.parameters(), lr=actor_lr)\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the networks.\n",
    "\n",
    "        Args:\n",
    "            x: A batched vector of states.\n",
    "\n",
    "        Returns:\n",
    "            state_values: A tensor with the state values, with shape [n_envs,].\n",
    "            action_logits_vec: A tensor with the action logits, with shape [n_envs, n_actions].\n",
    "        \"\"\"\n",
    "        x = torch.Tensor(x).to(self.device)\n",
    "        state_values = self.critic(x)  # shape: [n_envs,]\n",
    "        action_logits_vec = self.actor(x)  # shape: [n_envs, n_actions]\n",
    "        return (state_values, action_logits_vec)\n",
    "\n",
    "    def select_action(\n",
    "        self, x: np.ndarray\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns a tuple of the chosen actions and the log-probs of those actions.\n",
    "\n",
    "        Args:\n",
    "            x: A batched vector of states.\n",
    "\n",
    "        Returns:\n",
    "            actions: A tensor with the actions, with shape [n_steps_per_update, n_envs].\n",
    "            action_log_probs: A tensor with the log-probs of the actions, with shape [n_steps_per_update, n_envs].\n",
    "            state_values: A tensor with the state values, with shape [n_steps_per_update, n_envs].\n",
    "        \"\"\"\n",
    "        state_values, action_logits = self.forward(x)\n",
    "        action_pd = torch.distributions.Categorical(\n",
    "            logits=action_logits\n",
    "        )  # implicitly uses softmax\n",
    "        actions = action_pd.sample()\n",
    "        action_log_probs = action_pd.log_prob(actions)\n",
    "        entropy = action_pd.entropy()\n",
    "        return (actions, action_log_probs, state_values, entropy)\n",
    "\n",
    "    def get_losses(\n",
    "        self,\n",
    "        rewards: torch.Tensor,\n",
    "        action_log_probs: torch.Tensor,\n",
    "        value_preds: torch.Tensor,\n",
    "        entropy: torch.Tensor,\n",
    "        masks: torch.Tensor,\n",
    "        gamma: float,\n",
    "        lam: float,\n",
    "        ent_coef: float,\n",
    "        device: torch.device,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the loss of a minibatch (transitions collected in one sampling phase) for actor and critic\n",
    "        using Generalized Advantage Estimation (GAE) to compute the advantages (https://arxiv.org/abs/1506.02438).\n",
    "\n",
    "        Args:\n",
    "            rewards: A tensor with the rewards for each time step in the episode, with shape [n_steps_per_update, n_envs].\n",
    "            action_log_probs: A tensor with the log-probs of the actions taken at each time step in the episode, with shape [n_steps_per_update, n_envs].\n",
    "            value_preds: A tensor with the state value predictions for each time step in the episode, with shape [n_steps_per_update, n_envs].\n",
    "            masks: A tensor with the masks for each time step in the episode, with shape [n_steps_per_update, n_envs].\n",
    "            gamma: The discount factor.\n",
    "            lam: The GAE hyperparameter. (lam=1 corresponds to Monte-Carlo sampling with high variance and no bias,\n",
    "                                          and lam=0 corresponds to normal TD-Learning that has a low variance but is biased\n",
    "                                          because the estimates are generated by a Neural Net).\n",
    "            device: The device to run the computations on (e.g. CPU or GPU).\n",
    "\n",
    "        Returns:\n",
    "            critic_loss: The critic loss for the minibatch.\n",
    "            actor_loss: The actor loss for the minibatch.\n",
    "        \"\"\"\n",
    "        T = len(rewards)\n",
    "        advantages = torch.zeros(T, self.n_envs, device=device)\n",
    "\n",
    "        # compute the advantages using GAE\n",
    "        gae = 0.0\n",
    "        for t in reversed(range(T - 1)):\n",
    "            td_error = (\n",
    "                rewards[t] + gamma * masks[t] * value_preds[t + 1] - value_preds[t]\n",
    "            )\n",
    "            gae = td_error + gamma * lam * masks[t] * gae\n",
    "            advantages[t] = gae\n",
    "\n",
    "        # calculate the loss of the minibatch for actor and critic\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "\n",
    "        # give a bonus for higher entropy to encourage exploration\n",
    "        actor_loss = (\n",
    "            -(advantages.detach() * action_log_probs).mean() - ent_coef * entropy.mean()\n",
    "        )\n",
    "        return (critic_loss, actor_loss)\n",
    "\n",
    "    def update_parameters(\n",
    "        self, critic_loss: torch.Tensor, actor_loss: torch.Tensor\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Updates the parameters of the actor and critic networks.\n",
    "\n",
    "        Args:\n",
    "            critic_loss: The critic loss.\n",
    "            actor_loss: The actor loss.\n",
    "        \"\"\"\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/vector/__init__.py:53: UserWarning: \u001b[33mWARN: `gymnasium.vector.make(...)` is deprecated and will be replaced by `gymnasium.make_vec(...)` in v1.0\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "envs = gym.vector.make(\"LunarLander-v2\", num_envs=3, max_episode_steps=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.AsyncVectorEnv(\n",
    "    [\n",
    "        lambda: gym.make(\n",
    "            \"LunarLander-v2\",\n",
    "            gravity=-10.0,\n",
    "            enable_wind=True,\n",
    "            wind_power=15.0,\n",
    "            turbulence_power=1.5,\n",
    "            max_episode_steps=600,\n",
    "        ),\n",
    "        lambda: gym.make(\n",
    "            \"LunarLander-v2\",\n",
    "            gravity=-9.8,\n",
    "            enable_wind=True,\n",
    "            wind_power=10.0,\n",
    "            turbulence_power=1.3,\n",
    "            max_episode_steps=600,\n",
    "        ),\n",
    "        lambda: gym.make(\n",
    "            \"LunarLander-v2\", gravity=-7.0, enable_wind=False, max_episode_steps=600\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
