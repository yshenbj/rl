{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "from tictactoe.env import TicTacToeEnv\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    (Synchronous) Deep Q-Learning Network agent class\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_actions, hidden_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(n_features, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.steps_done = 0\n",
    "        \n",
    "    def select_action(self, net, states):\n",
    "        x = torch.Tensor(states).to(self.device)\n",
    "        logits = net.forward(x)\n",
    "        return logits.argmax(1).cpu().numpy(), logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# envs = gym.vector.AsyncVectorEnv([GridWorldEnv for i in range(4)])\n",
    "# envs_wrapper = gym.wrappers.RecordEpisodeStatistics(envs)\n",
    "# envs_wrapper.reset()\n",
    "n_envs = 6\n",
    "n_episodes = 1000\n",
    "n_steps_per_update = 9\n",
    "eps = 0.1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "envs = gym.vector.AsyncVectorEnv([TicTacToeEnv for _ in range(n_envs)])\n",
    "envs_wrapper = gym.wrappers.RecordEpisodeStatistics(envs)\n",
    "n_observations = np.prod(envs.single_observation_space.shape)\n",
    "n_actions = envs.single_action_space.n\n",
    "policy_net = DQN(n_observations, n_actions, 128)\n",
    "policy_net.to(device)\n",
    "# states, info = envs_wrapper.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_pred_q = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "ep_rewards = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "masks = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "\n",
    "agent = Agent(device)\n",
    "states, info = envs_wrapper.reset()\n",
    "for step in range(n_steps_per_update):\n",
    "    actions, logits = agent.select_action(policy_net, states.reshape(n_envs, -1))\n",
    "    # epsilon greedy\n",
    "    actions = (np.random.rand(n_envs) > eps) * actions + (np.random.rand(n_envs) < eps) * envs.action_space.sample() \n",
    "    next_states, rewards, terminated, truncated, infos = envs_wrapper.step(actions)\n",
    "    # state_action_values = logits.gather(1, actions.unsqueeze(-1)).reshape(-1)\n",
    "    # ep_rewards[step] = torch.tensor(rewards, device=device)\n",
    "    # masks[ste                                     `p] = torch.tensor([not term for term in terminated])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, info = envs_wrapper.reset()\n",
    "x = torch.Tensor(states).to(device)\n",
    "logits = policy_net.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcts(net, envs_wrapper, ):\n",
    "    states, info = envs_wrapper.reset()\n",
    "    x = torch.Tensor(states).to(device)\n",
    "    logits = policy_net.forward(x)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    \"\"\" \n",
    "    A tree node in the MCTS. Each node keeps track of its own value Q, prior probability P, \n",
    "    and its visited-count-adjusted prior score u.\n",
    "    TODO: gamma is 1.0 in this case.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parent, prior_p):\n",
    "        self._parent = parent\n",
    "        self._children = {}\n",
    "        self._n_visits = 0 \n",
    "        self._Q = 0\n",
    "        self._P = prior_p \n",
    "        self._u = 0\n",
    "\n",
    "    def expand(self, action_priors):\n",
    "        \"\"\"\n",
    "        Expand tree by creating new children.\n",
    "        action_priors: a list of tuples of actions and their prior probability according to the policy function.\n",
    "        \"\"\"\n",
    "        for action, prob in action_priors:\n",
    "            if action not in self._children:\n",
    "                self._children[action] = TreeNode(prob)\n",
    "\n",
    "    def select(self, c_puct):\n",
    "        \"\"\"\n",
    "        Selection action among children that gives maximum action value Q plus bonus u(P).\n",
    "        \"\"\"\n",
    "        return max(self._children.items(), key=lambda act_node:act_node[1].get_value(c_puct))\n",
    "    \n",
    "    def update(self, G):\n",
    "        \"\"\"\n",
    "        Update node values form Monte-Carlo evaluation with return G.\n",
    "        TODO: alpha is 1.0 in this case.\n",
    "        \"\"\"\n",
    "        self._n_visits += 1\n",
    "        self._Q += 1.0 * (G - self._Q) / self._n_visits\n",
    "\n",
    "    def update_recursive(self, G):\n",
    "        \"\"\"\n",
    "        Update recursively for all ancestors\n",
    "        \"\"\"\n",
    "        if self._parent:\n",
    "            self._parent.update_resursive(-G)\n",
    "        self.update(G)\n",
    "\n",
    "    def get_value(self, c_puct):\n",
    "        \"\"\"\n",
    "        Calculate and return the value for this node.\n",
    "        It is a combination of leaf evaluation Q, and this node's prior adjusted for its visit count u.\n",
    "        c_punt: a number in (0, inf) controlling the relative impact of value Q, and prior probability P, \n",
    "        on this node's score.\n",
    "        \"\"\"\n",
    "        self._u = (c_puct * self._P * np.sqrt(self._parent._n_visits)) / (1 + self._n_visits)\n",
    "        return self._Q + self._u \n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"\n",
    "        Check if leaf node.\n",
    "        \"\"\"\n",
    "        return self._chilfren == {}\n",
    "    \n",
    "    def is_root(self):\n",
    "        \"\"\"\n",
    "        Check if root node.\n",
    "        \"\"\"\n",
    "        return self._parent is None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    \"\"\"A simple implementation of Monte Carlo Tree Search.\"\"\"\n",
    "\n",
    "    def __init__(self, policy_value_fn, c_puct=5, n_playout=10000):\n",
    "        \"\"\"\n",
    "        policy_value_fn: a function that takes in a board state, outputs a list of (action, probability, q) tuples.\n",
    "        c_puct: a number in (0, inf) that controls how quickly exploration converages to the maximum-value policy.\n",
    "        \"\"\"\n",
    "        self._root = TreeNode(parent=None, prior_p=1.0)\n",
    "        self._policy = policy_value_fn\n",
    "        self._c_puct = c_puct \n",
    "        self._n_playout = n_playout \n",
    "\n",
    "    def _playout(self, state):\n",
    "        \"\"\"\n",
    "        Run a single playout from the root to the leaf, getting a value at the leaf and propagating it back through\n",
    "        its partents. State is modified in-place, so a copy must be provided.\n",
    "        \"\"\"\n",
    "        node = self._root\n",
    "        action, node = node.select(self._c_puct)\n",
    "        # state.do_mode(action)\n",
    "\n",
    "        action_probs = self._policy(state)\n",
    "        # end, winner = state.game_end()\n",
    "        # if not end:\n",
    "            # node.expend(action_probs)\n",
    "        leaf_value = self._evaluate_rollout(state)\n",
    "        node.update_recursive(-leaf_value)\n",
    "\n",
    "    def _evaluate_rollout(self, state, limit=1000):\n",
    "        \"\"\"\n",
    "        Use the rollout policy to play until the end of the game, returning 1 if the current player wins, -1 if \n",
    "        the oppnent wins and 0 if it is a tie.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_move(self, state):\n",
    "        \"\"\"\n",
    "        Runs all playout sequentially and returns the most visited action.\n",
    "        state: the current game state.\n",
    "        \"\"\"\n",
    "        for n in range(self._n_playout):\n",
    "            state_copy = copy.deepcopy(state)\n",
    "            self._playoput(state_copy)\n",
    "            self._playout(state_copy)\n",
    "            return max(self._root._children.items(), key=lambda act_node: act_node[1]._n_visits)[0]\n",
    "        \n",
    "    def update_with_move(self, last_move):\n",
    "        \"\"\"\n",
    "        Step forward in the tree, keeping everything we already know about the subtree.\n",
    "        \"\"\"\n",
    "        if last_move in self._root._children:\n",
    "            self._root = self._root._children[last_move]\n",
    "            self._root._parent = None \n",
    "        else:\n",
    "            self._root = TreeNode(None, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSPlayer:\n",
    "    \"\"\"\n",
    "    AI player based on MCTS\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i_episode in range(n_episodes):\n",
    "#     states, info = envs_wrapper.reset()\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "state, info = env.reset()\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max().indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor(env.action_space.sample(), device=device, dtype=torch.long)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(env.action_space.sample(), device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episode = 100\n",
    "# def train(num_episode):\n",
    "for i_episode in range(num_episode):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "state = torch.tensor(state.reshape(-1), dtype=torch.float32, device=device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = torch.tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 1., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 1., 0., 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = policy_net(states).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.argmax(a, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = torch.tensor([[i, j] for i in range(3) for j in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.index_select(actions, 0, idx).cpu().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
