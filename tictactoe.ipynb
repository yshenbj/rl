{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from itertools import starmap\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # common layers\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        # action policy layers\n",
    "        self.act_conv1 = nn.Conv2d(128, 4, kernel_size=1)\n",
    "        self.act_fc1 = nn.Linear(4*3*3, 3*3)\n",
    "        # state value layers\n",
    "        self.val_conv1 = nn.Conv2d(128, 4, kernel_size=1)\n",
    "        self.val_fc1 = nn.Linear(4*3*3, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x_act = F.relu(self.act_conv1(x))\n",
    "        x_act = x_act.view(-1, 4*3*3)\n",
    "        x_act = F.log_softmax(self.act_fc1(x_act))\n",
    "        x_val = F.relu(self.val_conv1(x))\n",
    "        x_val = x_val.view(-1, 4*3*3)\n",
    "        x_val = F.relu(self.val_fc1(x_val))\n",
    "        return x_act, x_val\n",
    "    \n",
    "class PolicyValueNet:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.net = Net().to(self.device)\n",
    "        self.lr = 1e-3\n",
    "        self.c = 1e-4\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr, weight_decay=self.c)\n",
    "    \n",
    "    def policy_value(self, state):\n",
    "        state = torch.as_tensor(state, dtype=torch.float32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            log_action_p, value = self.net(state)\n",
    "        action_p = np.exp(log_action_p.cpu().numpy())\n",
    "        value = value.cpu().numpy()\n",
    "        return action_p, value\n",
    "    \n",
    "    # def policy_value(self, state):\n",
    "    #     state = torch.as_tensor(state, dtype=torch.float32, device=self.device)\n",
    "    #     with torch.no_grad():\n",
    "    #         log_action_p, value = self.net(state)\n",
    "    #     action_p = np.exp(log_action_p.cpu().numpy())\n",
    "    #     value = value.cpu().numpy()\n",
    "    #     return action_p, value        \n",
    "    \n",
    "    def update(self, state, mcts_p, is_winner):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        mcts_p = torch.tensor(mcts_p, dtype=torch.float32, device=self.device)\n",
    "        is_winner = torch.tensor(is_winner, dtype=torch.float32, device=self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        log_action_p, value = self.net(state)\n",
    "        # mse loss for the state value\n",
    "        value_loss = F.mse_loss(value.view(-1), is_winner)\n",
    "        # cross entorpy loss for the search probabilities \n",
    "        policy_loss = torch.mean(torch.sum(mcts_p * log_action_p, -1))\n",
    "        # total loss\n",
    "        loss = value_loss - policy_loss\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return value_loss.item(), -1 * policy_loss.item()\n",
    "            \n",
    "    def save(self, filename):\n",
    "        torch.save(self.state_dict(), filename)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        policy_value_net = cls()\n",
    "        policy_value_net.net.load_state_dict(torch.load(filename))\n",
    "        return policy_value_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# envs = gym.vector.SyncVectorEnv(\n",
    "#     [\n",
    "#     lambda: gym.make('games/TicTacToe') for _ in range(10)\n",
    "#     ]\n",
    "# )\n",
    "net = Net()\n",
    "env =  gym.make('games/TicTacToe')\n",
    "observation, info = env.reset()\n",
    "states = torch.tensor(observation, dtype=torch.float32)\n",
    "action_spaces = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = torch.zeros([10,4,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentNode:\n",
    "    def __init__(self, env, parent, prior, reward=0.0):\n",
    "        self.env = env\n",
    "        self.parent = parent\n",
    "        self.prior = prior\n",
    "        self.reward = reward\n",
    "        self.terminated = False\n",
    "        self.n = 0\n",
    "        self.q = None\n",
    "        self.u = None\n",
    "        self.children = {}\n",
    "        \n",
    "    def select(self):\n",
    "        if not self.terminated:\n",
    "            return self\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    def expand(self):\n",
    "        if not self.terminated:\n",
    "            self.children[action] = []\n",
    "            return self.get_outcome_child(action, reward, next_state)\n",
    "        else:\n",
    "            return self\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_outcome_child(self, action, reward, next_state):\n",
    "        # Find the corresponding state and return if this already exists.\n",
    "        for child in self.children[action]:\n",
    "            if next_state == child.state:\n",
    "                return child\n",
    "        # Create one if it is not occured from this state-action pair previously.\n",
    "        new_child = AgentNode(\n",
    "            self,\n",
    "            next_state,\n",
    "            reward\n",
    "        )\n",
    "        self.children[action] += [new_child]\n",
    "        return child\n",
    "        \n",
    "        \n",
    "    def bandit_select(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (2435132113.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[295], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    def selfplay(self)\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, envs, bandit, qfunction):\n",
    "        self.envs = envs\n",
    "        self.bandit = bandit\n",
    "        self.qfunction = qfunction\n",
    "        \n",
    "    def self_play(self, root_nodes=None):\n",
    "        if root_nodes is None:\n",
    "            root_nodes = self.create_root_nodes()\n",
    "            \n",
    "        non_final_mask = np.full(self.envs.num_envs, True)\n",
    "        while non_final_mask.any():\n",
    "            pass\n",
    "        \n",
    "    def create_root_nodes(self, num_agents):\n",
    "        return [AgentNode(self.bandit, self.qfunction), for _ in range(num_agents)]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
