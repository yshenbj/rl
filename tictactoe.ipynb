{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from itertools import count\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "def is_end(board, mark):\n",
    "    n_rows, n_cols = board.shape()\n",
    "    for row_index in range(n_rows):\n",
    "        row = board[row_index, :]\n",
    "        if (row == mark).all():\n",
    "            return 1, True\n",
    "    for col_index in range(n_rows):\n",
    "        col = board[:, col_index]\n",
    "        if (col == mark).all():\n",
    "            return 1, True    \n",
    "    if (board.diagonal() == mark).all() or (np.fliplr(board).diagonal() == mark).all():\n",
    "        return 1, True\n",
    "    else:\n",
    "        return 0, False\n",
    "\n",
    "class TicTacToeEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "    \n",
    "    def __init__(self, render_mode=None):\n",
    "        self.window_size = 512\n",
    "        # Observation space is a 3 * 3 deck.\n",
    "        self.observation_space = spaces.Box(-1, 1, shape=(3, 3), dtype=np.int8)\n",
    "        self.action_space = spaces.Discrete(9)\n",
    "        self.agent_index_space = spaces.Discrete(2)\n",
    "        self.agent_mark_mapping = {\n",
    "            0: 1,\n",
    "            1: 2\n",
    "        }\n",
    "        self.render_mode = render_mode\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        return self._board\n",
    "    \n",
    "    def _get_info(self):\n",
    "        # Return the index of agent which ready to act.\n",
    "        return {\"agent_index\": self._agent_index}\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        # Initialize the deck with zeros and the agent index.\n",
    "        self._board = np.zeros((3, 3), dtype=np.int8)\n",
    "        # Randomly pick an agent.\n",
    "        self._agent_index = self.agent_index_space.sample()\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "        \n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        self._agent_index += 1\n",
    "        if self._agent_index >= self.agent_space.n:\n",
    "            self._agent_index = 0        \n",
    "\n",
    "        mark = self.agent_mark_mapping[self._agent_index]\n",
    "        move = (action // 3, action % 3)\n",
    "        reward, terminated = 0, False\n",
    "        if self._board[move] == 0:\n",
    "            self._board[move] = mark\n",
    "            reward, terminated = is_end(self._board, mark)\n",
    "        return self._get_obs(), reward, terminated, self.get_info()\n",
    "    \n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            self.window = pygame.display.set_mode(\n",
    "                (self.window_size, self.window_size)\n",
    "            )\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "        canvas = pygame.Surface(\n",
    "            (self.window_size, self.window_size)\n",
    "        )\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = self.window_size / 3\n",
    "        # Draw lines to separate boxes.\n",
    "        pygame.draw.aaline(\n",
    "            canvas, \n",
    "            (255, 255, 255),\n",
    "            (pix_square_size * 1,  pix_square_size * 0),\n",
    "            (pix_square_size * 1,  pix_square_size * 3)\n",
    "        )\n",
    "        pygame.draw.aaline(\n",
    "            canvas, \n",
    "            (255, 255, 255),\n",
    "            (pix_square_size * 2,  pix_square_size * 0),\n",
    "            (pix_square_size * 2,  pix_square_size * 3)\n",
    "        )\n",
    "        pygame.draw.aaline(\n",
    "            canvas, \n",
    "            (255, 255, 255),\n",
    "            (pix_square_size * 0,  pix_square_size * 1),\n",
    "            (pix_square_size * 3,  pix_square_size * 1)\n",
    "        )\n",
    "        pygame.draw.aaline(\n",
    "            canvas, \n",
    "            (255, 255, 255),\n",
    "            (pix_square_size * 0,  pix_square_size * 2),\n",
    "            (pix_square_size * 3,  pix_square_size * 2)\n",
    "        )\n",
    "        # Draw \"X\"s or \"O\"s.\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if self._deck[i, j] == -1:\n",
    "                    pygame.draw.circle(\n",
    "                        canvas,\n",
    "                        (255, 255, 255),\n",
    "                        (pix_square_size * (0.5 + i),  pix_square_size * (0.5 + j)),\n",
    "                        pix_square_size - 5\n",
    "                    )\n",
    "                    pygame.draw.circle(\n",
    "                        canvas,\n",
    "                        (0, 0, 0),\n",
    "                        (pix_square_size * (0.5 + i),  pix_square_size * (0.5 + j)),\n",
    "                        pix_square_size - 10\n",
    "                    )\n",
    "                elif self._deck[i, j] == 1:\n",
    "                    pygame.draw.line(\n",
    "                        canvas, \n",
    "                        (255, 255, 255),\n",
    "                        (pix_square_size * i + 5,  pix_square_size * j + 5),\n",
    "                        (pix_square_size * (i + 1) - 5,  pix_square_size * (j + 1) - 5),\n",
    "                        7\n",
    "                    )\n",
    "                    pygame.draw.line(\n",
    "                        canvas, \n",
    "                        (255, 255, 255),\n",
    "                        (pix_square_size * (i + 1) - 5,  pix_square_size * j + 5),\n",
    "                        (pix_square_size * i + 5,  pix_square_size * (j + 1) - 5),\n",
    "                        7\n",
    "                    )\n",
    "        \n",
    "        if self.render_mode == \"human\":\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surface.pixels3d(canvas), axis=(1, 0, 2))\n",
    "            )\n",
    "    \n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "        \n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def select(self, num_envs, states, action_spaces, qfunction):\n",
    "        mask = np.random.rand(num_envs) < self.epsilon\n",
    "        mask_inv = np.invert(mask)\n",
    "        return action_spaces.sample() * mask + qfunction.get_argmax_q(states) * mask_inv\n",
    "\n",
    "\n",
    "class EpsilonDecreasing:\n",
    "    def __init__(self, epsilon=1.0, alpha=0.99, lower_bound=0.01):\n",
    "        self.epsilon_greedy_bandit = EpsilonGreedy(epsilon)\n",
    "        self.initial_epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.lower_bound = lower_bound\n",
    "\n",
    "    def reset(self):\n",
    "        self.epsilon_greedy_bandit = EpsilonGreedy(self.initial_epsilon)\n",
    "\n",
    "    def select(self, num_envs, states, action_spaces, qfunction):\n",
    "        results = self.epsilon_greedy_bandit.select(num_envs, states, action_spaces, qfunction)\n",
    "        self.epsilon_greedy_bandit.epsilon = max(\n",
    "            self.epsilon_greedy_bandit.epsilon * self.alpha, self.lower_bound\n",
    "            # self.epsilon_greedy_bandit.epsilon * self.alpha ** num_envs, self.lower_bound\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQFunction(nn.Module):\n",
    "    def __init__(self, num_states, num_actions, hidden_dim=32, alpha=1e-4, device=None):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_states, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_actions)\n",
    "        ).to(device)\n",
    "        self.optimiser = optim.AdamW(self.parameters(), lr=alpha, amsgrad=True)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def get_q(self, states, actions):\n",
    "        states_tensor = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
    "        actions_tensor = torch.as_tensor(actions, dtype=torch.int64, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(states_tensor)\n",
    "        q_values = logits.gather(-1, actions_tensor.unsqueeze(-1))\n",
    "        return q_values.squeeze(-1).cpu().numpy()\n",
    "    \n",
    "    def get_max_q(self, states):\n",
    "        states_tensor = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(states_tensor)\n",
    "        return logits.max(-1).values.cpu().numpy()\n",
    "    \n",
    "    def get_argmax_q(self, states):\n",
    "        states_tensor = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(states_tensor)\n",
    "        return logits.max(-1).indices.cpu().numpy()        \n",
    "    \n",
    "    def update(self, states, actions, deltas, non_final_mask):\n",
    "        states_tensor = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
    "        actions_tensor = torch.as_tensor(actions, dtype=torch.int64, device=self.device).unsqueeze(-1)\n",
    "        deltas_tensor = torch.as_tensor(deltas, dtype=torch.float32, device=self.device).unsqueeze(-1)\n",
    "        non_final_mask = torch.tensor(non_final_mask, device=self.device).unsqueeze(-1)\n",
    "        q_values = self.forward(states_tensor).gather(-1, actions_tensor)\n",
    "        loss = F.smooth_l1_loss(\n",
    "            torch.masked_select(q_values, non_final_mask),\n",
    "            torch.masked_select(deltas_tensor, non_final_mask)\n",
    "        )\n",
    "        self.optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_value_(self.parameters(), 100)\n",
    "        self.optimiser.step()\n",
    "    \n",
    "    def save(self, filename):\n",
    "        torch.save(self.state_dict(), filename)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filename, num_states, num_actions, hidden_dim=32, alpha=1e-4):\n",
    "        qfunction = cls(num_states, num_actions, hidden_dim, alpha)\n",
    "        qfunction.load_state_dict(torch.load(filename))\n",
    "        return qfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, envs, bandit, qfunction, gamma=1):\n",
    "        self.envs = envs\n",
    "        self.bandit = bandit\n",
    "        self.qfunction = qfunction\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def execute(self, episodes=2000):\n",
    "        mean_rewards = []\n",
    "        for episode in range(episodes):\n",
    "            # Get initial states and actions.\n",
    "            observations, infos = self.envs.reset()\n",
    "            states = np.array(observations).T\n",
    "            action_spaces = self.envs.action_space\n",
    "            actions = self.bandit.select(self.envs.num_envs, states, action_spaces, self.qfunction)\n",
    "            # Run the episode in batched environments.\n",
    "            episode_rewards = np.zeros(self.envs.num_envs)\n",
    "            non_final_mask = np.full(self.envs.num_envs, True)\n",
    "            steps = np.zeros(self.envs.num_envs)\n",
    "            # If all environment end, break the loop.\n",
    "            while non_final_mask.any():\n",
    "                (next_observations, rewards, terminateds, truncateds, infos) = self.envs.step(actions)             \n",
    "                next_states = np.array(next_observations).T\n",
    "                next_actions = self.bandit.select(self.envs.num_envs, next_states, action_spaces, self.qfunction)\n",
    "                deltas = self.get_deltas(states, actions, rewards, next_states, next_actions, terminateds)\n",
    "                # Update q network by deltas.\n",
    "                self.qfunction.update(states, actions, deltas, non_final_mask)\n",
    "                # Update some veriables.\n",
    "                episode_rewards += rewards * (self.gamma ** steps) * non_final_mask\n",
    "                non_final_mask = np.logical_and(non_final_mask, np.invert(terminateds + truncateds))\n",
    "                steps += 1\n",
    "                states = next_states\n",
    "                actions = next_actions    \n",
    "            mean_rewards.append(episode_rewards.mean())\n",
    "        return mean_rewards\n",
    "    \n",
    "    def get_deltas(self, states, actions, rewards, next_states, next_actions, terminateds):\n",
    "        q_values = self.qfunction.get_q(states, actions)\n",
    "        next_state_values = self.state_value(next_states) * np.invert(terminateds)\n",
    "        delta = rewards + self.gamma * next_state_values - q_values\n",
    "        return delta\n",
    "    \n",
    "    def state_value(self, states):\n",
    "        max_q_value = self.qfunction.get_max_q(states)\n",
    "        return max_q_value\n",
    "\n",
    "\n",
    "def get_ema(rewards, smoothing_factor=0.9):\n",
    "    smoothed_rewards = []\n",
    "    for reward in rewards:\n",
    "        if smoothed_rewards == []:\n",
    "            smoothed_rewards = [reward]\n",
    "        else:\n",
    "            smoothed_rewards += [\n",
    "                smoothed_rewards[-1] * smoothing_factor\n",
    "                + reward * (1 - smoothing_factor)\n",
    "            ]\n",
    "    return smoothed_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
