{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import multiprocessing as mp\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # common layers\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        # action policy layers\n",
    "        self.act_conv1 = nn.Conv2d(128, 4, kernel_size=1)\n",
    "        self.act_fc1 = nn.Linear(4*3*3, 3*3)\n",
    "        # state value layers\n",
    "        self.val_conv1 = nn.Conv2d(128, 4, kernel_size=1)\n",
    "        self.val_fc1 = nn.Linear(4*3*3, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x_act = F.relu(self.act_conv1(x))\n",
    "        x_act = x_act.view(-1, 4*3*3)\n",
    "        x_act = F.softmax(self.act_fc1(x_act), dim=-1)\n",
    "        x_val = F.relu(self.val_conv1(x))\n",
    "        x_val = x_val.view(-1, 4*3*3)\n",
    "        x_val = self.val_fc1(x_val)\n",
    "        return x_act, x_val\n",
    "    \n",
    "class PolicyValueNet:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.net = Net().to(self.device)\n",
    "        self.lr = 1e-3\n",
    "        self.c = 1e-4\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr, weight_decay=self.c)\n",
    "    \n",
    "    def policy_value(self, state):\n",
    "        state = torch.as_tensor(state, dtype=torch.float32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            action_p, value = self.net(state)\n",
    "        action_p = action_p.cpu().numpy()\n",
    "        value = value.cpu().numpy()\n",
    "        return action_p, value\n",
    "    \n",
    "    def update(self, state, mcts_p, reward):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        mcts_p = torch.tensor(mcts_p, dtype=torch.float32, device=self.device)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32, device=self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        action_p, value = self.net(state)\n",
    "        # cross entorpy loss for the search probabilities \n",
    "        policy_loss = torch.mean(torch.sum(mcts_p * torch.log(action_p), -1))\n",
    "        # mse loss for the state value\n",
    "        value_loss = F.mse_loss(value.view(-1), reward)\n",
    "        # total loss\n",
    "        loss = value_loss - policy_loss\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return value_loss.item(), -1 * policy_loss.item()\n",
    "            \n",
    "    def save(self, filename):\n",
    "        torch.save(self.net.state_dict(), filename)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        policy_value_net = cls()\n",
    "        policy_value_net.net.load_state_dict(torch.load(filename))\n",
    "        return policy_value_net\n",
    "\n",
    "\n",
    "class AgentNode:\n",
    "    \"\"\"\n",
    "    Agent node of MCTS\n",
    "    \"\"\"\n",
    "    def __init__(self, parent, action, num_actions, P, N=0, W=0):\n",
    "        \"\"\"\n",
    "        Each state_action pair (s, a) stores a set of statistics, {N(s, a), W(s, a), Q(s, a), P(s, a)},\n",
    "        where N(s, a) is the visit count, W(s, a) is the total action-value, Q(s, a) is the mean action-value,\n",
    "        and P(s, a) is the prior probability of selecting a in s.\n",
    "        \"\"\"\n",
    "        self.parent = parent\n",
    "        self.action = action\n",
    "        self.num_actions = num_actions\n",
    "        self.P = P\n",
    "        self.N = N\n",
    "        self.W = W\n",
    "        self.children = {}\n",
    "        self.child_N = np.zeros(num_actions, dtype=np.float32)\n",
    "        self.child_W = np.zeros(num_actions, dtype=np.float32)\n",
    "        self.child_P = None\n",
    "        self.agent_index = None\n",
    "        self.is_expanded = False\n",
    "\n",
    "    def select(self, c_puct_base, c_puct_init):\n",
    "        c_puct = np.log((1 + self.N + c_puct_base) / c_puct_base) + c_puct_init\n",
    "        Q = self.child_W / np.where(self.child_N > 0, self.child_N, 1)\n",
    "        U = c_puct * self.child_P * np.sqrt(self.N) / (1 + self.child_N)\n",
    "        action = np.argmax(-1 * Q + U)\n",
    "    \n",
    "        if action not in self.children.keys():\n",
    "            self.children[action] = AgentNode(\n",
    "                self,\n",
    "                action,\n",
    "                self.num_actions,\n",
    "                self.child_P[action]\n",
    "            )\n",
    "        return action, self.children[action]\n",
    "    \n",
    "    def expand(self, agent_index, next_P):\n",
    "        self.agent_index = agent_index\n",
    "        self.child_P = next_P\n",
    "        self.is_expanded = True\n",
    "\n",
    "    def back_propagate(self, value):\n",
    "        self.N += 1\n",
    "        self.W += value\n",
    "        if self.parent:\n",
    "            self.parent.child_N[self.action] = self.N\n",
    "            self.parent.child_W[self.action] = self.W\n",
    "            self.parent.back_propagate(-value)\n",
    "            \n",
    "    def as_root(self):\n",
    "        self.parent = None\n",
    "        self.action = None\n",
    "        self.P = 1\n",
    "    \n",
    "    @property\n",
    "    def Q(self):\n",
    "        return self.W / self.N\n",
    "\n",
    "\n",
    "class MCTSPlayer:\n",
    "    def __init__(self, policy_value_net, c_puct_base=5, c_puct_init=0, num_simulations=1000, noise=False, deterministic=False):\n",
    "        self.policy_value_net = policy_value_net\n",
    "        self.c_puct_base = c_puct_base\n",
    "        self.c_puct_init = c_puct_init\n",
    "        self.num_simulations = num_simulations\n",
    "        self.noise = noise\n",
    "        self.deterministic = deterministic\n",
    "        self.rng = np.random.default_rng()\n",
    "    \n",
    "    def to_state(self, observation, info, agent_mark_mapping):\n",
    "        \"\"\" \n",
    "        Transfer environmental observation and information to a state tensor as neural network input.\n",
    "        Board observation will transfer to an N * N * M image stack, each plane represent the board positions \n",
    "        oriented to a certain player (N * N with dummy), current player's plane is on the top.\n",
    "        state: numpy array with shape (2, 3, 3)\n",
    "        \"\"\"\n",
    "        agent_index = info['agent_index']\n",
    "        mark_list = list(agent_mark_mapping.values())\n",
    "        num_agents = len(mark_list)\n",
    "        array_list = []\n",
    "        for i in range(num_agents):\n",
    "            index = (agent_index + i) % num_agents\n",
    "            mark = mark_list[index]\n",
    "            array_list.append(observation == mark)\n",
    "        state = np.stack(array_list, dtype=np.float32)\n",
    "        return state\n",
    "    \n",
    "    def add_dirchleet_noise(self, node, action_space, epsilon=0.25, alpha=0.03):\n",
    "        alphas = np.ones(action_space.n) * alpha\n",
    "        noise = self.rng.dirichlet(alphas)\n",
    "        node.child_P = node.child_P * (1 - epsilon) + noise * epsilon\n",
    "        \n",
    "    def get_mcts_p(self, child_N, temperature=0.1):\n",
    "        child_N = child_N ** (1 / temperature)\n",
    "        return child_N / sum(child_N)\n",
    "    \n",
    "    def mcts(self, env, observation, info, root_node=None):\n",
    "        # Initialize environment and root node.\n",
    "        action_space = env.unwrapped.action_space\n",
    "        agent_mark_mapping = env.unwrapped.agent_mark_mapping\n",
    "        root_state = self.to_state(observation, info, agent_mark_mapping)\n",
    "        prior_p, value = self.policy_value_net.policy_value(root_state)\n",
    "        if not root_node:\n",
    "            root_node = AgentNode(None, None, action_space.n, 1)\n",
    "        root_node.expand(info['agent_index'], prior_p[0])\n",
    "        root_node.back_propagate(value.item())\n",
    "        \n",
    "        # Start mcts simulation.\n",
    "        while root_node.N < self.num_simulations:\n",
    "            sim_env = copy.deepcopy(env)\n",
    "            node = root_node\n",
    "            # Add dirchleet noise.\n",
    "            if self.noise:\n",
    "                self.add_dirchleet_noise(node, action_space)\n",
    "                \n",
    "            while node.is_expanded:\n",
    "                # SELECT\n",
    "                action, node = node.select(self.c_puct_base, self.c_puct_init)\n",
    "                # INTERACT\n",
    "                observation, reward, terminated, truncated, info = sim_env.step(action)               \n",
    "                if terminated or truncated:\n",
    "                    # BACK PROPAGATE (REWARD)\n",
    "                    node.back_propagate(-reward)\n",
    "            # EVALUATE\n",
    "            state = self.to_state(observation, info, agent_mark_mapping)\n",
    "            prior_p, value = self.policy_value_net.policy_value(state)\n",
    "            # EXPAND\n",
    "            node.expand(info['agent_index'], prior_p[0])\n",
    "            # BACK PROPAGATE (VALUE)\n",
    "            node.back_propagate(value.item())\n",
    "        \n",
    "        # Choose best action for root node (deterministic or stochastic).\n",
    "        mcts_p = self.get_mcts_p(root_node.child_N)\n",
    "        if self.deterministic:\n",
    "            action = np.argmax(mcts_p)\n",
    "        else:\n",
    "            action = self.rng.choice(np.arange(action_space.n), p=mcts_p)\n",
    "        \n",
    "        if action in root_node.children.keys():\n",
    "            next_root_node = root_node.children[action]\n",
    "            next_root_node.as_root()\n",
    "        else:\n",
    "            next_root_node = None\n",
    "        \n",
    "        return root_state, action, mcts_p, next_root_node\n",
    "\n",
    "\n",
    "def selfplay(env, policy_value_net):\n",
    "    agent_index_list = []\n",
    "    state_list = []\n",
    "    mcts_p_list = []\n",
    "    \n",
    "    \n",
    "    observation, info = env.reset()\n",
    "    player = MCTSPlayer(policy_value_net, noise=True, deterministic=False)\n",
    "    root_node = None\n",
    "    is_end = False\n",
    "    \n",
    "    while not is_end:\n",
    "        agent_index_list.append(info['agent_index'])\n",
    "        root_state, action, mcts_p, root_node = player.mcts(env, observation, info, root_node)\n",
    "        state_list.append(root_state)\n",
    "        mcts_p_list.append(mcts_p)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        is_end = terminated or truncated\n",
    "\n",
    "    reward_list = [\n",
    "        reward if index == agent_index_list[-1] else -reward for index in agent_index_list\n",
    "    ]\n",
    "    \n",
    "    return np.array(state_list), np.array(mcts_p_list), np.array(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | value loss: 1.008391261100769, policy loss: 2.2225682735443115\n",
      "10 | value loss: 0.9066977500915527, policy loss: 2.1793534755706787\n",
      "20 | value loss: 0.687954306602478, policy loss: 2.198564291000366\n",
      "30 | value loss: 0.12655089795589447, policy loss: 2.172548770904541\n",
      "40 | value loss: 0.07143088430166245, policy loss: 2.1501615047454834\n",
      "50 | value loss: 0.03706717491149902, policy loss: 1.9558643102645874\n",
      "60 | value loss: 0.12368446588516235, policy loss: 2.046569585800171\n",
      "70 | value loss: 0.03864136338233948, policy loss: 1.6512224674224854\n",
      "80 | value loss: 0.009196563623845577, policy loss: 1.288096308708191\n",
      "90 | value loss: 0.01358513068407774, policy loss: 0.8362845778465271\n",
      "100 | value loss: 0.008470283821225166, policy loss: 0.650665819644928\n",
      "110 | value loss: 0.009876101277768612, policy loss: 1.034562110900879\n",
      "120 | value loss: 0.34120598435401917, policy loss: 2.1500678062438965\n",
      "130 | value loss: 0.5082666277885437, policy loss: 1.6898002624511719\n",
      "140 | value loss: 0.03681167960166931, policy loss: 1.3379637002944946\n",
      "150 | value loss: 0.14461970329284668, policy loss: 1.9510499238967896\n",
      "160 | value loss: 0.02394256554543972, policy loss: 1.0539380311965942\n",
      "170 | value loss: 0.008688246831297874, policy loss: 0.8831513524055481\n",
      "180 | value loss: 0.006381544750183821, policy loss: 0.5860479474067688\n",
      "190 | value loss: 0.077473483979702, policy loss: 0.3736971318721771\n",
      "200 | value loss: 0.15905320644378662, policy loss: 0.2541409134864807\n",
      "210 | value loss: 0.03209993243217468, policy loss: 1.9030773639678955\n",
      "220 | value loss: 0.011572733521461487, policy loss: 0.2223598212003708\n",
      "230 | value loss: 0.00431800214573741, policy loss: 0.11543295532464981\n",
      "240 | value loss: 0.0008061722037382424, policy loss: 0.05058136209845543\n",
      "250 | value loss: 0.0007718601264059544, policy loss: 0.023918455466628075\n",
      "260 | value loss: 0.00019674030772875994, policy loss: 0.013347703032195568\n",
      "270 | value loss: 0.00011808368435595185, policy loss: 0.00892491266131401\n",
      "280 | value loss: 2.5436396754230373e-05, policy loss: 0.006617013365030289\n",
      "290 | value loss: 5.633735327137401e-06, policy loss: 0.0052652969025075436\n",
      "300 | value loss: 4.3891500354220625e-06, policy loss: 0.004395884927362204\n",
      "310 | value loss: 6.189787313815032e-07, policy loss: 0.0038315150886774063\n",
      "320 | value loss: 2.479991962900385e-07, policy loss: 0.0033798483200371265\n",
      "330 | value loss: 5.113934093969874e-07, policy loss: 0.002949062269181013\n",
      "340 | value loss: 1.1334128657836118e-07, policy loss: 0.0027739328797906637\n",
      "350 | value loss: 1.724134648384279e-07, policy loss: 0.002335237804800272\n",
      "360 | value loss: 6.23402698352038e-08, policy loss: 0.0021494252141565084\n",
      "370 | value loss: 1.1459524529300325e-07, policy loss: 0.0019257630920037627\n",
      "380 | value loss: 7.592657169652739e-08, policy loss: 0.001762870349921286\n",
      "390 | value loss: 5.7856066604244916e-08, policy loss: 0.0016289538471028209\n",
      "400 | value loss: 3.165071404964692e-08, policy loss: 0.001516752876341343\n",
      "410 | value loss: 5.44237153121685e-08, policy loss: 0.0013991262530907989\n",
      "420 | value loss: 2.378604868624734e-08, policy loss: 0.0013057374162599444\n",
      "430 | value loss: 6.120180984225954e-08, policy loss: 0.0012174000730738044\n",
      "440 | value loss: 1.675706506887309e-08, policy loss: 0.0011520716361701488\n",
      "450 | value loss: 5.962589710861721e-08, policy loss: 0.0010789502412080765\n",
      "460 | value loss: 1.7141070784987278e-08, policy loss: 0.0010145962005481124\n",
      "470 | value loss: 5.311785500339283e-08, policy loss: 0.0009635469759814441\n",
      "480 | value loss: 1.3658492115098397e-08, policy loss: 0.0009154343279078603\n",
      "490 | value loss: 3.105872892206207e-08, policy loss: 0.0008929044124670327\n",
      "500 | value loss: 7.2060863942624565e-09, policy loss: 0.000824827526230365\n",
      "510 | value loss: 3.0432623532306025e-08, policy loss: 0.0007864306098781526\n",
      "520 | value loss: 2.6475794001612485e-08, policy loss: 0.0007680637063458562\n",
      "530 | value loss: 9.052458693759036e-08, policy loss: 0.0007266487227752805\n",
      "540 | value loss: 1.0253381077518497e-07, policy loss: 0.0006991486879996955\n",
      "550 | value loss: 3.067687615043724e-08, policy loss: 0.0006595649756491184\n",
      "560 | value loss: 2.5743664977539993e-08, policy loss: 0.00065106205875054\n",
      "570 | value loss: 4.006610865303628e-08, policy loss: 0.0006010156939737499\n",
      "580 | value loss: 3.666187708972757e-08, policy loss: 0.0005728962714783847\n",
      "590 | value loss: 1.5222136440229406e-08, policy loss: 0.0006951086106710136\n",
      "600 | value loss: 2.573750990109147e-08, policy loss: 0.0005342823569662869\n",
      "610 | value loss: 1.755678447068476e-08, policy loss: 0.0005250468966551125\n",
      "620 | value loss: 2.1945737671558163e-08, policy loss: 0.0004928788403049111\n",
      "630 | value loss: 4.813931653302461e-08, policy loss: 0.0004829753306694329\n",
      "640 | value loss: 5.74835112843175e-08, policy loss: 0.0005212572286836803\n",
      "650 | value loss: 6.904742377855655e-08, policy loss: 0.00045264651998877525\n",
      "660 | value loss: 2.8388484896169075e-09, policy loss: 0.0004963778192177415\n",
      "670 | value loss: 6.228905391481021e-08, policy loss: 0.0004995415802113712\n",
      "680 | value loss: 4.009688936434941e-08, policy loss: 0.00040335278026759624\n",
      "690 | value loss: 3.1226509378257106e-08, policy loss: 0.00042346856207586825\n",
      "700 | value loss: 2.8841260046874595e-08, policy loss: 0.00037139328196644783\n",
      "710 | value loss: 2.0800426270284333e-08, policy loss: 0.00037573528243228793\n",
      "720 | value loss: 2.3239611124381554e-08, policy loss: 0.0003518212179187685\n",
      "730 | value loss: 4.306808065734913e-08, policy loss: 0.0003458350838627666\n",
      "740 | value loss: 3.7777486028289786e-08, policy loss: 0.0003478349244687706\n",
      "750 | value loss: 1.7508677174760123e-08, policy loss: 0.00034035660792142153\n",
      "760 | value loss: 2.2045639980206033e-08, policy loss: 0.0003576275776140392\n",
      "770 | value loss: 1.295300400272481e-08, policy loss: 0.00031580697395838797\n",
      "780 | value loss: 1.0043915033008943e-08, policy loss: 0.00030601766775362194\n",
      "790 | value loss: 6.926950391061837e-08, policy loss: 0.00036196401924826205\n",
      "800 | value loss: 1.076769677865741e-07, policy loss: 0.0002960330748464912\n",
      "810 | value loss: 2.2361041018825745e-08, policy loss: 0.00032066297717392445\n",
      "820 | value loss: 1.4739606868374722e-08, policy loss: 0.0002847135765478015\n",
      "830 | value loss: 2.679358734880566e-09, policy loss: 0.00028229496092535555\n",
      "840 | value loss: 3.1198574834689907e-09, policy loss: 0.00039461333653889596\n",
      "850 | value loss: 1.0674638062369013e-08, policy loss: 0.0002697172749321908\n",
      "860 | value loss: 7.370861254685224e-09, policy loss: 0.0002631973475217819\n",
      "870 | value loss: 5.127477820110471e-08, policy loss: 0.00025991344591602683\n",
      "880 | value loss: 2.2194072357706318e-08, policy loss: 0.0002547145413700491\n",
      "890 | value loss: 2.138243537785911e-08, policy loss: 0.0002551836660131812\n",
      "900 | value loss: 1.602534283051682e-08, policy loss: 0.000255235587246716\n",
      "910 | value loss: 5.2624411495116874e-08, policy loss: 0.00024115382984746248\n",
      "920 | value loss: 1.4354539779048991e-08, policy loss: 0.00031760759884491563\n",
      "930 | value loss: 1.5924973340020188e-08, policy loss: 0.0002433401095913723\n",
      "940 | value loss: 2.561486800445323e-09, policy loss: 0.0003351826744619757\n",
      "950 | value loss: 3.289371974801725e-08, policy loss: 0.00022639858070760965\n",
      "960 | value loss: 1.5577994005866458e-08, policy loss: 0.00022358445858117193\n",
      "970 | value loss: 1.6026033833327347e-08, policy loss: 0.00022049355902709067\n",
      "980 | value loss: 8.913545457289729e-09, policy loss: 0.00022180448286235332\n",
      "990 | value loss: 4.655846552736875e-08, policy loss: 0.00024300898076035082\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('games/TicTacToe', max_episode_steps=9)\n",
    "policy_value_net = PolicyValueNet()\n",
    "num_episodes = 1000\n",
    "for i in range(num_episodes):\n",
    "    policy_value_net.net.eval()\n",
    "    state, mcts_p, reward = selfplay(env, policy_value_net)\n",
    "    policy_value_net.net.train()\n",
    "    value_loss, policy_loss = policy_value_net.update(state, mcts_p, reward)\n",
    "    if i % 10 == 0:\n",
    "        print(f'{i} | value loss: {value_loss}, policy loss: {policy_loss}')\n",
    "policy_value_net.save('weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
