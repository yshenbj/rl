{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "from copy import copy, deepcopy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class env():\n",
    "    \"\"\"\n",
    "    Jack's Car Rental example in leacture 3.\n",
    "    States: Two locations, maximum ofn20 car at each.\n",
    "    Actions: Move up to 5 cars between location overnight.\n",
    "    Reward: $10 for each car rented (must be avaliable).\n",
    "    Transitions: Cars returned and requested randomly.\n",
    "        Poisson distribution, n returns/requests with prob Poisson(lambda)\n",
    "        1st location: average requests = 3, average returns = 3.\n",
    "        2nd loaction: average requests = 4, average returns = 2.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.p = np.zeros((21, 21)) \n",
    "        self.v = np.zeros((21, 21))\n",
    "        self.a = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
    "        self.avg_lambda = {\"loc1\": {\"req\": 3, \"ret\": 3}, \"loc2\": {\"req\": 4, \"ret\": 2}}\n",
    "        self.matrixes = {\"loc1\": None, \"loc2\": None}\n",
    "    \n",
    "    def get_action(self, s1, s2, action):\n",
    "        sign = np.sign(action)\n",
    "        if sign >= 0:\n",
    "            action = np.min([20-s1, s2, action])\n",
    "        else:\n",
    "            action = np.min([s1, 20-s2, np.abs(action)]) * -1\n",
    "        return int(action)\n",
    "\n",
    "\n",
    "    def calculate_transition_matrix_item(self, loc, s_start, s_end):\n",
    "        \"\"\" Calculate P(s=s_start, s=s_end) \"\"\"\n",
    "        p_start_end = 0\n",
    "        reward_start_end = 0\n",
    "        for n_req in range(max(0, s_start - s_end), s_start + 1):\n",
    "             n_ret = s_end - (s_start - n_req)\n",
    "             p_ret = self.avg_lambda[loc][\"ret\"] ** n_ret * np.exp(-self.avg_lambda[loc][\"ret\"]) / math.factorial(n_ret)\n",
    "             p_req = self.avg_lambda[loc][\"req\"] ** n_req * np.exp(-self.avg_lambda[loc][\"req\"]) / math.factorial(n_req)\n",
    "             #print(s_start, \"+\", n_ret, \"-\", n_ret+s_start-s_end, \"=\", s_end, \"prob=\", p_ret*p_req)\n",
    "             p_start_end += p_ret * p_req\n",
    "             reward_start_end += 10 * n_req * p_ret * p_req\n",
    "        return p_start_end, reward_start_end\n",
    "\n",
    "\n",
    "    def calculate_transition_matrix(self, loc):\n",
    "        # P(ss')\n",
    "        p_transition_matrix = np.empty((21, 21)) \n",
    "        # P(ss') * R(s'|s)\n",
    "        reward_transition_matrix = np.empty((21, 21)) \n",
    "\n",
    "        for s_start in range(21):\n",
    "            for s_end in range(21):\n",
    "                p_transition_matrix[s_start, s_end] = self.calculate_transition_matrix_item(loc, s_start, s_end)[0]\n",
    "                reward_transition_matrix[s_start, s_end] = self.calculate_transition_matrix_item(loc, s_start, s_end)[1]\n",
    "\n",
    "        # P(s'|s) = P(ss') / P(s)\n",
    "        p_start = np.sum(p_transition_matrix, axis=1)\n",
    "        p_transition_matrix = p_transition_matrix / p_start.reshape(-1, 1)\n",
    "        # R(s'|s) = R(ss') / p(s)\n",
    "        reward_transition_matrix = reward_transition_matrix / p_start.reshape(-1, 1)\n",
    "        \n",
    "        return p_transition_matrix, reward_transition_matrix\n",
    "\n",
    "\n",
    "    def get_one_step_value(self, s1_start, s2_start, p_transition_matrix_loc1, p_transition_matrix_loc2, v):\n",
    "        # sum(Pss' * v(s') for all s' in S)\n",
    "        one_step_value = np.sum(np.outer(p_transition_matrix_loc1[s1_start], p_transition_matrix_loc2[s2_start]) * v.T)\n",
    "        return one_step_value\n",
    "        \n",
    "        \n",
    "    def iteration(self, n_iter):\n",
    "        p_transition_matrix_loc1, reward_transition_matrix_loc1 = self.calculate_transition_matrix(\"loc1\")\n",
    "        p_transition_matrix_loc2, reward_transition_matrix_loc2 = self.calculate_transition_matrix(\"loc2\")\n",
    "\n",
    "        \n",
    "        for _ in range(n_iter):\n",
    "            # # Policy evaluation\n",
    "            # tmp_v = copy(self.v)\n",
    "            # for i in range(21):\n",
    "            #     for j in range(21):\n",
    "            #         action = self.get_action(i, j, self.p[i, j])\n",
    "            #         s1_start, s2_start = i + action, j - action\n",
    "            #         reward = np.sum(reward_transition_matrix_loc1[s1_start]) + np.sum(reward_transition_matrix_loc2[s2_start])\n",
    "            #         one_step_value = self.get_one_step_value(s1_start, s2_start, p_transition_matrix_loc1, p_transition_matrix_loc2, tmp_v)\n",
    "            #         self.v[i, j] = reward + one_step_value\n",
    "\n",
    "            # Policy Improvement\n",
    "            v = copy(self.v)\n",
    "            q =  np.empty((21, 21, 11))\n",
    "            for i in range(21):\n",
    "                for j in range(21):\n",
    "                    # Act by policy\n",
    "                    action_true = [0] * 11\n",
    "                    for k in range(11):\n",
    "                        action = self.a[k] \n",
    "                        action_true[k] = self.get_action(i, j, action)\n",
    "                        s1_start, s2_start = i + action_true[k], j - action_true[k] \n",
    "                        reward = np.sum(reward_transition_matrix_loc1[s1_start]) + np.sum(reward_transition_matrix_loc2[s2_start])\n",
    "                        one_step_value = self.get_one_step_value(s1_start, s2_start, p_transition_matrix_loc1, p_transition_matrix_loc2, v)\n",
    "                        q[i, j, k] = reward + one_step_value\n",
    "                    # Improve the policy by acting greedily.\n",
    "                    self.p[i, j] = action_true[np.argmax(q[i, j])]\n",
    "                    # print(\"greedy: \", q[i, j], np.argmax(q[i, j]), action_true[np.argmax(q[i, j])])\n",
    "                    # Improve the value from any state over one greedy step.\n",
    "                    self.v[i, j] = np.max(q[i, j])\n",
    "\n",
    "        return self.p, self.v\n",
    "\n",
    "\n",
    "def main():\n",
    "    e = env()\n",
    "    p, v = e.iteration(1)\n",
    "    p = p*-1\n",
    "    print(p)\n",
    "    print(v)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = env()\n",
    "p, v = e.iteration(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.heatmap(p, linewidth=0.5, annot=False).invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    e = env()\n",
    "    p, v = e.iteration(i)\n",
    "    ax = sn.heatmap(v, linewidth=0.5, annot=False).invert_yaxis()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
