{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from itertools import count\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv([\n",
    "    lambda: gym.make('games/Blackjack'),\n",
    "    lambda: gym.make('games/Blackjack'),\n",
    "    lambda: gym.make('games/Blackjack'),\n",
    "    lambda: gym.make('games/Blackjack'),\n",
    "    lambda: gym.make('games/Blackjack'),\n",
    "    lambda: gym.make('games/Blackjack'),\n",
    "    lambda: gym.make('games/Blackjack'),\n",
    "    lambda: gym.make('games/Blackjack'),\n",
    "    lambda: gym.make('games/Blackjack'),\n",
    "    lambda: gym.make('games/Blackjack')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations, infos = envs.reset()\n",
    "action_spaces = envs.action_space\n",
    "states = np.array(observations).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "    def select(self, num_envs, states, action_spaces, qfunction):\n",
    "        mask = np.random.rand(num_envs) < self.epsilon\n",
    "        mask_inv = np.invert(mask)\n",
    "        return action_spaces.sample() * mask + qfunction.get_argmax_q(states, action_spaces) * mask_inv\n",
    "\n",
    "\n",
    "class EpsilonDecreasing:\n",
    "    def __init__(self, epsilon=1.0, alpha=0.999, lower_bound=0.1):\n",
    "        self.epsilon_greedy_bandit = EpsilonGreedy(epsilon)\n",
    "        self.initial_epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.lower_bound = lower_bound\n",
    "\n",
    "    def reset(self):\n",
    "        self.epsilon_greedy_bandit = EpsilonGreedy(self.initial_epsilon)\n",
    "\n",
    "    def select(self, num_envs, states, action_spaces, qfunction):\n",
    "        results = self.epsilon_greedy_bandit.select(num_envs, states, action_spaces, qfunction)\n",
    "        self.epsilon_greedy_bandit.epsilon = max(\n",
    "            self.epsilon_greedy_bandit.epsilon * self.alpha ** num_envs, self.lower_bound\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQFunction(nn.Module):\n",
    "    def __init__(self, state_space, action_space, hidden_dim=32, alpha=1e-4):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(state_space, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "    def get_q(self, states, actions):\n",
    "        states_tensor = torch.as_tensor(states, dtype=torch.float32)\n",
    "        actions_tensor = torch.as_tensor(actions, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.forward(states_tensor).gather(1, actions_tensor.unsqueeze(1))\n",
    "        return q_values.squeeze(1).tolist()\n",
    "    \n",
    "    def get_max(self, states):\n",
    "        state_tensor = torch.as_tensor(states, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            max_q_values = self.forward(state_tensor).max(1).values\n",
    "        return max_q_values.tolist()\n",
    "    \n",
    "    def get_argmax_q(self, states, action_spaces):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # def get_max_pair(self, state, actions):\n",
    "    #     state_tensor = torch.as_tensor(state, dtype=torch.float32)\n",
    "    #     with torch.on_grad():\n",
    "    #         q_values = self.forward(state_tensor)\n",
    "    #     arg_max_q, max_q = None, float('-inf')\n",
    "    #     for action in actions:\n",
    "    #         q_value = q_values[action].item()\n",
    "    #         if max_q < q_value:\n",
    "    #             arg_max_q = action\n",
    "    #             max_q = q_value\n",
    "    #     return (arg_max_q, max_q)\n",
    "    \n",
    "    def batch_update(self, states, actions, deltas):\n",
    "        states_tensor = torch.as_tensor(states, dtype=torch.float32)\n",
    "        actions_tensor = torch.as_tensor(actions, dtype=torch.long)\n",
    "        deltas_tensor = torch.as_tensor(deltas, dtype=torch.float32)\n",
    "        q_values = self.forward(states_tensor).gather(1, actions_tensor.unsqueeze(1))\n",
    "        loss = nn.functional.smooth_l1_loss(\n",
    "            q_values,\n",
    "            \n",
    "        )\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = DeepQFunction(state_space=3, action_space=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = torch.as_tensor(states, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = f.forward(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3495,  0.9334],\n",
       "        [-1.5174,  0.9594],\n",
       "        [-1.6958,  1.1379],\n",
       "        [-1.6173,  0.7624],\n",
       "        [-1.8507,  0.9054],\n",
       "        [-1.3644,  0.5227],\n",
       "        [-1.6529,  1.0423],\n",
       "        [-2.1724,  1.2958],\n",
       "        [-1.7010,  0.7857],\n",
       "        [-2.0722,  1.2161]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, envs, bandit, qfunction, gamma=0.9):\n",
    "        self.envs = envs\n",
    "        self.bandit = bandit\n",
    "        self.qfunction = qfunction\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def execute(self, episodes=2000):\n",
    "        rewards = []\n",
    "        for episode in range(episodes):\n",
    "            observations, infos = self.envs.reset()\n",
    "            states = observations\n",
    "            action_spaces = self.envs.action_space\n",
    "            actions = self.bandit.select(states, action_spaces, self.qfunction)\n",
    "            episode_rewards = [0.0] \n",
    "            for step in count():\n",
    "                (next_observation, reward, terminated, truncated, info) = self.envs.step(action)             \n",
    "                next_state = tuple(\n",
    "                    np.concatenate(\n",
    "                        [next_observation['agent'], next_observation['target']]\n",
    "                        )\n",
    "                    )\n",
    "                next_action = self.bandit.select(next_state, actions, self.qfunction)\n",
    "                delta = self.get_delta(state, action, reward, next_state, next_action)\n",
    "                self.qfunction.update(state, action, delta)\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                episode_reward += reward * (self.gamma ** step)\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "            rewards.append(episode_reward)\n",
    "        return rewards\n",
    "    \n",
    "    def get_delta(self, states, actions, rewards, next_states, next_actions):\n",
    "        # q_value = self.qfunction.get_q(state, action)\n",
    "        # next_state_value = self.state_value(next_state, next_action)\n",
    "        # delta = reward + self.gamma * next_state_value - q_value\n",
    "        # return delta\n",
    "        pass\n",
    "    \n",
    "    def state_value(self, state, action):\n",
    "        # actions = self.env.action_space\n",
    "        # max_q_value = self.qfunction.get_max_q(state, actions)\n",
    "        # return max_q_value\n",
    "        pass\n",
    "\n",
    "\n",
    "def get_ema(rewards, smoothing_factor=0.9):\n",
    "    smoothed_rewards = []\n",
    "    for reward in rewards:\n",
    "        if smoothed_rewards == []:\n",
    "            smoothed_rewards = [reward]\n",
    "        else:\n",
    "            smoothed_rewards += [\n",
    "                smoothed_rewards[-1] * smoothing_factor\n",
    "                + reward * (1 - smoothing_factor)\n",
    "            ]\n",
    "    return smoothed_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridworld = GridWorld()\n",
    "# qfunction = DeepQFunction(state_space=len(gridworld.get_initial_state()), action_space=5)\n",
    "# rewards = QLearning(gridworld, EpsilonGreedy(), qfunction).execute(episodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
