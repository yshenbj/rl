{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from itertools import starmap\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def select(self, num_envs, states, action_spaces, qfunction):\n",
    "        mask = np.random.rand(num_envs) < self.epsilon\n",
    "        mask_inv = np.invert(mask)\n",
    "        return action_spaces.sample() * mask + qfunction.get_argmax_q(states) * mask_inv\n",
    "\n",
    "\n",
    "class EpsilonDecreasing:\n",
    "    def __init__(self, epsilon=1.0, alpha=0.99, lower_bound=0.01):\n",
    "        self.epsilon_greedy_bandit = EpsilonGreedy(epsilon)\n",
    "        self.initial_epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.lower_bound = lower_bound\n",
    "\n",
    "    def reset(self):\n",
    "        self.epsilon_greedy_bandit = EpsilonGreedy(self.initial_epsilon)\n",
    "\n",
    "    def select(self, num_envs, states, action_spaces, qfunction):\n",
    "        results = self.epsilon_greedy_bandit.select(num_envs, states, action_spaces, qfunction)\n",
    "        self.epsilon_greedy_bandit.epsilon = max(\n",
    "            self.epsilon_greedy_bandit.epsilon * self.alpha, self.lower_bound\n",
    "            # self.epsilon_greedy_bandit.epsilon * self.alpha ** num_envs, self.lower_bound\n",
    "        )\n",
    "        return results\n",
    "\n",
    "class UpperConfidenceBounds:\n",
    "    def __init__(self, num_envs):\n",
    "        self.num_envs = num_envs\n",
    "        self.total_list = [0] * num_envs\n",
    "        self.times_selected_list = [{} for _ in range(num_envs)]\n",
    "        \n",
    "    def singel_select(self, env_index, action_space, all_q_value):\n",
    "        for action in range(action_space.start, action_space.n):\n",
    "            if action not in self.times_selected_list[env_index].keys():\n",
    "                self.times_selected_list[env_index][action] = 1\n",
    "                self.total_list[env_index] += 1\n",
    "                return action\n",
    "        max_actions = []\n",
    "        max_value = float('-inf')\n",
    "        for action in range(action_space.start, action_space.n):\n",
    "            value = all_q_value[action] + math.sqrt(\n",
    "                2 * math.log(self.total_list[env_index]) / self.times_selected_list[env_index][action]\n",
    "            )\n",
    "            if value > max_value:\n",
    "                max_actions = [action]\n",
    "                max_value = value\n",
    "            elif value == max_value:\n",
    "                max_actions += [action]\n",
    "        result = random.choice(max_actions)\n",
    "        self.times_selected_list[env_index][result] += 1\n",
    "        self.total_list[env_index] += 1\n",
    "        return result \n",
    "\n",
    "    def select(self, _, states, action_spaces, qfunction):\n",
    "        all_q_values = qfunction.get_all_q(states)\n",
    "        actions = list(starmap(self.singel_select, zip(range(self.num_envs), action_spaces, all_q_values)))\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN (CPU version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQFunction(nn.Module):\n",
    "    def __init__(self, num_states, num_actions, hidden_dim=32, alpha=1e-4):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(num_states, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, num_actions)\n",
    "        self.optimiser = optim.AdamW(self.parameters(), lr=alpha, amsgrad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "    def get_q(self, states, actions):\n",
    "        states_tensor = torch.as_tensor(states, dtype=torch.float32)\n",
    "        actions_tensor = torch.as_tensor(actions, dtype=torch.int64)\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(states_tensor)\n",
    "        q_values = logits.gather(-1, actions_tensor.unsqueeze(-1))\n",
    "        return q_values.squeeze(-1).numpy()\n",
    "\n",
    "    def get_all_q(self, states):\n",
    "        states_tensor = torch.as_tensor(states, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(states_tensor)\n",
    "        return logits.cpu().numpy()\n",
    "    \n",
    "    def get_max_q(self, states):\n",
    "        states_tensor = torch.as_tensor(states, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(states_tensor)\n",
    "        return logits.max(-1).values.numpy()\n",
    "    \n",
    "    def get_argmax_q(self, states):\n",
    "        states_tensor = torch.as_tensor(states, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(states_tensor)\n",
    "        return logits.max(-1).indices.numpy()        \n",
    "    \n",
    "    def update(self, states, actions, deltas, non_final_mask):\n",
    "        states_tensor = torch.as_tensor(states, dtype=torch.float32)\n",
    "        actions_tensor = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
    "        deltas_tensor = torch.as_tensor(deltas, dtype=torch.float32).unsqueeze(-1)\n",
    "        non_final_mask = torch.tensor(non_final_mask).unsqueeze(-1)\n",
    "        q_values = self.forward(states_tensor).gather(-1, actions_tensor)\n",
    "        loss = F.smooth_l1_loss(\n",
    "            torch.masked_select(q_values, non_final_mask),\n",
    "            torch.masked_select(deltas_tensor, non_final_mask)\n",
    "        )\n",
    "        self.optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_value_(self.parameters(), 100)\n",
    "        self.optimiser.step()\n",
    "    \n",
    "    def save(self, filename):\n",
    "        torch.save(self.state_dict(), filename)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filename, num_states, num_actions, hidden_dim=32, alpha=1e-4):\n",
    "        qfunction = cls(num_states, num_actions, hidden_dim, alpha)\n",
    "        qfunction.load_state_dict(torch.load(filename))\n",
    "        return qfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, envs, bandit, qfunction, gamma=1):\n",
    "        self.envs = envs\n",
    "        self.bandit = bandit\n",
    "        self.qfunction = qfunction\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def execute(self, episodes=2000):\n",
    "        mean_rewards = []\n",
    "        for episode in range(episodes):\n",
    "            # Get initial states and actions.\n",
    "            observations, infos = self.envs.reset()\n",
    "            states = np.array(observations).T\n",
    "            action_spaces = self.envs.action_space\n",
    "            actions = self.bandit.select(self.envs.num_envs, states, action_spaces, self.qfunction)\n",
    "            # Run the episode in batched environments.\n",
    "            episode_rewards = np.zeros(self.envs.num_envs)\n",
    "            non_final_mask = np.full(self.envs.num_envs, True)\n",
    "            steps = np.zeros(self.envs.num_envs)\n",
    "            # If all environment end, break the loop.\n",
    "            while non_final_mask.any():\n",
    "                (next_observations, rewards, terminateds, truncateds, infos) = self.envs.step(actions)             \n",
    "                next_states = np.array(next_observations).T\n",
    "                next_actions = self.bandit.select(self.envs.num_envs, next_states, action_spaces, self.qfunction)\n",
    "                deltas = self.get_deltas(states, actions, rewards, next_states, next_actions, terminateds)\n",
    "                # Update q network by deltas.\n",
    "                self.qfunction.update(states, actions, deltas, non_final_mask)\n",
    "                # Update some veriables.\n",
    "                episode_rewards += rewards * (self.gamma ** steps) * non_final_mask\n",
    "                non_final_mask = np.logical_and(non_final_mask, np.invert(terminateds + truncateds))\n",
    "                steps += 1\n",
    "                states = next_states\n",
    "                actions = next_actions    \n",
    "            mean_rewards.append(episode_rewards.mean())\n",
    "        return mean_rewards\n",
    "    \n",
    "    def get_deltas(self, states, actions, rewards, next_states, next_actions, terminateds):\n",
    "        q_values = self.qfunction.get_q(states, actions)\n",
    "        next_state_values = self.state_value(next_states) * np.invert(terminateds)\n",
    "        delta = rewards + self.gamma * next_state_values - q_values\n",
    "        return delta\n",
    "    \n",
    "    def state_value(self, states):\n",
    "        max_q_value = self.qfunction.get_max_q(states)\n",
    "        return max_q_value\n",
    "\n",
    "\n",
    "def get_ema(rewards, smoothing_factor=0.9):\n",
    "    smoothed_rewards = []\n",
    "    for reward in rewards:\n",
    "        if smoothed_rewards == []:\n",
    "            smoothed_rewards = [reward]\n",
    "        else:\n",
    "            smoothed_rewards += [\n",
    "                smoothed_rewards[-1] * smoothing_factor\n",
    "                + reward * (1 - smoothing_factor)\n",
    "            ]\n",
    "    return smoothed_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [\n",
    "    lambda: gym.make('games/Blackjack') for _ in range(1000)\n",
    "    ]\n",
    ")\n",
    "qfunction = DeepQFunction(num_states=3, num_actions=2)\n",
    "rewards = QLearning(envs, EpsilonDecreasing(), qfunction).execute(episodes=2000)\n",
    "smoothed_rewards = get_ema(rewards)\n",
    "# qfunction.save(filename='weights_32.pth')\n",
    "\n",
    "plt.plot(rewards, label='Rewards')\n",
    "plt.plot(smoothed_rewards, label='Smoothed Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.title('DQN + Epsilon-Decreasing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [\n",
    "    lambda: gym.make('games/Blackjack') for _ in range(1000)\n",
    "    ]\n",
    ")\n",
    "qfunction = DeepQFunction(num_states=3, num_actions=2)\n",
    "rewards = QLearning(envs, UpperConfidenceBounds(1000), qfunction).execute(episodes=2000)\n",
    "smoothed_rewards = get_ema(rewards)\n",
    "# qfunction.save(filename='weights_32.pth')\n",
    "\n",
    "plt.plot(rewards, label='Rewards')\n",
    "plt.plot(smoothed_rewards, label='Smoothed Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.title('DQN + UCB1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_policy(envs, qfunction):\n",
    "    observations, infos = envs.reset()\n",
    "    states = np.array(observations).T\n",
    "    actions = qfunction.get_argmax_q(states)\n",
    "    non_final_mask = np.full(envs.num_envs, True)\n",
    "    results = np.zeros(envs.num_envs)\n",
    "    while non_final_mask.any():\n",
    "        (observations, rewards, terminateds, truncateds, infos) = envs.step(actions)\n",
    "        states = np.array(observations).T\n",
    "        actions = qfunction.get_argmax_q(states)\n",
    "        results += rewards * non_final_mask\n",
    "        non_final_mask = np.logical_and(non_final_mask, np.invert(terminateds + truncateds))\n",
    "        \n",
    "    rewards, counts = np.unique(results, return_counts=True)\n",
    "    for i in range(len(rewards)):\n",
    "        print(f'P({rewards[i]}) = {counts[i] / envs.num_envs}')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [\n",
    "    lambda: gym.make('games/Blackjack') for _ in range(50000)\n",
    "    ]\n",
    ")\n",
    "qfunction32 = DeepQFunction.load('weights_32.pth', num_states=3, num_actions=2)\n",
    "# qfunction64 = DeepQFunction.load('weights_64.pth', num_states=3, num_actions=2, hidden_dim=64)\n",
    "# qfunction128 = DeepQFunction.load('weights_128.pth', num_states=3, num_actions=2, hidden_dim=128)\n",
    "print('dimention=32:')\n",
    "res1 = execute_policy(envs, qfunction32)\n",
    "# print('dimention=64:')\n",
    "# res2 = execute_policy(envs, qfunction64)\n",
    "# print('dimention=128:')\n",
    "# res3 = execute_policy(envs, qfunction128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN (GPU version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQFunction(nn.Module):\n",
    "    def __init__(self, num_states, num_actions, hidden_dim=32, alpha=1e-4, device=None):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_states, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_actions)\n",
    "        ).to(device)\n",
    "        self.optimiser = optim.AdamW(self.parameters(), lr=alpha, amsgrad=True)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def get_q(self, states, actions):\n",
    "        states_tensor = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
    "        actions_tensor = torch.as_tensor(actions, dtype=torch.int64, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(states_tensor)\n",
    "        q_values = logits.gather(-1, actions_tensor.unsqueeze(-1))\n",
    "        return q_values.squeeze(-1).cpu().numpy()\n",
    "    \n",
    "    def get_max_q(self, states):\n",
    "        states_tensor = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(states_tensor)\n",
    "        return logits.max(-1).values.cpu().numpy()\n",
    "    \n",
    "    def get_argmax_q(self, states):\n",
    "        states_tensor = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(states_tensor)\n",
    "        return logits.max(-1).indices.cpu().numpy()        \n",
    "    \n",
    "    def update(self, states, actions, deltas, non_final_mask):\n",
    "        states_tensor = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
    "        actions_tensor = torch.as_tensor(actions, dtype=torch.int64, device=self.device).unsqueeze(-1)\n",
    "        deltas_tensor = torch.as_tensor(deltas, dtype=torch.float32, device=self.device).unsqueeze(-1)\n",
    "        non_final_mask = torch.tensor(non_final_mask, device=self.device).unsqueeze(-1)\n",
    "        q_values = self.forward(states_tensor).gather(-1, actions_tensor)\n",
    "        loss = F.smooth_l1_loss(\n",
    "            torch.masked_select(q_values, non_final_mask),\n",
    "            torch.masked_select(deltas_tensor, non_final_mask)\n",
    "        )\n",
    "        self.optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_value_(self.parameters(), 100)\n",
    "        self.optimiser.step()\n",
    "    \n",
    "    def save(self, filename):\n",
    "        torch.save(self.state_dict(), filename)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filename, num_states, num_actions, hidden_dim=32, alpha=1e-4):\n",
    "        qfunction = cls(num_states, num_actions, hidden_dim, alpha)\n",
    "        qfunction.load_state_dict(torch.load(filename))\n",
    "        return qfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, envs, bandit, qfunction, gamma=1):\n",
    "        self.envs = envs\n",
    "        self.bandit = bandit\n",
    "        self.qfunction = qfunction\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def execute(self, episodes=2000):\n",
    "        mean_rewards = []\n",
    "        for episode in range(episodes):\n",
    "            # Get initial states and actions.\n",
    "            observations, infos = self.envs.reset()\n",
    "            states = np.array(observations).T\n",
    "            action_spaces = self.envs.action_space\n",
    "            actions = self.bandit.select(self.envs.num_envs, states, action_spaces, self.qfunction)\n",
    "            # Run the episode in batched environments.\n",
    "            episode_rewards = np.zeros(self.envs.num_envs)\n",
    "            non_final_mask = np.full(self.envs.num_envs, True)\n",
    "            steps = np.zeros(self.envs.num_envs)\n",
    "            # If all environment end, break the loop.\n",
    "            while non_final_mask.any():\n",
    "                (next_observations, rewards, terminateds, truncateds, infos) = self.envs.step(actions)             \n",
    "                next_states = np.array(next_observations).T\n",
    "                next_actions = self.bandit.select(self.envs.num_envs, next_states, action_spaces, self.qfunction)\n",
    "                deltas = self.get_deltas(states, actions, rewards, next_states, next_actions, terminateds)\n",
    "                # Update q network by deltas.\n",
    "                self.qfunction.update(states, actions, deltas, non_final_mask)\n",
    "                # Update some veriables.\n",
    "                episode_rewards += rewards * (self.gamma ** steps) * non_final_mask\n",
    "                non_final_mask = np.logical_and(non_final_mask, np.invert(terminateds + truncateds))\n",
    "                steps += 1\n",
    "                states = next_states\n",
    "                actions = next_actions    \n",
    "            mean_rewards.append(episode_rewards.mean())\n",
    "        return mean_rewards\n",
    "    \n",
    "    def get_deltas(self, states, actions, rewards, next_states, next_actions, terminateds):\n",
    "        q_values = self.qfunction.get_q(states, actions)\n",
    "        next_state_values = self.state_value(next_states) * np.invert(terminateds)\n",
    "        delta = rewards + self.gamma * next_state_values - q_values\n",
    "        return delta\n",
    "    \n",
    "    def state_value(self, states):\n",
    "        max_q_value = self.qfunction.get_max_q(states)\n",
    "        return max_q_value\n",
    "\n",
    "\n",
    "def get_ema(rewards, smoothing_factor=0.9):\n",
    "    smoothed_rewards = []\n",
    "    for reward in rewards:\n",
    "        if smoothed_rewards == []:\n",
    "            smoothed_rewards = [reward]\n",
    "        else:\n",
    "            smoothed_rewards += [\n",
    "                smoothed_rewards[-1] * smoothing_factor\n",
    "                + reward * (1 - smoothing_factor)\n",
    "            ]\n",
    "    return smoothed_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [\n",
    "    lambda: gym.make('games/Blackjack') for _ in range(1000)\n",
    "    ]\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "qfunction = DeepQFunction(num_states=3, num_actions=2, device=device)\n",
    "rewards = QLearning(envs, EpsilonDecreasing(), qfunction).execute(episodes=2000)\n",
    "smoothed_rewards = get_ema(rewards)\n",
    "qfunction.save(filename='weights_32.pth')\n",
    "\n",
    "plt.plot(rewards, label='Rewards')\n",
    "plt.plot(smoothed_rewards, label='Smoothed Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.title('DQN + Epsilon-Decreasing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
