{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from itertools import count\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('games/Blackjack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpperConfidenceBounds:\n",
    "    def __init__(self):\n",
    "        # total number of selections\n",
    "        self.total = 0\n",
    "        # number of times each action has been chosen\n",
    "        self.times_selected = {}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.total = 0\n",
    "        self.times_selected = {}\n",
    "    \n",
    "    def select(self, state, actions, qfunction):\n",
    "        # First execute each action on time\n",
    "        for action in range(actions.start, actions.n):\n",
    "            if action not in self.times_selected.keys():\n",
    "                self.times_selected[action] = 1\n",
    "                self.total += 1\n",
    "                return action\n",
    "\n",
    "        max_actions = []\n",
    "        max_value = float('-inf')\n",
    "        for action in range(actions.start, actions.n):\n",
    "            value = qfunction.get_q_value(state, action) + math.sqrt(\n",
    "                (2 * math.log(self.total)) / self.times_selected[action]\n",
    "            )\n",
    "            if value > max_value:\n",
    "                max_actions = [action]\n",
    "                max_value = value\n",
    "            elif value == max_value:\n",
    "                max_actions += [action]\n",
    "        \n",
    "        # For multiple actions with highest value, choose one randomly\n",
    "        result = random.choice(max_actions)\n",
    "        self.times_selected[result] += 1\n",
    "        self.total += 1\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQFunction(nn.Module):\n",
    "    def __init__(self, state_space, action_space, hidden_dim=32, alpha=1e-4):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(state_space, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "    def get_q_values(self, states, actions):\n",
    "        states_tensor = torch.as_tensor(states, dtype=torch.float32)\n",
    "        actions_tensor = torch.as_tensor(actions, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.forward(states_tensor).gather(1, actions_tensor.unsqueeze(1))\n",
    "        return q_values.squeeze(1).tolist()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, env, bandit, qfunction, gamma=0.9):\n",
    "        self.env = env\n",
    "        self.bandit = bandit\n",
    "        self.qfunction = qfunction\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def execute(self, episodes=2000):\n",
    "        rewards = []\n",
    "        for episode in range(episodes):\n",
    "            observation, info = self.env.reset(seed=1)\n",
    "            state = tuple(np.concatenate([observation['agent'], observation['target']]))\n",
    "            actions = self.env.action_space\n",
    "            action = self.bandit.select(state, actions, self.qfunction)\n",
    "            episode_reward = 0.0\n",
    "            for step in count():\n",
    "                (next_observation, reward, terminated, truncated, info) = self.env.step(action)             \n",
    "                next_state = tuple(\n",
    "                    np.concatenate(\n",
    "                        [next_observation['agent'], next_observation['target']]\n",
    "                        )\n",
    "                    )\n",
    "                next_action = self.bandit.select(next_state, actions, self.qfunction)\n",
    "                delta = self.get_delta(state, action, reward, next_state, next_action)\n",
    "                self.qfunction.update(state, action, delta)\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                episode_reward += reward * (self.gamma ** step)\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "            rewards.append(episode_reward)\n",
    "        return rewards\n",
    "    \n",
    "    def get_delta(self, state, action, reward, next_state, next_action):\n",
    "        q_value = self.qfunction.get_q_value(state, action)\n",
    "        next_state_value = self.state_value(next_state, next_action)\n",
    "        delta = reward + self.gamma * next_state_value - q_value\n",
    "        return delta\n",
    "    \n",
    "    def state_value(self, state, action):\n",
    "        actions = self.env.action_space\n",
    "        max_q_value = self.qfunction.get_max_q(state, actions)\n",
    "        return max_q_value\n",
    "    \n",
    "\n",
    "class SARSA:\n",
    "    def __init__(self, env, bandit, qfunction, gamma=0.9):\n",
    "        self.env = env\n",
    "        self.bandit = bandit\n",
    "        self.qfunction = qfunction\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def execute(self, episodes=2000):\n",
    "        rewards = []\n",
    "        for episode in range(episodes):\n",
    "            observation, info = self.env.reset(seed=1)\n",
    "            state = tuple(np.concatenate([observation['agent'], observation['target']]))\n",
    "            actions = self.env.action_space\n",
    "            action = self.bandit.select(state, actions, self.qfunction)\n",
    "            episode_reward = 0.0\n",
    "            for step in count():\n",
    "                (next_observation, reward, terminated, truncated, info) = self.env.step(action)             \n",
    "                next_state = tuple(\n",
    "                    np.concatenate(\n",
    "                        [next_observation['agent'], next_observation['target']]\n",
    "                        )\n",
    "                    )\n",
    "                next_action = self.bandit.select(next_state, actions, self.qfunction)\n",
    "                delta = self.get_delta(state, action, reward, next_state, next_action)\n",
    "                self.qfunction.update(state, action, delta)\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                episode_reward += reward * (self.gamma ** step)\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "            rewards.append(episode_reward)\n",
    "        return rewards\n",
    "    \n",
    "    def get_delta(self, state, action, reward, next_state, next_action):\n",
    "        q_value = self.qfunction.get_q_value(state, action)\n",
    "        next_state_value = self.state_value(next_state, next_action)\n",
    "        delta = reward + self.gamma * next_state_value - q_value\n",
    "        return delta\n",
    "    \n",
    "    def state_value(self, state, action):\n",
    "        actions = self.env.action_space\n",
    "        max_q_value = self.qfunction.get_q_value(state, action)\n",
    "        return max_q_value\n",
    "\n",
    "\n",
    "def get_ema(rewards, smoothing_factor=0.9):\n",
    "    smoothed_rewards = []\n",
    "    for reward in rewards:\n",
    "        if smoothed_rewards == []:\n",
    "            smoothed_rewards = [reward]\n",
    "        else:\n",
    "            smoothed_rewards += [\n",
    "                smoothed_rewards[-1] * smoothing_factor\n",
    "                + reward * (1 - smoothing_factor)\n",
    "            ]\n",
    "    return smoothed_rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
