{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based vs Model-free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-based reinforcement learning need to **know the model**; in particular, we have access to $P_a(s' \\mid s)$ and $r(s,a,s')$.\n",
    "\n",
    "While in model-free reinforcement learning, we don't know the transitions and the rewards?!* We **learn through experience** by trying actions and seeing what the results is, making this machine learning problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    \"\"\"Return all states of this MDP\"\"\"\n",
    "\n",
    "    def get_states(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return all actions with non-zero probability from this state \"\"\"\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return all non-zero probability transitions for this action\n",
    "        from this state, as a list of (state, probability) pairs\n",
    "    \"\"\"\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return the reward for transitioning from state to\n",
    "        nextState via action\n",
    "    \"\"\"\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return true if and only if state is a terminal state of this MDP \"\"\"\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return the discount factor for this MDP \"\"\"\n",
    "\n",
    "    def get_discount_factor(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return the initial state of this MDP \"\"\"\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return all goal states of this MDP \"\"\"\n",
    "\n",
    "    def get_goal_states(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return a new state and a reward for executing action in state,\n",
    "    based on the underlying probability. This can be used for\n",
    "    model-free learning methods, but requires a model to operate.\n",
    "    Override for simulation-based learning\n",
    "    \"\"\"\n",
    "\n",
    "    def execute(self, state, action):\n",
    "        rand = random.random()\n",
    "        cumulative_probability = 0.0\n",
    "        for (new_state, probability) in self.get_transitions(state, action):\n",
    "            if cumulative_probability <= rand <= probability + cumulative_probability:\n",
    "                reward = self.get_reward(state, action, new_state)\n",
    "                return (new_state, reward, self.is_terminal(new_state))\n",
    "            cumulative_probability += probability\n",
    "            if cumulative_probability >= 1.0:\n",
    "                raise (\n",
    "                    \"Cumulative probability >= 1.0 for action \"\n",
    "                    + str(action)\n",
    "                    + \" from \"\n",
    "                    + str(state)\n",
    "                )\n",
    "\n",
    "        raise BaseException(\n",
    "            \"No outcome state in simulation for action \"\n",
    "            + str(action)\n",
    "            + \" from \"\n",
    "            + str(state)\n",
    "        )\n",
    "\n",
    "    \"\"\" \n",
    "    Execute a policy on this mdp for a number of episodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def execute_policy(self, policy, episodes=100, max_step=100):\n",
    "        cumulative_rewards = []\n",
    "        states = set()\n",
    "        for _ in range(episodes):\n",
    "            cumulative_reward = 0.0\n",
    "            state = self.get_initial_state()\n",
    "            step = 0\n",
    "            while not self.is_terminal(state):\n",
    "                actions = self.get_actions(state)\n",
    "                action = policy.select_action(state, actions)\n",
    "                (next_state, reward, done) = self.execute(state, action)\n",
    "                cumulative_reward += reward * (self.discount_factor ** step)\n",
    "                state = next_state\n",
    "                step += 1\n",
    "                if step > max_step:\n",
    "                    break\n",
    "            cumulative_rewards += [cumulative_reward]\n",
    "        return cumulative_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning: Off-policy Temporal-difference Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFreeLearner:\n",
    "    def execute(self, eposodes=2000):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TempralDifferenceLearner(ModelFreeLearner):\n",
    "    def __init__(self, mdp, bandit, qfunction):\n",
    "        self.mdp = mdp\n",
    "        self.bandit = bandit\n",
    "        self.qfunction = qfunction\n",
    "\n",
    "    def execute(self, episodes=2000):\n",
    "\n",
    "        rewards = []\n",
    "        for episode in range(episodes):\n",
    "            state = self.mdp.get_initial_state()\n",
    "            actions = self.mdp.get_actions(state)\n",
    "            action = self.bandit.select(state, actions, self.qfunction)\n",
    "\n",
    "            episode_reward = 0.0\n",
    "            step = 0\n",
    "            while not self.mdp.is_terminal(state):\n",
    "                (next_state, reward, done) = self.mdp.execute(state, action)\n",
    "                actions = self.mdp.get_actions(next_state)\n",
    "                next_action = self.bandit.select(next_state, actions, self.qfunction)\n",
    "\n",
    "                delta = self.get_delta(reward, state, action, next_state, next_action)\n",
    "                self.qfunction.update(state, action, delta)\n",
    "                \n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                episode_reward += reward * (self.mdp.discount_factor ** step)\n",
    "                step += 1\n",
    "            \n",
    "            rewards.append(episode_reward)\n",
    "\n",
    "        return rewards\n",
    "    \n",
    "    \"\"\" Calculate the delta for the update \"\"\"\n",
    "\n",
    "    def get_delta(self, reward, state, action, next_state, next_action):\n",
    "        q_value = self.qfunction.get_q_value(state, action)\n",
    "        next_state_value = self.state_value(next_state, next_action)\n",
    "        delta = reward + self.mdp.discount_factor * next\n",
    "        pass\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
