{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from rendering_utils import *\n",
    "from plot import Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    \"\"\"Return all states of this MDP\"\"\"\n",
    "\n",
    "    def get_states(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return all actions with non-zero probability from this state \"\"\"\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return all non-zero probability transitions for this action\n",
    "        from this state, as a list of (state, probability) pairs\n",
    "    \"\"\"\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return the reward for transitioning from state to\n",
    "        nextState via action\n",
    "    \"\"\"\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return true if and only if state is a terminal state of this MDP \"\"\"\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return the discount factor for this MDP \"\"\"\n",
    "\n",
    "    def get_discount_factor(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return the initial state of this MDP \"\"\"\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return all goal states of this MDP \"\"\"\n",
    "\n",
    "    def get_goal_states(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return a new state and a reward for executing action in state,\n",
    "    based on the underlying probability. This can be used for\n",
    "    model-free learning methods, but requires a model to operate.\n",
    "    Override for simulation-based learning\n",
    "    \"\"\"\n",
    "\n",
    "    def execute(self, state, action):\n",
    "        rand = random.random()\n",
    "        cumulative_probability = 0.0\n",
    "        for (new_state, probability) in self.get_transitions(state, action):\n",
    "            if cumulative_probability <= rand <= probability + cumulative_probability:\n",
    "                reward = self.get_reward(state, action, new_state)\n",
    "                return (new_state, reward, self.is_terminal(new_state))\n",
    "            cumulative_probability += probability\n",
    "            if cumulative_probability >= 1.0:\n",
    "                raise (\n",
    "                    \"Cumulative probability >= 1.0 for action \"\n",
    "                    + str(action)\n",
    "                    + \" from \"\n",
    "                    + str(state)\n",
    "                )\n",
    "\n",
    "        raise BaseException(\n",
    "            \"No outcome state in simulation for action \"\n",
    "            + str(action)\n",
    "            + \" from \"\n",
    "            + str(state)\n",
    "        )\n",
    "\n",
    "    \"\"\" \n",
    "    Execute a policy on this mdp for a number of episodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def execute_policy(self, policy, episodes=100, max_step=100):\n",
    "        cumulative_rewards = []\n",
    "        states = set()\n",
    "        for _ in range(episodes):\n",
    "            cumulative_reward = 0.0\n",
    "            state = self.get_initial_state()\n",
    "            step = 0\n",
    "            while not self.is_terminal(state):\n",
    "                actions = self.get_actions(state)\n",
    "                action = policy.select_action(state, actions)\n",
    "                (next_state, reward, done) = self.execute(state, action)\n",
    "                cumulative_reward += reward * (self.discount_factor ** step)\n",
    "                state = next_state\n",
    "                step += 1\n",
    "                if step > max_step:\n",
    "                    break\n",
    "            cumulative_rewards += [cumulative_reward]\n",
    "        return cumulative_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(MDP):\n",
    "    # labels for terminate action and terminal state\n",
    "    TERMINAL = (-1, -1)\n",
    "    TERMINATE = 0\n",
    "    LEFT = 1\n",
    "    UP = 2\n",
    "    RIGHT = 3\n",
    "    DOWN = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        noise=0.1,\n",
    "        width=4,\n",
    "        height=3,\n",
    "        discount_factor=0.9,\n",
    "        blocked_states=[(1, 1)],\n",
    "        action_cost=0.0,\n",
    "        initial_state=(0, 0),\n",
    "        goals=None,\n",
    "    ):\n",
    "        self.noise = noise\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.blocked_states = blocked_states\n",
    "        self.discount_factor = discount_factor\n",
    "        self.action_cost = action_cost\n",
    "        self.initial_state = initial_state\n",
    "        if goals is None:\n",
    "            self.goal_states = dict(\n",
    "                [((width - 1, height - 1), 1), ((width - 1, height - 2), -1)]\n",
    "            )\n",
    "        else:\n",
    "            self.goal_states = dict(goals)\n",
    "\n",
    "        # A list of lists that records all rewards given at each step\n",
    "        # for each episode of a simulated gridworld\n",
    "        self.rewards = []\n",
    "\n",
    "        # A list of cumulative rewards for each episode\n",
    "        self.cumulative_rewards = []\n",
    "    \n",
    "        # The rewards for the current episode\n",
    "        self.episode_rewards = []\n",
    "\n",
    "\n",
    "    def get_states(self):\n",
    "        states = [self.TERMINAL]\n",
    "        for x in range(self.width):\n",
    "            for y in range(self.height):\n",
    "                if not (x, y) in self.blocked_states:\n",
    "                    states.append((x, y))\n",
    "        return states\n",
    "\n",
    "    def get_actions(self, state=None):\n",
    "\n",
    "        actions = [self.TERMINATE, self.LEFT, self.UP, self.RIGHT, self.DOWN]\n",
    "        if state is None:\n",
    "            return actions\n",
    "\n",
    "        valid_actions = []\n",
    "        for action in actions:\n",
    "            for (new_state, probability) in self.get_transitions(state, action):\n",
    "                if probability > 0:\n",
    "                    valid_actions.append(action)\n",
    "                    break\n",
    "        return valid_actions\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        self.episode_rewards = []\n",
    "        return self.initial_state\n",
    "\n",
    "    def get_goal_states(self):\n",
    "        return self.goal_states\n",
    "\n",
    "    def valid_add(self, state, new_state, probability):\n",
    "        # If the next state is blocked, stay in the same state\n",
    "        if probability == 0.0:\n",
    "            return []\n",
    "\n",
    "        if new_state in self.blocked_states:\n",
    "            return [(state, probability)]\n",
    "\n",
    "        # Move to the next space if it is not off the grid\n",
    "        (x, y) = new_state\n",
    "        if x >= 0 and x < self.width and y >= 0 and y < self.height:\n",
    "            return [((x, y), probability)]\n",
    "\n",
    "        # If off the grid, state in the same state\n",
    "        return [(state, probability)]\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        transitions = []\n",
    "\n",
    "        if state == self.TERMINAL:\n",
    "            if action == self.TERMINATE:\n",
    "                return [(self.TERMINAL, 1.0)]\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        # Probability of not slipping left or right\n",
    "        straight = 1 - (2 * self.noise)\n",
    "\n",
    "        (x, y) = state\n",
    "        if state in self.get_goal_states().keys():\n",
    "            if action == self.TERMINATE:\n",
    "                transitions += [(self.TERMINAL, 1.0)]\n",
    "\n",
    "        elif action == self.UP:\n",
    "            transitions += self.valid_add(state, (x, y + 1), straight)\n",
    "            transitions += self.valid_add(state, (x - 1, y), self.noise)\n",
    "            transitions += self.valid_add(state, (x + 1, y), self.noise)\n",
    "\n",
    "        elif action == self.DOWN:\n",
    "            transitions += self.valid_add(state, (x, y - 1), straight)\n",
    "            transitions += self.valid_add(state, (x - 1, y), self.noise)\n",
    "            transitions += self.valid_add(state, (x + 1, y), self.noise)\n",
    "\n",
    "        elif action == self.RIGHT:\n",
    "            transitions += self.valid_add(state, (x + 1, y), straight)\n",
    "            transitions += self.valid_add(state, (x, y - 1), self.noise)\n",
    "            transitions += self.valid_add(state, (x, y + 1), self.noise)\n",
    "\n",
    "        elif action == self.LEFT:\n",
    "            transitions += self.valid_add(state, (x - 1, y), straight)\n",
    "            transitions += self.valid_add(state, (x, y - 1), self.noise)\n",
    "            transitions += self.valid_add(state, (x, y + 1), self.noise)\n",
    "\n",
    "        # Merge any duplicate outcomes\n",
    "        merged = defaultdict(lambda: 0.0)\n",
    "        for (state, probability) in transitions:\n",
    "            merged[state] = merged[state] + probability\n",
    "\n",
    "        transitions = []\n",
    "        for outcome in merged.keys():\n",
    "            transitions += [(outcome, merged[outcome])]\n",
    "\n",
    "        return transitions\n",
    "\n",
    "    def get_reward(self, state, action, new_state):\n",
    "        reward = 0.0\n",
    "        if state in self.get_goal_states().keys() and new_state == self.TERMINAL:\n",
    "            reward = self.get_goal_states().get(state)\n",
    "        else:\n",
    "            reward = self.action_cost\n",
    "        step = len(self.episode_rewards)\n",
    "        self.episode_rewards += [reward * (self.discount_factor ** step)]\n",
    "        return reward\n",
    "\n",
    "    def get_discount_factor(self):\n",
    "        return self.discount_factor\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        if state == self.TERMINAL:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    \"\"\"\n",
    "        Returns a list of lists, which records all rewards given at each step\n",
    "        for each episode of a simulated gridworld\n",
    "    \"\"\"\n",
    "\n",
    "    def get_rewards(self):\n",
    "        return self.rewards\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "        Returns a list of all cumulative rewards\n",
    "        for each episode of a simulated gridworld\n",
    "    \"\"\"\n",
    "\n",
    "    def get_cumulative_rewards(self):\n",
    "        return self.cumulative_rewards\n",
    "\n",
    "    \"\"\"\n",
    "        Create a gridworld from an array of strings: one for each line\n",
    "        - First line is rewards as a dictionary from cell to value: {'A': 1, ...}\n",
    "        - space is an empty cell\n",
    "        - # is a blocked cell\n",
    "        - @ is the agent (initial state)\n",
    "        - new 'line' is a new row\n",
    "        - a letter is a cell with a reward for transitioning\n",
    "          into that cell. The reward defined by the first line.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create(string):\n",
    "        # Parse the reward on the first line\n",
    "        import ast\n",
    "\n",
    "        rewards = ast.literal_eval(string[0])\n",
    "\n",
    "        width = 0\n",
    "        height = len(string) - 1\n",
    "\n",
    "        blocked_cells = []\n",
    "        initial_state = (0, 0)\n",
    "        goals = []\n",
    "        row = 0\n",
    "        for next_row in string[1:]:\n",
    "            column = 0\n",
    "            for cell in next_row:\n",
    "                if cell == \"#\":\n",
    "                    blocked_cells += [(column, row)]\n",
    "                elif cell == \"@\":\n",
    "                    initial_state = (column, row)\n",
    "                elif cell.isalpha():\n",
    "                    goals += [((column, row), rewards[cell])]\n",
    "                column += 1\n",
    "            width = max(width, column)\n",
    "            row += 1\n",
    "        return GridWorld(\n",
    "            width=width,\n",
    "            height=height,\n",
    "            blocked_states=blocked_cells,\n",
    "            initial_state=initial_state,\n",
    "            goals=goals,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def open(file):\n",
    "        file = open(file, \"r\")\n",
    "        string = file.read().splitlines()\n",
    "        file.close()\n",
    "        return GridWorld.create(string)\n",
    "\n",
    "    @staticmethod\n",
    "    def matplotlib_installed():\n",
    "        try:\n",
    "            import matplotlib as mpl\n",
    "            import matplotlib.pyplot as plt\n",
    "            return True\n",
    "        except ModuleNotFoundError:\n",
    "            return False\n",
    "\n",
    "    \"\"\" Visualise a Grid World problem \"\"\"\n",
    "\n",
    "    def visualise(self, agent_position=None, title=\"\", grid_size=1.0, gif=False):\n",
    "        if self.matplotlib_installed():\n",
    "            return self.visualise_as_image(agent_position=agent_position, title=title, grid_size=grid_size, gif=gif)\n",
    "        else:\n",
    "            print(self.to_string(title=title))\n",
    "\n",
    "    \"\"\" Visualise a Grid World value function \"\"\"\n",
    "    def visualise_value_function(self, value_function, title=\"\", grid_size=1.0, gif=False):\n",
    "        if self.matplotlib_installed():\n",
    "            return self.visualise_value_function_as_image(value_function, title=title, grid_size=grid_size, gif=gif)\n",
    "        else:\n",
    "            print(self.value_function_to_string(value_function, title=title))\n",
    "\n",
    "    def visualise_q_function(self, qfunction, title=\"\", grid_size=1.5, gif=False):\n",
    "        if self.matplotlib_installed():\n",
    "            return self.visualise_q_function_as_image(qfunction, title=title, grid_size=grid_size, gif=gif)\n",
    "        else:\n",
    "            print(self.q_function_to_string(qfunction, title=title))\n",
    "\n",
    "    def visualise_policy(self, policy, title=\"\", grid_size=1.0, gif=False):\n",
    "        if self.matplotlib_installed():\n",
    "            return self.visualise_policy_as_image(policy, title=title, grid_size=grid_size, gif=gif)\n",
    "        else:\n",
    "            print(self.policy_to_string(policy, title=title))\n",
    "\n",
    "    def visualise_stochastic_policy(self, policy, title=\"\", grid_size=1.0, gif=False):\n",
    "        if self.matplotlib_installed():\n",
    "            return self.visualise_stochastic_policy_as_image(policy, title=title, grid_size=grid_size, gif=gif)\n",
    "        else:\n",
    "            # TODO make a stochastic policy to string\n",
    "            pass\n",
    "\n",
    "    \"\"\" Visualise a grid world problem as a formatted string \"\"\"\n",
    "    def to_string(self, title=\"\"):\n",
    "        left_arrow = \"\\u25C4\"\n",
    "        up_arrow = \"\\u25B2\"\n",
    "        right_arrow = \"\\u25BA\"\n",
    "        down_arrow = \"\\u25BC\"\n",
    "\n",
    "\n",
    "        space = \" |              \"\n",
    "        block = \" | #############\"\n",
    "\n",
    "        line = \"  \"\n",
    "        for x in range(self.width):\n",
    "            line += \"--------------- \"\n",
    "        line += \"\\n\"\n",
    "\n",
    "        result = \" \" + title + \"\\n\"\n",
    "        result += line\n",
    "        for y in range(self.height - 1, -1, -1):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.get_goal_states().keys():\n",
    "                    result += space\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                else:\n",
    "                    result += \" |       {}      \".format(up_arrow)\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) == self.get_initial_state():\n",
    "                    result += \" |     _____    \"\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                else:\n",
    "                    result += space\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) == self.get_initial_state():\n",
    "                    result += \" |    ||o  o|   \"\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                else:\n",
    "                    result += space\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) == self.get_initial_state():\n",
    "                    result += \" | {}  ||  * |  {}\".format(left_arrow, right_arrow)\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                elif (x, y) in self.get_goal_states().keys():\n",
    "                    result += \" |     {:+0.2f}    \".format(\n",
    "                        self.get_goal_states()[(x, y)]\n",
    "                    )\n",
    "                else:\n",
    "                    result += \" | {}           {}\".format(left_arrow, right_arrow)\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) == self.get_initial_state():\n",
    "                    result += \" |    ||====|   \".format(left_arrow, right_arrow)\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                else:\n",
    "                    result += space\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) == self.get_initial_state():\n",
    "                    result += \" |     -----    \"\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                else:\n",
    "                    result += space\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.get_goal_states().keys():\n",
    "                    result += space\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                else:\n",
    "                    result += \" |       {}      \".format(down_arrow)\n",
    "            result += \" |\\n\"\n",
    "            result += line\n",
    "        return result\n",
    "\n",
    "    \"\"\" Convert a grid world value function to a formatted string \"\"\"\n",
    "\n",
    "    def value_function_to_string(self, values, title=\"\"):\n",
    "        line = \" {:-^{n}}\\n\".format(\"\", n=len(\" | +0.00\") * self.width + 1)\n",
    "        result = \" \" + title + \"\\n\"\n",
    "        result += line\n",
    "        for y in range(self.height - 1, -1, -1):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states:\n",
    "                    result += \" | #####\"\n",
    "                else:\n",
    "                    result += \" | {:+0.2f}\".format(values.get_value((x, y)))\n",
    "            result += \" |\\n\"\n",
    "            result += line\n",
    "\n",
    "        return result\n",
    "\n",
    "    \"\"\" Convert a grid world Q function to a formatted string \"\"\"\n",
    "\n",
    "    def q_function_to_string(self, qfunction, title=\"\"):\n",
    "        left_arrow = \"\\u25C4\"\n",
    "        up_arrow = \"\\u25B2\"\n",
    "        right_arrow = \"\\u25BA\"\n",
    "        down_arrow = \"\\u25BC\"\n",
    "\n",
    "        space = \" |               \"\n",
    "\n",
    "        line = \"  \"\n",
    "        for x in range(self.width):\n",
    "            line += \"---------------- \"\n",
    "        line += \"\\n\"\n",
    "\n",
    "        result = \" \" + title + \"\\n\"\n",
    "        result += line\n",
    "        for y in range(self.height - 1, -1, -1):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states or (\n",
    "                    x,\n",
    "                    y,\n",
    "                ) in self.get_goal_states().keys():\n",
    "                    result += space\n",
    "                else:\n",
    "                    result += \" |       {}       \".format(up_arrow)\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states or (\n",
    "                    x,\n",
    "                    y,\n",
    "                ) in self.get_goal_states().keys():\n",
    "                    result += space\n",
    "                else:\n",
    "                    result += \" |     {:+0.2f}     \".format(\n",
    "                        qfunction.get_q_value((x, y), self.UP)\n",
    "                    )\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                result += space\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states:\n",
    "                    result += \" |     #####     \"\n",
    "                elif (x, y) in self.get_goal_states().keys():\n",
    "                    result += \" |     {:+0.2f}     \".format(\n",
    "                        self.get_goal_states()[(x, y)]\n",
    "                    )\n",
    "                else:\n",
    "                    result += \" | {}{:+0.2f}  {:+0.2f}{}\".format(\n",
    "                        left_arrow,\n",
    "                        qfunction.get_q_value((x, y), self.LEFT),\n",
    "                        qfunction.get_q_value((x, y), self.RIGHT),\n",
    "                        right_arrow,\n",
    "                    )\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                result += space\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states or (\n",
    "                    x,\n",
    "                    y,\n",
    "                ) in self.get_goal_states().keys():\n",
    "                    result += space\n",
    "                else:\n",
    "                    result += \" |     {:+0.2f}     \".format(\n",
    "                        qfunction.get_q_value((x, y), self.DOWN)\n",
    "                    )\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states or (\n",
    "                    x,\n",
    "                    y,\n",
    "                ) in self.get_goal_states().keys():\n",
    "                    result += space\n",
    "                else:\n",
    "                    result += \" |       {}       \".format(down_arrow)\n",
    "            result += \" |\\n\"\n",
    "            result += line\n",
    "        return result\n",
    "\n",
    "    \"\"\" Convert a grid world policy to a formatted string \"\"\"\n",
    "    def policy_to_string(self, policy, title=\"\"):\n",
    "        arrow_map = {self.UP:'\\u25B2',\n",
    "                     self.DOWN:'\\u25BC',\n",
    "                     self.LEFT:'\\u25C4',\n",
    "                     self.RIGHT:'\\u25BA',\n",
    "                    }\n",
    "        line = \" {:-^{n}}\\n\".format(\"\", n=len(\" |  N \") * self.width + 1)\n",
    "        result = \" \" + title + \"\\n\"\n",
    "        result += line\n",
    "        for y in range(self.height - 1, -1, -1):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states:\n",
    "                    result += \" | ###\"\n",
    "                elif policy.select_action((x, y), self.get_actions((x, y))) == self.TERMINATE:\n",
    "                    result += \" | {:+0d} \".format(self.goal_states[(x, y)])\n",
    "                else:\n",
    "                    result += \" |  \" + arrow_map[policy.select_action((x, y), self.get_actions((x, y)))] + \" \"\n",
    "            result += \" |\\n\"\n",
    "            result += line\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    \"\"\" Initialise a gridworld grid \"\"\"\n",
    "    def initialise_grid(self, grid_size=1.0):\n",
    "        fig = plt.figure(figsize=(self.width * grid_size, self.height * grid_size))\n",
    "\n",
    "        # Trim whitespace \n",
    "        plt.subplots_adjust(top=0.92, bottom=0.01, right=1, left=0, hspace=0, wspace=0)\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "        # Initialise the map to all white\n",
    "        img = [[COLOURS['white'] for _ in range(self.width)] for _ in range(self.height)]\n",
    "\n",
    "        # Render the grid\n",
    "        for y in range(0, self.height):\n",
    "            for x in range(0, self.width):\n",
    "                if (x, y) in self.goal_states:\n",
    "                    img[y][x] = COLOURS['red'] if self.goal_states[(x, y)] < 0 else COLOURS['green']\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    img[y][x] = COLOURS['grey']\n",
    "\n",
    "        ax.xaxis.set_ticklabels([])  # clear x tick labels\n",
    "        ax.axes.yaxis.set_ticklabels([])  # clear y tick labels\n",
    "        ax.tick_params(which='both', top=False, left=False, right=False, bottom=False)\n",
    "        ax.set_xticks([w - 0.5 for w in range(0, self.width, 1)])\n",
    "        ax.set_yticks([h - 0.5 for h in range(0, self.height, 1)])\n",
    "        ax.grid(color='lightgrey')\n",
    "        return fig, ax, img\n",
    "\n",
    "    \"\"\" visualise the gridworld problem as a matplotlib image \"\"\"\n",
    "\n",
    "    def visualise_as_image(self, agent_position=None, title=\"\", grid_size=1.0, gif=False):\n",
    "        fig, ax, img = self.initialise_grid(grid_size=grid_size)\n",
    "        current_position = (\n",
    "            self.get_initial_state() if agent_position is None else agent_position\n",
    "        )\n",
    "\n",
    "        # Render the grid\n",
    "        for y in range(0, self.height):\n",
    "            for x in range(0, self.width):\n",
    "                if (x, y) == current_position:\n",
    "                    ax.scatter(x, y, s=2000, marker='o', edgecolors='none')\n",
    "                elif (x, y) in self.goal_states:\n",
    "                    plt.text(\n",
    "                        x,\n",
    "                        y,\n",
    "                        f\"{self.get_goal_states()[(x, y)]:+0.2f}\",\n",
    "                        fontsize=\"x-large\",\n",
    "                        horizontalalignment=\"center\",\n",
    "                        verticalalignment=\"center\",\n",
    "                    )\n",
    "        im = plt.imshow(img, origin=\"lower\")\n",
    "        plt.title(title)\n",
    "        if gif:\n",
    "            return fig, ax, im\n",
    "        else:\n",
    "            return fig\n",
    "\n",
    "    \"\"\"Render each tile individually depending on the current state of the cell\"\"\"\n",
    "\n",
    "    def render_tile(self, x, y, tile_size, img, tile_type=None):\n",
    "        ymin = y * tile_size\n",
    "        ymax = (y + 1) * tile_size\n",
    "        xmin = x * tile_size\n",
    "        xmax = (x + 1) * tile_size\n",
    "\n",
    "        for i in range(ymin, ymax):\n",
    "            for j in range(xmin, xmax):\n",
    "                if i == ymin or i == ymax - 1 or j == xmin or j == xmax + 1:\n",
    "                    draw_grid_lines(i, j, img)\n",
    "                else:\n",
    "                    if tile_type == \"goal\":\n",
    "                        render_goal(\n",
    "                            i,\n",
    "                            j,\n",
    "                            img,\n",
    "                            reward=self.goal_states[(x, y)],\n",
    "                            reward_max=max(self.get_goal_states().values()),\n",
    "                            reward_min=min(self.get_goal_states().values()),\n",
    "                        )\n",
    "                    elif tile_type == \"blocked\":\n",
    "                        render_blocked_tile(i, j, img)\n",
    "                    elif tile_type == \"agent\":\n",
    "                        render_agent(\n",
    "                            i,\n",
    "                            j,\n",
    "                            img,\n",
    "                            center_x=xmin + tile_size / 2,\n",
    "                            center_y=ymin + tile_size / 2,\n",
    "                            radius=tile_size / 4,\n",
    "                        )\n",
    "                    elif tile_type == \"empty\":\n",
    "                        img[i][j] = [255, 255, 255]\n",
    "                    else:\n",
    "                        raise ValueError(\"Invalid tile type\")\n",
    "\n",
    "    \"\"\" Visualise the value function \"\"\"\n",
    "\n",
    "    def visualise_value_function_as_image(self, value_function, title=\"\", grid_size=1.0, gif=False):\n",
    "        if not gif:\n",
    "            fig, ax, img = self.initialise_grid(grid_size=grid_size)\n",
    "        texts = []\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                value = value_function.get_value((x, y))\n",
    "                if (x, y) not in self.blocked_states:\n",
    "                    text = plt.text(\n",
    "                        x,\n",
    "                        y,\n",
    "                        f\"{float(value):+0.2f}\",\n",
    "                        fontsize=\"medium\",\n",
    "                        horizontalalignment=\"center\",\n",
    "                        verticalalignment=\"center\",\n",
    "                        color='lightgrey' if value == 0.0 else 'black',\n",
    "                    )\n",
    "                    texts.append(text)\n",
    "        if gif:\n",
    "            return texts\n",
    "        else:\n",
    "            ax.imshow(img, origin=\"lower\")\n",
    "            plt.title(title, fontsize=\"large\")\n",
    "            plt.show()\n",
    "\n",
    "    \"\"\" Visualise the value function using a heat-map where green is high value and\n",
    "    red is low value\n",
    "    \"\"\"\n",
    "\n",
    "    def visualise_value_function_as_heatmap(self, value_function, title=\"\"):\n",
    "        values = [[0 for _ in range(self.width)] for _ in range(self.height)]\n",
    "        fig, ax = self.initialise_grid()\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states:\n",
    "                    plt.text(\n",
    "                        x,\n",
    "                        y,\n",
    "                        \"#\",\n",
    "                        horizontalalignment=\"center\",\n",
    "                        verticalalignment=\"center\",\n",
    "                    )\n",
    "                else:\n",
    "                    values[y][x] = value_function.get_value((x, y))\n",
    "                    plt.text(\n",
    "                        x,\n",
    "                        y,\n",
    "                        f\"{values[y][x]:.2f}\",\n",
    "                        horizontalalignment=\"center\",\n",
    "                        verticalalignment=\"center\",\n",
    "                    )\n",
    "        plt.imshow(values, origin=\"lower\", cmap=make_red_white_green_cmap())\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    \"\"\" Visualise the Q-function with matplotlib \"\"\"\n",
    "\n",
    "    def visualise_q_function_as_image(self, qfunction, title=\"\", grid_size=1.5, gif=False):\n",
    "        if not gif:\n",
    "            fig, ax, img = self.initialise_grid(grid_size=grid_size)\n",
    "        texts = []\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.goal_states:\n",
    "                    # gif player handles goal state rendering\n",
    "                    if not gif:\n",
    "                        texts.append(plt.text(\n",
    "                            x,\n",
    "                            y,\n",
    "                            f\"{self.get_goal_states()[(x,y)]:+0.2f}\",\n",
    "                            fontsize=\"large\",\n",
    "                            horizontalalignment=\"center\",\n",
    "                            verticalalignment=\"center\",\n",
    "                        ))\n",
    "                elif (x, y) not in self.blocked_states:\n",
    "                    up_value = qfunction.get_q_value((x, y), self.UP)\n",
    "                    down_value = qfunction.get_q_value((x, y), self.DOWN)\n",
    "                    left_value = qfunction.get_q_value((x, y), self.LEFT)\n",
    "                    right_value = qfunction.get_q_value((x, y), self.RIGHT)\n",
    "                    texts.append(plt.text(\n",
    "                        x,\n",
    "                        y + 0.35,\n",
    "                        f\"{up_value:+0.2f}\",\n",
    "                        fontsize=\"medium\",\n",
    "                        horizontalalignment=\"center\",\n",
    "                        verticalalignment=\"top\",\n",
    "                        color='lightgrey' if up_value == 0.0 else 'black',\n",
    "                    ))\n",
    "                    texts.append(plt.text(\n",
    "                        x,\n",
    "                        y - 0.35,\n",
    "                        f\"{down_value:+0.2f}\",\n",
    "                        fontsize=\"medium\",\n",
    "                        horizontalalignment=\"center\",\n",
    "                        verticalalignment=\"bottom\",\n",
    "                        color='lightgrey' if down_value == 0.0 else 'black',\n",
    "                    ))\n",
    "                    texts.append(plt.text(\n",
    "                        x - 0.45,\n",
    "                        y,\n",
    "                        f\"{left_value:+0.2f}\",\n",
    "                        fontsize=\"medium\",\n",
    "                        horizontalalignment=\"left\",\n",
    "                        verticalalignment=\"center\",\n",
    "                        color='lightgrey' if left_value == 0.0 else 'black'\n",
    "                    ))\n",
    "                    texts.append(plt.text(\n",
    "                        x + 0.45,\n",
    "                        y,\n",
    "                        f\"{right_value:+0.2f}\",\n",
    "                        fontsize=\"medium\",\n",
    "                        horizontalalignment=\"right\",\n",
    "                        verticalalignment=\"center\",\n",
    "                        color='lightgrey' if right_value == 0.0 else 'black'\n",
    "                    ))\n",
    "                    plt.plot([x-0.5, x+0.5], [y-0.5, y+0.5], ls='-', lw=1, color='lightgrey')\n",
    "                    plt.plot([x + 0.5, x - 0.5], [y - 0.5, y + 0.5], ls='-', lw=1, color='lightgrey')\n",
    "        if gif:\n",
    "            return texts\n",
    "        ax.imshow(img, origin=\"lower\")\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    \"\"\" Visualise the Q-function with a matplotlib visual\"\"\"\n",
    "\n",
    "    def visualise_q_function_rendered(self, q_values, title=\"\", tile_size=32, show_text=False):\n",
    "        width_px = self.width * tile_size\n",
    "        height_px = self.height * tile_size\n",
    "        img = [[[0, 0, 0] for _ in range(width_px)] for _ in range(height_px)]\n",
    "\n",
    "        # provide these to scale the colours between the highest and lowest value\n",
    "        reward_max = max(self.get_goal_states().values())\n",
    "        reward_min = min(self.get_goal_states().values())\n",
    "        # Render the grid\n",
    "        for y in range(0, self.height):\n",
    "            for x in range(0, self.width):\n",
    "                # Draw in the blocked states as a black and white mesh\n",
    "                if (x, y) in self.blocked_states:\n",
    "                    render_full_blocked_tile(\n",
    "                        x * tile_size, y * tile_size, tile_size, img\n",
    "                    )\n",
    "                    continue\n",
    "                # Draw goal states\n",
    "                if (x, y) in self.goal_states:\n",
    "                    render_full_goal_tile(\n",
    "                        x * tile_size,\n",
    "                        y * tile_size,\n",
    "                        tile_size,\n",
    "                        img,\n",
    "                        reward=self.goal_states[(x, y)],\n",
    "                        rewardMax=reward_max,\n",
    "                        rewardMin=reward_min,\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                # Draw the action value for action available in each cell\n",
    "                # Break the grid up into 4 sections, using triangles that meet\n",
    "                # in the middle. The base of the triangle points toward the\n",
    "                # direction of the action\n",
    "                render_action_q_value(\n",
    "                    tile_size,\n",
    "                    x,\n",
    "                    y,\n",
    "                    self.UP,\n",
    "                    q_values,\n",
    "                    img,\n",
    "                    show_text,\n",
    "                    v_text_offset=8,\n",
    "                    rewardMax=reward_max,\n",
    "                    rewardMin=reward_min,\n",
    "                )\n",
    "                render_action_q_value(\n",
    "                    tile_size,\n",
    "                    x,\n",
    "                    y,\n",
    "                    self.DOWN,\n",
    "                    q_values,\n",
    "                    img,\n",
    "                    show_text,\n",
    "                    v_text_offset=-8,\n",
    "                    rewardMax=reward_max,\n",
    "                    rewardMin=reward_min,\n",
    "                )\n",
    "                render_action_q_value(\n",
    "                    tile_size,\n",
    "                    x,\n",
    "                    y,\n",
    "                    self.LEFT,\n",
    "                    q_values,\n",
    "                    img,\n",
    "                    show_text,\n",
    "                    h_text_offset=-8,\n",
    "                    rewardMax=reward_max,\n",
    "                    rewardMin=reward_min,\n",
    "                )\n",
    "                render_action_q_value(\n",
    "                    tile_size,\n",
    "                    x,\n",
    "                    y,\n",
    "                    self.RIGHT,\n",
    "                    q_values,\n",
    "                    img,\n",
    "                    show_text,\n",
    "                    h_text_offset=8,\n",
    "                    rewardMax=reward_max,\n",
    "                    rewardMin=reward_min,\n",
    "                )\n",
    "\n",
    "        ax.imshow(img, origin=\"lower\", interpolation=\"bilinear\")\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    \"\"\" Visualise the policy of the agent with a matplotlib visual \"\"\"\n",
    "\n",
    "    def visualise_policy_as_image(self, policy, title=\"\", grid_size=1.0, gif=False):\n",
    "        # Map from action names to prettier arrows\n",
    "        arrow_map = {self.UP:'\\u2191',\n",
    "                     self.DOWN:'\\u2193',\n",
    "                     self.LEFT:'\\u2190',\n",
    "                     self.RIGHT:'\\u2192',\n",
    "                    }\n",
    "        if not gif:\n",
    "            fig, ax, img = self.initialise_grid(grid_size=grid_size)\n",
    "        texts = []\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) not in self.blocked_states and (x, y) not in self.goal_states:\n",
    "                    if policy.select_action((x, y), self.get_actions((x, y))) != self.TERMINATE:\n",
    "                        action = arrow_map[policy.select_action((x, y), self.get_actions((x, y)))]\n",
    "                        fontsize = \"xx-large\"\n",
    "                    texts.append(plt.text(\n",
    "                                x,\n",
    "                                y,\n",
    "                                action,\n",
    "                                fontsize=fontsize,\n",
    "                                horizontalalignment=\"center\",\n",
    "                                verticalalignment=\"center\",\n",
    "                            ))\n",
    "                elif (x, y) in self.goal_states:\n",
    "                    # gif player handles goal state rendering\n",
    "                    if not gif:\n",
    "                        plt.text(\n",
    "                            x,\n",
    "                            y,\n",
    "                            f\"{self.get_goal_states()[(x, y)]:+0.2f}\",\n",
    "                            fontsize=\"x-large\",\n",
    "                            horizontalalignment=\"center\",\n",
    "                            verticalalignment=\"center\",\n",
    "                        )\n",
    "        if gif:\n",
    "            return texts\n",
    "        ax.imshow(img, origin=\"lower\")\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    def execute(self, state, action):\n",
    "        if state in self.goal_states:\n",
    "            self.rewards += [self.episode_rewards]\n",
    "            self.cumulative_rewards += [sum(self.episode_rewards)]\n",
    "            return MDP.execute(self, state=state, action=self.TERMINATE)\n",
    "        return super().execute(state, action)\n",
    "\n",
    "    def visualise_stochastic_policy_as_image(self, policy, title=\"\", grid_size=1.0, gif=False):\n",
    "        if not gif:\n",
    "            fig, ax, img = self.initialise_grid(grid_size=grid_size)\n",
    "        texts = []\n",
    "\n",
    "        # Render the grid\n",
    "        for y in range(0, self.height):\n",
    "            for x in range(0, self.width):\n",
    "                prob_up = 0.0\n",
    "                prob_down = 0.0\n",
    "                prob_left = policy.get_probability((x, y), self.LEFT)\n",
    "                prob_right = policy.get_probability((x, y), self.RIGHT)\n",
    "                if self.height > 1:\n",
    "                    prob_up = policy.get_probability((x, y), self.UP)\n",
    "                    prob_down = policy.get_probability((x, y), self.DOWN)\n",
    "                # Normalise to account for the 'terminate' action that is not visualised\n",
    "                total = prob_left + prob_right + prob_down + prob_up\n",
    "                if total != 0:\n",
    "                    prob_left = prob_left / total\n",
    "                    prob_right = prob_right / total\n",
    "                    prob_down = prob_down / total\n",
    "                    prob_up = prob_up / total\n",
    "                if (x, y) in self.goal_states:\n",
    "                    # gif player handles goal state rendering\n",
    "                    if not gif:\n",
    "                        plt.text(\n",
    "                            x,\n",
    "                            y,\n",
    "                            f\"{self.get_goal_states()[(x, y)]:+0.2f}\",\n",
    "                            fontsize=\"x-large\",\n",
    "                            horizontalalignment=\"center\",\n",
    "                            verticalalignment=\"center\",\n",
    "                        )\n",
    "                elif (x, y) not in self.blocked_states:\n",
    "                    left_triangle = '\\u25C4'\n",
    "                    up_triangle = '\\u25B2'\n",
    "                    right_triangle = '\\u25BA'\n",
    "                    down_triangle = '\\u25BC'\n",
    "                    if self.height > 1:\n",
    "                        texts.append(plt.text(\n",
    "                            x,\n",
    "                            y,\n",
    "                            f\"{prob_up:0.2f}\\n{up_triangle}\\n{prob_left:0.2f}{left_triangle} {right_triangle}{prob_right:0.2f}\\n{down_triangle}\\n{prob_down:0.2f}\",\n",
    "                            fontsize=\"medium\",\n",
    "                            horizontalalignment=\"center\",\n",
    "                            verticalalignment=\"center\",\n",
    "                        ))\n",
    "                    else:\n",
    "                        texts.append(plt.text(\n",
    "                            x,\n",
    "                            y,\n",
    "                            f\"{prob_left:0.2f}{left_triangle} {right_triangle}{prob_right:0.2f}\",\n",
    "                            fontsize=\"medium\",\n",
    "                            horizontalalignment=\"center\",\n",
    "                            verticalalignment=\"center\",\n",
    "                        ))\n",
    "        if gif:\n",
    "            return texts\n",
    "        ax.imshow(img, origin=\"lower\")\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit():\n",
    "\n",
    "    \"\"\" Select an action for this state given from a list given a Q-function \"\"\"\n",
    "\n",
    "    def select(self, state, actions, qfunction):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Reset a multi-armed bandit to its initial configuration \"\"\"\n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__()\n",
    "\n",
    "\n",
    "class EpsilonGreedy(MultiArmedBandit):\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def select(self, state, actions, qfunction):\n",
    "        # Select a random action with epsilon probability\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(actions)\n",
    "        arg_max_q = qfunction.get_argmax_q(state, actions)\n",
    "        return arg_max_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFunction:\n",
    "\n",
    "    \"\"\" Update the Q-value of (state, action) by delta \"\"\"\n",
    "\n",
    "    def update(self, state, action, delta):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Get a Q value for a given state-action pair \"\"\"\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Save a policy to a specified filename \"\"\"\n",
    "    def save_policy(self, filename):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Load a policy from a specified filename \"\"\"\n",
    "    def load_policy(self, filename):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return the action with the maximum Q-value \"\"\"\n",
    "    def get_argmax_q(self, state, actions):\n",
    "        (argmax_q, max_q) = self.get_max_pair(state, actions)\n",
    "        return argmax_q\n",
    "\n",
    "    \"\"\" Return the maximum Q-value in this Q-function \"\"\"\n",
    "    def get_max_q(self, state, actions):\n",
    "        (argmax_q, max_q) = self.get_max_pair(state, actions)\n",
    "        return max_q\n",
    "\n",
    "    \"\"\" Return a pair containing the action and Q-value, where the\n",
    "        action has the maximum Q-value in state\n",
    "    \"\"\"\n",
    "    def get_max_pair(self, state, actions):\n",
    "        arg_max_q = None\n",
    "        max_q = float(\"-inf\")\n",
    "        for action in actions:\n",
    "            value = self.get_q_value(state, action)\n",
    "            if max_q < value:\n",
    "                arg_max_q = action\n",
    "                max_q = value\n",
    "        return (arg_max_q, max_q)\n",
    "\n",
    "\n",
    "class QTable(QFunction):\n",
    "    def __init__(self, alpha=0.1, default_q_value=0.0):\n",
    "        self.qtable = defaultdict(lambda: default_q_value)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def update(self, state, action, delta):\n",
    "        self.qtable[(state, action)] = self.qtable[(state, action)] + self.alpha * delta\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        return self.qtable[(state, action)]\n",
    "\n",
    "    def save(self, filename):\n",
    "        with open(filename, \"w\") as file:\n",
    "            serialised = {str(key): value for key, value in self.qtable.items()}\n",
    "            json.dump(serialised, file)\n",
    "\n",
    "    def load(self, filename, default=0.0):\n",
    "        with open(filename, \"r\") as file:\n",
    "            serialised = json.load(file)\n",
    "            self.qtable = defaultdict(\n",
    "                lambda: default,\n",
    "                {tuple(eval(key)): value for key, value in serialised.items()},\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFreeLearner:\n",
    "    def execute(self, eposodes=2000):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TemporalDifferenceLearner(ModelFreeLearner):\n",
    "    def __init__(self, mdp, bandit, qfunction):\n",
    "        self.mdp = mdp\n",
    "        self.bandit = bandit\n",
    "        self.qfunction = qfunction\n",
    "\n",
    "    def execute(self, episodes=2000):\n",
    "\n",
    "        rewards = []\n",
    "        for episode in range(episodes):\n",
    "            state = self.mdp.get_initial_state()\n",
    "            actions = self.mdp.get_actions(state)\n",
    "            action = self.bandit.select(state, actions, self.qfunction)\n",
    "\n",
    "            episode_reward = 0.0\n",
    "            step = 0\n",
    "            while not self.mdp.is_terminal(state):\n",
    "                (next_state, reward, done) = self.mdp.execute(state, action)\n",
    "                actions = self.mdp.get_actions(next_state)\n",
    "                next_action = self.bandit.select(next_state, actions, self.qfunction)\n",
    "\n",
    "                delta = self.get_delta(reward, state, action, next_state, next_action)\n",
    "                self.qfunction.update(state, action, delta)\n",
    "                \n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                episode_reward += reward * (self.mdp.discount_factor ** step)\n",
    "                step += 1\n",
    "            \n",
    "            rewards.append(episode_reward)\n",
    "\n",
    "        return rewards\n",
    "    \n",
    "    \"\"\" Calculate the delta for the update \"\"\"\n",
    "\n",
    "    def get_delta(self, reward, state, action, next_state, next_action):\n",
    "        q_value = self.qfunction.get_q_value(state, action)\n",
    "        next_state_value = self.state_value(next_state, next_action)\n",
    "        delta = reward + self.mdp.discount_factor * next_state_value - q_value\n",
    "        return delta\n",
    "    \n",
    "    \"\"\" Get the value of a state \"\"\"\n",
    "    \n",
    "    def state_value(self, state, action):\n",
    "        pass\n",
    "\n",
    "\n",
    "class QLearning(TemporalDifferenceLearner):\n",
    "    def state_value(self, state, action):\n",
    "        max_q_value = self.qfunction.get_max_q(state, self.mdp.get_actions(state))\n",
    "        return max_q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def select_action(self, state, action):\n",
    "        pass\n",
    "\n",
    "\n",
    "class DeterministicPolicy(Policy):\n",
    "    def update(self, state, action):\n",
    "        pass\n",
    "\n",
    "\n",
    "class QPolicy(DeterministicPolicy):\n",
    "    def __init__(self, qfunction):\n",
    "        self.qfunction = qfunction\n",
    "\n",
    "    def select_action(self, state, actions):\n",
    "        return self.qfunction.get_argmax_q(state, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PotentialFunction:\n",
    "    def get_potential(self, state):\n",
    "        pass\n",
    "\n",
    "   \n",
    "class GridWorldPotentialFunction(PotentialFunction):\n",
    "    def __init__(self, mdp):\n",
    "        self.mdp = mdp\n",
    "\n",
    "    def get_potential(self, state):\n",
    "        if state != GridWorld.TERMINAL:\n",
    "            goal = (self.mdp.width, self.mdp.height)\n",
    "            x = 0\n",
    "            y = 1\n",
    "            return 0.1 * (\n",
    "                1 - ((goal[x] - state[x] + goal[y] - state[y]) / (goal[x] + goal[y]))\n",
    "            )\n",
    "        else:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardShapedQLearning(QLearning):\n",
    "    def __init__(self, mdp, bandit, potential, qfunction):\n",
    "        super().__init__(mdp, bandit, qfunction=qfunction)\n",
    "        self.potential = potential\n",
    "\n",
    "    def get_delta(self, reward, state, action, next_state, next_action):\n",
    "        q_value = self.qfunction.get_q_value(state, action)\n",
    "        next_state_value = self.state_value(next_state, next_action)\n",
    "        state_potential = self.potential.get_potential(state)\n",
    "        next_state_potential = self.potential.get_potential(next_state)\n",
    "        potential = self.mdp.discount_factor * next_state_potential - state_potential\n",
    "        delta = reward + potential + self.mdp.discount_factor * next_state_value - q_value\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdp = GridWorld(width = 15, height = 12, goals = [((14,11), 1), ((13,11), -1)])\n",
    "# qfunction = QTable()\n",
    "# potential = GridWorldPotentialFunction(mdp)\n",
    "# RewardShapedQLearning(mdp, EpsilonGreedy(), potential, qfunction).execute(episodes=200)\n",
    "# policy = QPolicy(qfunction)\n",
    "# mdp.visualise_q_function(qfunction)\n",
    "# mdp.visualise_policy(policy)\n",
    "# reward_shaped_rewards = mdp.get_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = GridWorld(width = 15, height = 12, goals = [((14,11), 1), ((13,11), -1)])\n",
    "qfunction = QTable()\n",
    "QLearning(mdp, EpsilonGreedy(), qfunction).execute(episodes=200)\n",
    "policy = QPolicy(qfunction)\n",
    "mdp.visualise_q_function(qfunction)\n",
    "mdp.visualise_policy(policy)\n",
    "q_learning_rewards = mdp.get_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot.plot_episode_length(\n",
    "    [\"Tabular Q-learning\", \"Reward shaping\"],\n",
    "    [q_learning_rewards, reward_shaped_rewards],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-value initialisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
