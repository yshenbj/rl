{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Planning & Online Planning for MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Value iteration is an offline planning method since it solves the problem offline for all possiable states and then uses the solution (a policy) online to act.\n",
    "\n",
    "* In online planning (like AlphaZero), planning is undertaken immediately before executing an action.\n",
    "    * For each state $s$ visited, the set of all available actions $A(s)$ partially evaluated.\n",
    "    * The quality of each action $a$ is approximated by averaging the expected reward of trajectories over $S$ obtained by repeated simulations, giving as an approximation for $Q(s,a)$.\n",
    "    * The chosen action is $\\text{argmax}_{a'} Q(s,a)$\n",
    "\n",
    "* In online planning, we need access to a simulator that approximates the transitions function $P_a(s' |s)$ and reward function $r$ of our MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic framkwork is to build up a tree using simulation. The states that have been evaluated are stored in a search tree. The set of evaluated states is **incementally** built be interation over the following four steps:\n",
    "\n",
    "-   **Select**: Select a single node in the tree that is **not fully expanded**. By this, we mean at least one of its children is not yet explored.\n",
    "    \n",
    "-   **Expand**: Expand this node by applying one available action (as defined by the MDP) from the node.\n",
    "    \n",
    "-   **Simulate**: From one of the outcomes of the expanded, perform a complete random simulation of the MDP to a terminating state. This therefore assumes that the simulation is finite, but versions of MCTS exist in which we just execute for some time and then estimate the outcome.\n",
    "    \n",
    "-   **Backpropagate**: Finally, the value of the node is backpropagated to the root node, updating the value of each ancestor node on the way using expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Selection**: The first loop progressively selects a branch in the tree using a multi-armed bandit algorithm using $Q(s,a)$. The outcome that occurs from an action is chosen according to $P(s' \\mid s)$ defined in the MDP.\n",
    "\n",
    "2. **Expansion**: Select an action $a$ to apply in  state $s$, either randomly or using an heuristic. Get an outcome state $s'$ from applying action $a$ in state $s$ according to the probability distribution $P(s' \\mid s)$. Expand a new environment node and a new state node for that outcome.\n",
    "\n",
    "3. **Simulation**: Perform a randomised simulation of the MDP until we reach a terminating state. That is, at each choice point, randomly select an possible action from the MDP, and use transition probabilities $P_a(s' \\mid s)$ to choose an outcome for each action. Heuristics can be used to improve the random simulation by guiding it towards more promising states. $G$ is the cumulative discounted reward received from the simulation starting at $s'$ until the simulation terminates. \n",
    "\n",
    "4. **Backpropagation**: The reward from the simulation is backpropagated from the selected node to its ancestors recursively. We must not forget the discount factor! For each state $s$ and action $a$ selected in the Select step, update the cumulative reward of that state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have run out of computational time, we select the action that maximises are expected return, which is simply the one with the highest Q-value from our simulations: \n",
    "\n",
    "$$\\text{argmax}_{a \\in A(s)} Q(s_0, a)$$ \n",
    "\n",
    "We execute that action and wait to see which outcome occurs for the action.\n",
    "\n",
    "Once we see the outcome state, which we will call $s'$, we start the process all over again, except with $s_0 \\leftarrow s'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \n",
    "    # Record a unique node id to distinguish duplicated states\n",
    "    next_node_id = 0\n",
    "    \n",
    "    # Records the number of times states have been visited\n",
    "    visits = defaultdict(lambda: 0)\n",
    "    \n",
    "    def __init__(self, mdp, parent, state, qfunction, bandit, reward=0.0, action=None):\n",
    "        self.mdp = mdp\n",
    "        self.parent = parent\n",
    "        self.state = state\n",
    "        self.id = Node.next_node_id\n",
    "        Node.next_node_id += 1\n",
    "        \n",
    "        # The Q function used to store state-action values\n",
    "        self.qfunction = qfunction\n",
    "        \n",
    "        # A multi-armed bandit for this node\n",
    "        self.bandit = bandit\n",
    "        \n",
    "        # The immediate reward reveived for reaching this state, used for backpropagation\n",
    "        self.reward = reward\n",
    "        \n",
    "        # The action that generated this node\n",
    "        self.action = action\n",
    "        \n",
    "    \"\"\" Select a node that is not fully expanded \"\"\"\n",
    "    \n",
    "    def select(self):\n",
    "        pass\n",
    "    \n",
    "    \"\"\" Expand a node if it is not a termianl node \"\"\"\n",
    "    \n",
    "    def expand(self):\n",
    "        pass\n",
    "    \n",
    "    \"\"\" Backpropagate the reward back to the parent node \"\"\"\n",
    "    \n",
    "    def back_propagate(self, reward, child):\n",
    "        pass\n",
    "    \n",
    "    \"\"\" Return the value of this node \"\"\"\n",
    "    \n",
    "    def get_value(self):\n",
    "        max_q_value = self.qfunction.get_max_q(\n",
    "            self.state, self.mdp.get_actions(self.state)\n",
    "        )\n",
    "        return max_q_value\n",
    "    \n",
    "    \"\"\" Get the number of visits to this state \"\"\"\n",
    "    \n",
    "    def get_visits(self):\n",
    "        return Node.visits[self.state]\n",
    "\n",
    "\n",
    "class SingleAgentNode(Node):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mdp,\n",
    "        parent,\n",
    "        state,\n",
    "        qfunction,\n",
    "        bandit,\n",
    "        reward=0.0,\n",
    "        action=None,\n",
    "    ):\n",
    "        super().__init__(mdp, parent, state, qfunction, bandit, reward, action)\n",
    "        \n",
    "        # A dictionary from actions to a set of node-probability pairs\n",
    "        self.children = {}\n",
    "        \n",
    "    \"\"\" Return true if and only if all child actions have been expanded \"\"\"\n",
    "    \n",
    "    def is_fully_expanded(self):\n",
    "        valid_actions = self.mdp.get_actions(self.state)\n",
    "        if len(valid_actions) == len(self.children):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \"\"\" Select a node that is not fully expanded \"\"\"\n",
    "    \n",
    "    def select(self):\n",
    "        if not self.is_fully_expanded() or self.mdp.is_terminal(self.state):\n",
    "            return self\n",
    "        else:\n",
    "            actions = list(self.children.keys())\n",
    "            action = self.bandit.select(self.state, actions, self.qfunction)\n",
    "            return self.get_outcome_child(action).select()\n",
    "    \n",
    "    \"\"\" Expand a node if it is not a terminal node \"\"\"\n",
    "    \n",
    "    def expand(self):\n",
    "        if not self.mdp.is_terminal(self.state):\n",
    "            # Randomly select an unexpanded action to expand\n",
    "            actions = self.mdp.get_actions(self.state) - self.children.keys()\n",
    "            action = random.choice(list(actions))\n",
    "            \n",
    "            self.children[action] = []\n",
    "            return self.get_outcome_child(actions)\n",
    "        return self\n",
    "    \n",
    "    \"\"\" Backpropagate the reward back to the parent node \"\"\"\n",
    "    \n",
    "    def back_propagate(self, reward, child):\n",
    "        action = child.action\n",
    "        \n",
    "        Node.visits[self.state] = Node.visits[self.state] + 1\n",
    "        Node.visits[(self.state, action)] = Node.visits[(self.state, action)] + 1\n",
    "        \n",
    "        q_value = self.qfunction.get_q_value(self.state, action)\n",
    "        delta = (1 / (Node.visits[(self.state, action)])) * (\n",
    "            reward - self.qfunction.get_q_value(self.state, action)\n",
    "        )\n",
    "        self.qfunction.update(self.state, action, delta)\n",
    "        \n",
    "        if self.parent != None:\n",
    "            self.parent.back_propagate(self.reward + reward, self)\n",
    "    \n",
    "    \"\"\" Simulate the outcome of an action, and return the child node \"\"\"\n",
    "    \n",
    "    def get_outcome_child(self, action):\n",
    "        # Choose one outcome based on transition probabilities\n",
    "        (next_state, reward, done) = self.mdp.execute(self.state, action)\n",
    "        \n",
    "        # Find the corresponding state and return if this already exists\n",
    "        for (child, _) in self.children[action]:\n",
    "            if next_state == child.state:\n",
    "                return child\n",
    "        \n",
    "        # This outcome has not occured from this state-action pair previously\n",
    "        new_child = SingleAgentNode(\n",
    "            self.mdp, self, next_state, self.qfunction, self.bandit, reward, action\n",
    "        )\n",
    "        \n",
    "        # Find the probability of this outcome (only possible for model-based) for visualising tree\n",
    "        probability = 0.0\n",
    "        for (outcome, probability) in self.mdp.get_transitions(self.state, action):\n",
    "            if outcome == next_state:\n",
    "                self.children[action] += [(new_child, probability)]\n",
    "                return new_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, mdp, qfunction, bandit):\n",
    "        self.mdp = mdp\n",
    "        self.qfunction = qfunction\n",
    "        self.bandit = bandit\n",
    "\n",
    "    \"\"\"\n",
    "    Execute the MCTS algorithm from ths initial state given, with timeout in seconds\n",
    "    \"\"\"\n",
    "    \n",
    "    def mcts(self, timeout=1, root_node=None):\n",
    "        if root_node is None:\n",
    "            root_node = self.create_root_node()\n",
    "            \n",
    "        start_time = time.time()\n",
    "        current_time = time.time()\n",
    "        while current_time < start_time + timeout:\n",
    "            \n",
    "            # Find a state node to expand\n",
    "            selected_node = root_node.select()\n",
    "            if not self.mdp.is_terminal(selected_node):\n",
    "                \n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
