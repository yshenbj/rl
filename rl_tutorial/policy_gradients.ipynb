{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "from rendering_utils import *\n",
    "from plot import Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    \"\"\"Return all states of this MDP\"\"\"\n",
    "\n",
    "    def get_states(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return all actions with non-zero probability from this state \"\"\"\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return all non-zero probability transitions for this action\n",
    "        from this state, as a list of (state, probability) pairs\n",
    "    \"\"\"\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return the reward for transitioning from state to\n",
    "        nextState via action\n",
    "    \"\"\"\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return true if and only if state is a terminal state of this MDP \"\"\"\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return the discount factor for this MDP \"\"\"\n",
    "\n",
    "    def get_discount_factor(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return the initial state of this MDP \"\"\"\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return all goal states of this MDP \"\"\"\n",
    "\n",
    "    def get_goal_states(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return a new state and a reward for executing action in state,\n",
    "    based on the underlying probability. This can be used for\n",
    "    model-free learning methods, but requires a model to operate.\n",
    "    Override for simulation-based learning\n",
    "    \"\"\"\n",
    "\n",
    "    def execute(self, state, action):\n",
    "        rand = random.random()\n",
    "        cumulative_probability = 0.0\n",
    "        for (new_state, probability) in self.get_transitions(state, action):\n",
    "            if cumulative_probability <= rand <= probability + cumulative_probability:\n",
    "                reward = self.get_reward(state, action, new_state)\n",
    "                return (new_state, reward, self.is_terminal(new_state))\n",
    "            cumulative_probability += probability\n",
    "            if cumulative_probability >= 1.0:\n",
    "                raise (\n",
    "                    \"Cumulative probability >= 1.0 for action \"\n",
    "                    + str(action)\n",
    "                    + \" from \"\n",
    "                    + str(state)\n",
    "                )\n",
    "\n",
    "        raise BaseException(\n",
    "            \"No outcome state in simulation for action \"\n",
    "            + str(action)\n",
    "            + \" from \"\n",
    "            + str(state)\n",
    "        )\n",
    "\n",
    "    \"\"\" \n",
    "    Execute a policy on this mdp for a number of episodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def execute_policy(self, policy, episodes=100, max_step=100):\n",
    "        cumulative_rewards = []\n",
    "        states = set()\n",
    "        for _ in range(episodes):\n",
    "            cumulative_reward = 0.0\n",
    "            state = self.get_initial_state()\n",
    "            step = 0\n",
    "            while not self.is_terminal(state):\n",
    "                actions = self.get_actions(state)\n",
    "                action = policy.select_action(state, actions)\n",
    "                (next_state, reward, done) = self.execute(state, action)\n",
    "                cumulative_reward += reward * (self.discount_factor ** step)\n",
    "                state = next_state\n",
    "                step += 1\n",
    "                if step > max_step:\n",
    "                    break\n",
    "            cumulative_rewards += [cumulative_reward]\n",
    "        return cumulative_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(MDP):\n",
    "    # labels for terminate action and terminal state\n",
    "    TERMINAL = (-1, -1)\n",
    "    TERMINATE = 0\n",
    "    LEFT = 1\n",
    "    UP = 2\n",
    "    RIGHT = 3\n",
    "    DOWN = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        noise=0.1,\n",
    "        width=4,\n",
    "        height=3,\n",
    "        discount_factor=0.9,\n",
    "        blocked_states=[(1, 1)],\n",
    "        action_cost=0.0,\n",
    "        initial_state=(0, 0),\n",
    "        goals=None,\n",
    "    ):\n",
    "        self.noise = noise\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.blocked_states = blocked_states\n",
    "        self.discount_factor = discount_factor\n",
    "        self.action_cost = action_cost\n",
    "        self.initial_state = initial_state\n",
    "        if goals is None:\n",
    "            self.goal_states = dict(\n",
    "                [((width - 1, height - 1), 1), ((width - 1, height - 2), -1)]\n",
    "            )\n",
    "        else:\n",
    "            self.goal_states = dict(goals)\n",
    "\n",
    "        # A list of lists that records all rewards given at each step\n",
    "        # for each episode of a simulated gridworld\n",
    "        self.rewards = []\n",
    "\n",
    "        # A list of cumulative rewards for each episode\n",
    "        self.cumulative_rewards = []\n",
    "    \n",
    "        # The rewards for the current episode\n",
    "        self.episode_rewards = []\n",
    "\n",
    "\n",
    "    def get_states(self):\n",
    "        states = [self.TERMINAL]\n",
    "        for x in range(self.width):\n",
    "            for y in range(self.height):\n",
    "                if not (x, y) in self.blocked_states:\n",
    "                    states.append((x, y))\n",
    "        return states\n",
    "\n",
    "    def get_actions(self, state=None):\n",
    "\n",
    "        actions = [self.TERMINATE, self.LEFT, self.UP, self.RIGHT, self.DOWN]\n",
    "        if state is None:\n",
    "            return actions\n",
    "\n",
    "        valid_actions = []\n",
    "        for action in actions:\n",
    "            for (new_state, probability) in self.get_transitions(state, action):\n",
    "                if probability > 0:\n",
    "                    valid_actions.append(action)\n",
    "                    break\n",
    "        return valid_actions\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        self.episode_rewards = []\n",
    "        return self.initial_state\n",
    "\n",
    "    def get_goal_states(self):\n",
    "        return self.goal_states\n",
    "\n",
    "    def valid_add(self, state, new_state, probability):\n",
    "        # If the next state is blocked, stay in the same state\n",
    "        if probability == 0.0:\n",
    "            return []\n",
    "\n",
    "        if new_state in self.blocked_states:\n",
    "            return [(state, probability)]\n",
    "\n",
    "        # Move to the next space if it is not off the grid\n",
    "        (x, y) = new_state\n",
    "        if x >= 0 and x < self.width and y >= 0 and y < self.height:\n",
    "            return [((x, y), probability)]\n",
    "\n",
    "        # If off the grid, state in the same state\n",
    "        return [(state, probability)]\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        transitions = []\n",
    "\n",
    "        if state == self.TERMINAL:\n",
    "            if action == self.TERMINATE:\n",
    "                return [(self.TERMINAL, 1.0)]\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        # Probability of not slipping left or right\n",
    "        straight = 1 - (2 * self.noise)\n",
    "\n",
    "        (x, y) = state\n",
    "        if state in self.get_goal_states().keys():\n",
    "            if action == self.TERMINATE:\n",
    "                transitions += [(self.TERMINAL, 1.0)]\n",
    "\n",
    "        elif action == self.UP:\n",
    "            transitions += self.valid_add(state, (x, y + 1), straight)\n",
    "            transitions += self.valid_add(state, (x - 1, y), self.noise)\n",
    "            transitions += self.valid_add(state, (x + 1, y), self.noise)\n",
    "\n",
    "        elif action == self.DOWN:\n",
    "            transitions += self.valid_add(state, (x, y - 1), straight)\n",
    "            transitions += self.valid_add(state, (x - 1, y), self.noise)\n",
    "            transitions += self.valid_add(state, (x + 1, y), self.noise)\n",
    "\n",
    "        elif action == self.RIGHT:\n",
    "            transitions += self.valid_add(state, (x + 1, y), straight)\n",
    "            transitions += self.valid_add(state, (x, y - 1), self.noise)\n",
    "            transitions += self.valid_add(state, (x, y + 1), self.noise)\n",
    "\n",
    "        elif action == self.LEFT:\n",
    "            transitions += self.valid_add(state, (x - 1, y), straight)\n",
    "            transitions += self.valid_add(state, (x, y - 1), self.noise)\n",
    "            transitions += self.valid_add(state, (x, y + 1), self.noise)\n",
    "\n",
    "        # Merge any duplicate outcomes\n",
    "        merged = defaultdict(lambda: 0.0)\n",
    "        for (state, probability) in transitions:\n",
    "            merged[state] = merged[state] + probability\n",
    "\n",
    "        transitions = []\n",
    "        for outcome in merged.keys():\n",
    "            transitions += [(outcome, merged[outcome])]\n",
    "\n",
    "        return transitions\n",
    "\n",
    "    def get_reward(self, state, action, new_state):\n",
    "        reward = 0.0\n",
    "        if state in self.get_goal_states().keys() and new_state == self.TERMINAL:\n",
    "            reward = self.get_goal_states().get(state)\n",
    "        else:\n",
    "            reward = self.action_cost\n",
    "        step = len(self.episode_rewards)\n",
    "        self.episode_rewards += [reward * (self.discount_factor ** step)]\n",
    "        return reward\n",
    "\n",
    "    def get_discount_factor(self):\n",
    "        return self.discount_factor\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        if state == self.TERMINAL:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    \"\"\"\n",
    "        Returns a list of lists, which records all rewards given at each step\n",
    "        for each episode of a simulated gridworld\n",
    "    \"\"\"\n",
    "\n",
    "    def get_rewards(self):\n",
    "        return self.rewards\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "        Returns a list of all cumulative rewards\n",
    "        for each episode of a simulated gridworld\n",
    "    \"\"\"\n",
    "\n",
    "    def get_cumulative_rewards(self):\n",
    "        return self.cumulative_rewards\n",
    "\n",
    "    \"\"\"\n",
    "        Create a gridworld from an array of strings: one for each line\n",
    "        - First line is rewards as a dictionary from cell to value: {'A': 1, ...}\n",
    "        - space is an empty cell\n",
    "        - # is a blocked cell\n",
    "        - @ is the agent (initial state)\n",
    "        - new 'line' is a new row\n",
    "        - a letter is a cell with a reward for transitioning\n",
    "          into that cell. The reward defined by the first line.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create(string):\n",
    "        # Parse the reward on the first line\n",
    "        import ast\n",
    "\n",
    "        rewards = ast.literal_eval(string[0])\n",
    "\n",
    "        width = 0\n",
    "        height = len(string) - 1\n",
    "\n",
    "        blocked_cells = []\n",
    "        initial_state = (0, 0)\n",
    "        goals = []\n",
    "        row = 0\n",
    "        for next_row in string[1:]:\n",
    "            column = 0\n",
    "            for cell in next_row:\n",
    "                if cell == \"#\":\n",
    "                    blocked_cells += [(column, row)]\n",
    "                elif cell == \"@\":\n",
    "                    initial_state = (column, row)\n",
    "                elif cell.isalpha():\n",
    "                    goals += [((column, row), rewards[cell])]\n",
    "                column += 1\n",
    "            width = max(width, column)\n",
    "            row += 1\n",
    "        return GridWorld(\n",
    "            width=width,\n",
    "            height=height,\n",
    "            blocked_states=blocked_cells,\n",
    "            initial_state=initial_state,\n",
    "            goals=goals,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def open(file):\n",
    "        file = open(file, \"r\")\n",
    "        string = file.read().splitlines()\n",
    "        file.close()\n",
    "        return GridWorld.create(string)\n",
    "\n",
    "    @staticmethod\n",
    "    def matplotlib_installed():\n",
    "        try:\n",
    "            import matplotlib as mpl\n",
    "            import matplotlib.pyplot as plt\n",
    "            return True\n",
    "        except ModuleNotFoundError:\n",
    "            return False\n",
    "\n",
    "    \"\"\" Visualise a Grid World problem \"\"\"\n",
    "\n",
    "    def visualise(self, agent_position=None, title=\"\", grid_size=1.0, gif=False):\n",
    "        if self.matplotlib_installed():\n",
    "            return self.visualise_as_image(agent_position=agent_position, title=title, grid_size=grid_size, gif=gif)\n",
    "        else:\n",
    "            print(self.to_string(title=title))\n",
    "\n",
    "    \"\"\" Visualise a Grid World value function \"\"\"\n",
    "    def visualise_value_function(self, value_function, title=\"\", grid_size=1.0, gif=False):\n",
    "        if self.matplotlib_installed():\n",
    "            return self.visualise_value_function_as_image(value_function, title=title, grid_size=grid_size, gif=gif)\n",
    "        else:\n",
    "            print(self.value_function_to_string(value_function, title=title))\n",
    "\n",
    "    def visualise_q_function(self, qfunction, title=\"\", grid_size=1.5, gif=False):\n",
    "        if self.matplotlib_installed():\n",
    "            return self.visualise_q_function_as_image(qfunction, title=title, grid_size=grid_size, gif=gif)\n",
    "        else:\n",
    "            print(self.q_function_to_string(qfunction, title=title))\n",
    "\n",
    "    def visualise_policy(self, policy, title=\"\", grid_size=1.0, gif=False):\n",
    "        if self.matplotlib_installed():\n",
    "            return self.visualise_policy_as_image(policy, title=title, grid_size=grid_size, gif=gif)\n",
    "        else:\n",
    "            print(self.policy_to_string(policy, title=title))\n",
    "\n",
    "    def visualise_stochastic_policy(self, policy, title=\"\", grid_size=1.0, gif=False):\n",
    "        if self.matplotlib_installed():\n",
    "            return self.visualise_stochastic_policy_as_image(policy, title=title, grid_size=grid_size, gif=gif)\n",
    "        else:\n",
    "            # TODO make a stochastic policy to string\n",
    "            pass\n",
    "\n",
    "    \"\"\" Visualise a grid world problem as a formatted string \"\"\"\n",
    "    def to_string(self, title=\"\"):\n",
    "        left_arrow = \"\\u25C4\"\n",
    "        up_arrow = \"\\u25B2\"\n",
    "        right_arrow = \"\\u25BA\"\n",
    "        down_arrow = \"\\u25BC\"\n",
    "\n",
    "\n",
    "        space = \" |              \"\n",
    "        block = \" | #############\"\n",
    "\n",
    "        line = \"  \"\n",
    "        for x in range(self.width):\n",
    "            line += \"--------------- \"\n",
    "        line += \"\\n\"\n",
    "\n",
    "        result = \" \" + title + \"\\n\"\n",
    "        result += line\n",
    "        for y in range(self.height - 1, -1, -1):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.get_goal_states().keys():\n",
    "                    result += space\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                else:\n",
    "                    result += \" |       {}      \".format(up_arrow)\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) == self.get_initial_state():\n",
    "                    result += \" |     _____    \"\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                else:\n",
    "                    result += space\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) == self.get_initial_state():\n",
    "                    result += \" |    ||o  o|   \"\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                else:\n",
    "                    result += space\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) == self.get_initial_state():\n",
    "                    result += \" | {}  ||  * |  {}\".format(left_arrow, right_arrow)\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                elif (x, y) in self.get_goal_states().keys():\n",
    "                    result += \" |     {:+0.2f}    \".format(\n",
    "                        self.get_goal_states()[(x, y)]\n",
    "                    )\n",
    "                else:\n",
    "                    result += \" | {}           {}\".format(left_arrow, right_arrow)\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) == self.get_initial_state():\n",
    "                    result += \" |    ||====|   \".format(left_arrow, right_arrow)\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                else:\n",
    "                    result += space\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) == self.get_initial_state():\n",
    "                    result += \" |     -----    \"\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                else:\n",
    "                    result += space\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.get_goal_states().keys():\n",
    "                    result += space\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                else:\n",
    "                    result += \" |       {}      \".format(down_arrow)\n",
    "            result += \" |\\n\"\n",
    "            result += line\n",
    "        return result\n",
    "\n",
    "    \"\"\" Convert a grid world value function to a formatted string \"\"\"\n",
    "\n",
    "    def value_function_to_string(self, values, title=\"\"):\n",
    "        line = \" {:-^{n}}\\n\".format(\"\", n=len(\" | +0.00\") * self.width + 1)\n",
    "        result = \" \" + title + \"\\n\"\n",
    "        result += line\n",
    "        for y in range(self.height - 1, -1, -1):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states:\n",
    "                    result += \" | #####\"\n",
    "                else:\n",
    "                    result += \" | {:+0.2f}\".format(values.get_value((x, y)))\n",
    "            result += \" |\\n\"\n",
    "            result += line\n",
    "\n",
    "        return result\n",
    "\n",
    "    \"\"\" Convert a grid world Q function to a formatted string \"\"\"\n",
    "\n",
    "    def q_function_to_string(self, qfunction, title=\"\"):\n",
    "        left_arrow = \"\\u25C4\"\n",
    "        up_arrow = \"\\u25B2\"\n",
    "        right_arrow = \"\\u25BA\"\n",
    "        down_arrow = \"\\u25BC\"\n",
    "\n",
    "        space = \" |               \"\n",
    "\n",
    "        line = \"  \"\n",
    "        for x in range(self.width):\n",
    "            line += \"---------------- \"\n",
    "        line += \"\\n\"\n",
    "\n",
    "        result = \" \" + title + \"\\n\"\n",
    "        result += line\n",
    "        for y in range(self.height - 1, -1, -1):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states or (\n",
    "                    x,\n",
    "                    y,\n",
    "                ) in self.get_goal_states().keys():\n",
    "                    result += space\n",
    "                else:\n",
    "                    result += \" |       {}       \".format(up_arrow)\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states or (\n",
    "                    x,\n",
    "                    y,\n",
    "                ) in self.get_goal_states().keys():\n",
    "                    result += space\n",
    "                else:\n",
    "                    result += \" |     {:+0.2f}     \".format(\n",
    "                        qfunction.get_q_value((x, y), self.UP)\n",
    "                    )\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                result += space\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states:\n",
    "                    result += \" |     #####     \"\n",
    "                elif (x, y) in self.get_goal_states().keys():\n",
    "                    result += \" |     {:+0.2f}     \".format(\n",
    "                        self.get_goal_states()[(x, y)]\n",
    "                    )\n",
    "                else:\n",
    "                    result += \" | {}{:+0.2f}  {:+0.2f}{}\".format(\n",
    "                        left_arrow,\n",
    "                        qfunction.get_q_value((x, y), self.LEFT),\n",
    "                        qfunction.get_q_value((x, y), self.RIGHT),\n",
    "                        right_arrow,\n",
    "                    )\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                result += space\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states or (\n",
    "                    x,\n",
    "                    y,\n",
    "                ) in self.get_goal_states().keys():\n",
    "                    result += space\n",
    "                else:\n",
    "                    result += \" |     {:+0.2f}     \".format(\n",
    "                        qfunction.get_q_value((x, y), self.DOWN)\n",
    "                    )\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states or (\n",
    "                    x,\n",
    "                    y,\n",
    "                ) in self.get_goal_states().keys():\n",
    "                    result += space\n",
    "                else:\n",
    "                    result += \" |       {}       \".format(down_arrow)\n",
    "            result += \" |\\n\"\n",
    "            result += line\n",
    "        return result\n",
    "\n",
    "    \"\"\" Convert a grid world policy to a formatted string \"\"\"\n",
    "    def policy_to_string(self, policy, title=\"\"):\n",
    "        arrow_map = {self.UP:'\\u25B2',\n",
    "                     self.DOWN:'\\u25BC',\n",
    "                     self.LEFT:'\\u25C4',\n",
    "                     self.RIGHT:'\\u25BA',\n",
    "                    }\n",
    "        line = \" {:-^{n}}\\n\".format(\"\", n=len(\" |  N \") * self.width + 1)\n",
    "        result = \" \" + title + \"\\n\"\n",
    "        result += line\n",
    "        for y in range(self.height - 1, -1, -1):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states:\n",
    "                    result += \" | ###\"\n",
    "                elif policy.select_action((x, y), self.get_actions((x, y))) == self.TERMINATE:\n",
    "                    result += \" | {:+0d} \".format(self.goal_states[(x, y)])\n",
    "                else:\n",
    "                    result += \" |  \" + arrow_map[policy.select_action((x, y), self.get_actions((x, y)))] + \" \"\n",
    "            result += \" |\\n\"\n",
    "            result += line\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    \"\"\" Initialise a gridworld grid \"\"\"\n",
    "    def initialise_grid(self, grid_size=1.0):\n",
    "        fig = plt.figure(figsize=(self.width * grid_size, self.height * grid_size))\n",
    "\n",
    "        # Trim whitespace \n",
    "        plt.subplots_adjust(top=0.92, bottom=0.01, right=1, left=0, hspace=0, wspace=0)\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "        # Initialise the map to all white\n",
    "        img = [[COLOURS['white'] for _ in range(self.width)] for _ in range(self.height)]\n",
    "\n",
    "        # Render the grid\n",
    "        for y in range(0, self.height):\n",
    "            for x in range(0, self.width):\n",
    "                if (x, y) in self.goal_states:\n",
    "                    img[y][x] = COLOURS['red'] if self.goal_states[(x, y)] < 0 else COLOURS['green']\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    img[y][x] = COLOURS['grey']\n",
    "\n",
    "        ax.xaxis.set_ticklabels([])  # clear x tick labels\n",
    "        ax.axes.yaxis.set_ticklabels([])  # clear y tick labels\n",
    "        ax.tick_params(which='both', top=False, left=False, right=False, bottom=False)\n",
    "        ax.set_xticks([w - 0.5 for w in range(0, self.width, 1)])\n",
    "        ax.set_yticks([h - 0.5 for h in range(0, self.height, 1)])\n",
    "        ax.grid(color='lightgrey')\n",
    "        return fig, ax, img\n",
    "\n",
    "    \"\"\" visualise the gridworld problem as a matplotlib image \"\"\"\n",
    "\n",
    "    def visualise_as_image(self, agent_position=None, title=\"\", grid_size=1.0, gif=False):\n",
    "        fig, ax, img = self.initialise_grid(grid_size=grid_size)\n",
    "        current_position = (\n",
    "            self.get_initial_state() if agent_position is None else agent_position\n",
    "        )\n",
    "\n",
    "        # Render the grid\n",
    "        for y in range(0, self.height):\n",
    "            for x in range(0, self.width):\n",
    "                if (x, y) == current_position:\n",
    "                    ax.scatter(x, y, s=2000, marker='o', edgecolors='none')\n",
    "                elif (x, y) in self.goal_states:\n",
    "                    plt.text(\n",
    "                        x,\n",
    "                        y,\n",
    "                        f\"{self.get_goal_states()[(x, y)]:+0.2f}\",\n",
    "                        fontsize=\"x-large\",\n",
    "                        horizontalalignment=\"center\",\n",
    "                        verticalalignment=\"center\",\n",
    "                    )\n",
    "        im = plt.imshow(img, origin=\"lower\")\n",
    "        plt.title(title)\n",
    "        if gif:\n",
    "            return fig, ax, im\n",
    "        else:\n",
    "            return fig\n",
    "\n",
    "    \"\"\"Render each tile individually depending on the current state of the cell\"\"\"\n",
    "\n",
    "    def render_tile(self, x, y, tile_size, img, tile_type=None):\n",
    "        ymin = y * tile_size\n",
    "        ymax = (y + 1) * tile_size\n",
    "        xmin = x * tile_size\n",
    "        xmax = (x + 1) * tile_size\n",
    "\n",
    "        for i in range(ymin, ymax):\n",
    "            for j in range(xmin, xmax):\n",
    "                if i == ymin or i == ymax - 1 or j == xmin or j == xmax + 1:\n",
    "                    draw_grid_lines(i, j, img)\n",
    "                else:\n",
    "                    if tile_type == \"goal\":\n",
    "                        render_goal(\n",
    "                            i,\n",
    "                            j,\n",
    "                            img,\n",
    "                            reward=self.goal_states[(x, y)],\n",
    "                            reward_max=max(self.get_goal_states().values()),\n",
    "                            reward_min=min(self.get_goal_states().values()),\n",
    "                        )\n",
    "                    elif tile_type == \"blocked\":\n",
    "                        render_blocked_tile(i, j, img)\n",
    "                    elif tile_type == \"agent\":\n",
    "                        render_agent(\n",
    "                            i,\n",
    "                            j,\n",
    "                            img,\n",
    "                            center_x=xmin + tile_size / 2,\n",
    "                            center_y=ymin + tile_size / 2,\n",
    "                            radius=tile_size / 4,\n",
    "                        )\n",
    "                    elif tile_type == \"empty\":\n",
    "                        img[i][j] = [255, 255, 255]\n",
    "                    else:\n",
    "                        raise ValueError(\"Invalid tile type\")\n",
    "\n",
    "    \"\"\" Visualise the value function \"\"\"\n",
    "\n",
    "    def visualise_value_function_as_image(self, value_function, title=\"\", grid_size=1.0, gif=False):\n",
    "        if not gif:\n",
    "            fig, ax, img = self.initialise_grid(grid_size=grid_size)\n",
    "        texts = []\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                value = value_function.get_value((x, y))\n",
    "                if (x, y) not in self.blocked_states:\n",
    "                    text = plt.text(\n",
    "                        x,\n",
    "                        y,\n",
    "                        f\"{float(value):+0.2f}\",\n",
    "                        fontsize=\"medium\",\n",
    "                        horizontalalignment=\"center\",\n",
    "                        verticalalignment=\"center\",\n",
    "                        color='lightgrey' if value == 0.0 else 'black',\n",
    "                    )\n",
    "                    texts.append(text)\n",
    "        if gif:\n",
    "            return texts\n",
    "        else:\n",
    "            ax.imshow(img, origin=\"lower\")\n",
    "            plt.title(title, fontsize=\"large\")\n",
    "            plt.show()\n",
    "\n",
    "    \"\"\" Visualise the value function using a heat-map where green is high value and\n",
    "    red is low value\n",
    "    \"\"\"\n",
    "\n",
    "    def visualise_value_function_as_heatmap(self, value_function, title=\"\"):\n",
    "        values = [[0 for _ in range(self.width)] for _ in range(self.height)]\n",
    "        fig, ax = self.initialise_grid()\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states:\n",
    "                    plt.text(\n",
    "                        x,\n",
    "                        y,\n",
    "                        \"#\",\n",
    "                        horizontalalignment=\"center\",\n",
    "                        verticalalignment=\"center\",\n",
    "                    )\n",
    "                else:\n",
    "                    values[y][x] = value_function.get_value((x, y))\n",
    "                    plt.text(\n",
    "                        x,\n",
    "                        y,\n",
    "                        f\"{values[y][x]:.2f}\",\n",
    "                        horizontalalignment=\"center\",\n",
    "                        verticalalignment=\"center\",\n",
    "                    )\n",
    "        plt.imshow(values, origin=\"lower\", cmap=make_red_white_green_cmap())\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    \"\"\" Visualise the Q-function with matplotlib \"\"\"\n",
    "\n",
    "    def visualise_q_function_as_image(self, qfunction, title=\"\", grid_size=1.5, gif=False):\n",
    "        if not gif:\n",
    "            fig, ax, img = self.initialise_grid(grid_size=grid_size)\n",
    "        texts = []\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.goal_states:\n",
    "                    # gif player handles goal state rendering\n",
    "                    if not gif:\n",
    "                        texts.append(plt.text(\n",
    "                            x,\n",
    "                            y,\n",
    "                            f\"{self.get_goal_states()[(x,y)]:+0.2f}\",\n",
    "                            fontsize=\"large\",\n",
    "                            horizontalalignment=\"center\",\n",
    "                            verticalalignment=\"center\",\n",
    "                        ))\n",
    "                elif (x, y) not in self.blocked_states:\n",
    "                    up_value = qfunction.get_q_value((x, y), self.UP)\n",
    "                    down_value = qfunction.get_q_value((x, y), self.DOWN)\n",
    "                    left_value = qfunction.get_q_value((x, y), self.LEFT)\n",
    "                    right_value = qfunction.get_q_value((x, y), self.RIGHT)\n",
    "                    texts.append(plt.text(\n",
    "                        x,\n",
    "                        y + 0.35,\n",
    "                        f\"{up_value:+0.2f}\",\n",
    "                        fontsize=\"medium\",\n",
    "                        horizontalalignment=\"center\",\n",
    "                        verticalalignment=\"top\",\n",
    "                        color='lightgrey' if up_value == 0.0 else 'black',\n",
    "                    ))\n",
    "                    texts.append(plt.text(\n",
    "                        x,\n",
    "                        y - 0.35,\n",
    "                        f\"{down_value:+0.2f}\",\n",
    "                        fontsize=\"medium\",\n",
    "                        horizontalalignment=\"center\",\n",
    "                        verticalalignment=\"bottom\",\n",
    "                        color='lightgrey' if down_value == 0.0 else 'black',\n",
    "                    ))\n",
    "                    texts.append(plt.text(\n",
    "                        x - 0.45,\n",
    "                        y,\n",
    "                        f\"{left_value:+0.2f}\",\n",
    "                        fontsize=\"medium\",\n",
    "                        horizontalalignment=\"left\",\n",
    "                        verticalalignment=\"center\",\n",
    "                        color='lightgrey' if left_value == 0.0 else 'black'\n",
    "                    ))\n",
    "                    texts.append(plt.text(\n",
    "                        x + 0.45,\n",
    "                        y,\n",
    "                        f\"{right_value:+0.2f}\",\n",
    "                        fontsize=\"medium\",\n",
    "                        horizontalalignment=\"right\",\n",
    "                        verticalalignment=\"center\",\n",
    "                        color='lightgrey' if right_value == 0.0 else 'black'\n",
    "                    ))\n",
    "                    plt.plot([x-0.5, x+0.5], [y-0.5, y+0.5], ls='-', lw=1, color='lightgrey')\n",
    "                    plt.plot([x + 0.5, x - 0.5], [y - 0.5, y + 0.5], ls='-', lw=1, color='lightgrey')\n",
    "        if gif:\n",
    "            return texts\n",
    "        ax.imshow(img, origin=\"lower\")\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    \"\"\" Visualise the Q-function with a matplotlib visual\"\"\"\n",
    "\n",
    "    def visualise_q_function_rendered(self, q_values, title=\"\", tile_size=32, show_text=False):\n",
    "        width_px = self.width * tile_size\n",
    "        height_px = self.height * tile_size\n",
    "        img = [[[0, 0, 0] for _ in range(width_px)] for _ in range(height_px)]\n",
    "\n",
    "        # provide these to scale the colours between the highest and lowest value\n",
    "        reward_max = max(self.get_goal_states().values())\n",
    "        reward_min = min(self.get_goal_states().values())\n",
    "        # Render the grid\n",
    "        for y in range(0, self.height):\n",
    "            for x in range(0, self.width):\n",
    "                # Draw in the blocked states as a black and white mesh\n",
    "                if (x, y) in self.blocked_states:\n",
    "                    render_full_blocked_tile(\n",
    "                        x * tile_size, y * tile_size, tile_size, img\n",
    "                    )\n",
    "                    continue\n",
    "                # Draw goal states\n",
    "                if (x, y) in self.goal_states:\n",
    "                    render_full_goal_tile(\n",
    "                        x * tile_size,\n",
    "                        y * tile_size,\n",
    "                        tile_size,\n",
    "                        img,\n",
    "                        reward=self.goal_states[(x, y)],\n",
    "                        rewardMax=reward_max,\n",
    "                        rewardMin=reward_min,\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                # Draw the action value for action available in each cell\n",
    "                # Break the grid up into 4 sections, using triangles that meet\n",
    "                # in the middle. The base of the triangle points toward the\n",
    "                # direction of the action\n",
    "                render_action_q_value(\n",
    "                    tile_size,\n",
    "                    x,\n",
    "                    y,\n",
    "                    self.UP,\n",
    "                    q_values,\n",
    "                    img,\n",
    "                    show_text,\n",
    "                    v_text_offset=8,\n",
    "                    rewardMax=reward_max,\n",
    "                    rewardMin=reward_min,\n",
    "                )\n",
    "                render_action_q_value(\n",
    "                    tile_size,\n",
    "                    x,\n",
    "                    y,\n",
    "                    self.DOWN,\n",
    "                    q_values,\n",
    "                    img,\n",
    "                    show_text,\n",
    "                    v_text_offset=-8,\n",
    "                    rewardMax=reward_max,\n",
    "                    rewardMin=reward_min,\n",
    "                )\n",
    "                render_action_q_value(\n",
    "                    tile_size,\n",
    "                    x,\n",
    "                    y,\n",
    "                    self.LEFT,\n",
    "                    q_values,\n",
    "                    img,\n",
    "                    show_text,\n",
    "                    h_text_offset=-8,\n",
    "                    rewardMax=reward_max,\n",
    "                    rewardMin=reward_min,\n",
    "                )\n",
    "                render_action_q_value(\n",
    "                    tile_size,\n",
    "                    x,\n",
    "                    y,\n",
    "                    self.RIGHT,\n",
    "                    q_values,\n",
    "                    img,\n",
    "                    show_text,\n",
    "                    h_text_offset=8,\n",
    "                    rewardMax=reward_max,\n",
    "                    rewardMin=reward_min,\n",
    "                )\n",
    "\n",
    "        ax.imshow(img, origin=\"lower\", interpolation=\"bilinear\")\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    \"\"\" Visualise the policy of the agent with a matplotlib visual \"\"\"\n",
    "\n",
    "    def visualise_policy_as_image(self, policy, title=\"\", grid_size=1.0, gif=False):\n",
    "        # Map from action names to prettier arrows\n",
    "        arrow_map = {self.UP:'\\u2191',\n",
    "                     self.DOWN:'\\u2193',\n",
    "                     self.LEFT:'\\u2190',\n",
    "                     self.RIGHT:'\\u2192',\n",
    "                    }\n",
    "        if not gif:\n",
    "            fig, ax, img = self.initialise_grid(grid_size=grid_size)\n",
    "        texts = []\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) not in self.blocked_states and (x, y) not in self.goal_states:\n",
    "                    if policy.select_action((x, y), self.get_actions((x, y))) != self.TERMINATE:\n",
    "                        action = arrow_map[policy.select_action((x, y), self.get_actions((x, y)))]\n",
    "                        fontsize = \"xx-large\"\n",
    "                    texts.append(plt.text(\n",
    "                                x,\n",
    "                                y,\n",
    "                                action,\n",
    "                                fontsize=fontsize,\n",
    "                                horizontalalignment=\"center\",\n",
    "                                verticalalignment=\"center\",\n",
    "                            ))\n",
    "                elif (x, y) in self.goal_states:\n",
    "                    # gif player handles goal state rendering\n",
    "                    if not gif:\n",
    "                        plt.text(\n",
    "                            x,\n",
    "                            y,\n",
    "                            f\"{self.get_goal_states()[(x, y)]:+0.2f}\",\n",
    "                            fontsize=\"x-large\",\n",
    "                            horizontalalignment=\"center\",\n",
    "                            verticalalignment=\"center\",\n",
    "                        )\n",
    "        if gif:\n",
    "            return texts\n",
    "        ax.imshow(img, origin=\"lower\")\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    def execute(self, state, action):\n",
    "        if state in self.goal_states:\n",
    "            self.rewards += [self.episode_rewards]\n",
    "            self.cumulative_rewards += [sum(self.episode_rewards)]\n",
    "            return MDP.execute(self, state=state, action=self.TERMINATE)\n",
    "        return super().execute(state, action)\n",
    "\n",
    "    def visualise_stochastic_policy_as_image(self, policy, title=\"\", grid_size=1.0, gif=False):\n",
    "        if not gif:\n",
    "            fig, ax, img = self.initialise_grid(grid_size=grid_size)\n",
    "        texts = []\n",
    "\n",
    "        # Render the grid\n",
    "        for y in range(0, self.height):\n",
    "            for x in range(0, self.width):\n",
    "                prob_up = 0.0\n",
    "                prob_down = 0.0\n",
    "                prob_left = policy.get_probability((x, y), self.LEFT)\n",
    "                prob_right = policy.get_probability((x, y), self.RIGHT)\n",
    "                if self.height > 1:\n",
    "                    prob_up = policy.get_probability((x, y), self.UP)\n",
    "                    prob_down = policy.get_probability((x, y), self.DOWN)\n",
    "                # Normalise to account for the 'terminate' action that is not visualised\n",
    "                total = prob_left + prob_right + prob_down + prob_up\n",
    "                if total != 0:\n",
    "                    prob_left = prob_left / total\n",
    "                    prob_right = prob_right / total\n",
    "                    prob_down = prob_down / total\n",
    "                    prob_up = prob_up / total\n",
    "                if (x, y) in self.goal_states:\n",
    "                    # gif player handles goal state rendering\n",
    "                    if not gif:\n",
    "                        plt.text(\n",
    "                            x,\n",
    "                            y,\n",
    "                            f\"{self.get_goal_states()[(x, y)]:+0.2f}\",\n",
    "                            fontsize=\"x-large\",\n",
    "                            horizontalalignment=\"center\",\n",
    "                            verticalalignment=\"center\",\n",
    "                        )\n",
    "                elif (x, y) not in self.blocked_states:\n",
    "                    left_triangle = '\\u25C4'\n",
    "                    up_triangle = '\\u25B2'\n",
    "                    right_triangle = '\\u25BA'\n",
    "                    down_triangle = '\\u25BC'\n",
    "                    if self.height > 1:\n",
    "                        texts.append(plt.text(\n",
    "                            x,\n",
    "                            y,\n",
    "                            f\"{prob_up:0.2f}\\n{up_triangle}\\n{prob_left:0.2f}{left_triangle} {right_triangle}{prob_right:0.2f}\\n{down_triangle}\\n{prob_down:0.2f}\",\n",
    "                            fontsize=\"medium\",\n",
    "                            horizontalalignment=\"center\",\n",
    "                            verticalalignment=\"center\",\n",
    "                        ))\n",
    "                    else:\n",
    "                        texts.append(plt.text(\n",
    "                            x,\n",
    "                            y,\n",
    "                            f\"{prob_left:0.2f}{left_triangle} {right_triangle}{prob_right:0.2f}\",\n",
    "                            fontsize=\"medium\",\n",
    "                            horizontalalignment=\"center\",\n",
    "                            verticalalignment=\"center\",\n",
    "                        ))\n",
    "        if gif:\n",
    "            return texts\n",
    "        ax.imshow(img, origin=\"lower\")\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REDINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient:\n",
    "    def __init__(self, mdp, policy):\n",
    "        super().__init__()\n",
    "        self.mdp = mdp\n",
    "        self.policy = policy\n",
    "    \n",
    "    \"\"\" Generate and store an entire episode trajectory to use to updsate the policy \"\"\"\n",
    "    \n",
    "    def execute(self, episodes=100, max_episode_length=float('inf')):\n",
    "        total_steps = 0\n",
    "        random_steps = 50\n",
    "        episode_rewards = []\n",
    "        for episode in range(episodes):\n",
    "            actions = []\n",
    "            states = []\n",
    "            rewards = []\n",
    "            \n",
    "            state = self.mdp.get_initial_state()\n",
    "            episode_reward = 0.0\n",
    "            for step in count():\n",
    "                if total_steps < random_steps:\n",
    "                    action = random.choice(self.mdp.get_actions(state))\n",
    "                else:\n",
    "                    action = self.policy.select_action(state, self.mdp.get_actions(state))\n",
    "                (next_state, reward, done) = self.mdp.execute(state, action)\n",
    "                \n",
    "                # Store the information from this step of trajectory\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                state = next_state\n",
    "                episode_reward += reward * (self.mdp.discount_factor ** step)\n",
    "                total_steps += 1\n",
    "                \n",
    "                if done or step == max_episode_length:\n",
    "                    break\n",
    "            \n",
    "            deltas = self.calculate_deltas(rewards)\n",
    "            \n",
    "            self.policy.update(states, actions, deltas)\n",
    "            episode_rewards.append(episode_rewards)\n",
    "        \n",
    "        return episode_rewards\n",
    "    \n",
    "    def calculate_deltas(self, rewards):\n",
    "        \"\"\"\n",
    "        Generate a list of the discounted future rewards at each step of an episode\n",
    "        Note that discounted_reward[T-2] = rewards[T-1] + discounted_reward[T-1] * gamma.\n",
    "        We can use that pattern to populate the discounted_rewards array.\n",
    "        \"\"\"\n",
    "        T = len(rewards)\n",
    "        discounted_future_rewards = [0 for _ in range(T)]\n",
    "\n",
    "        # The final discounted reward is the reward you get at that step\n",
    "        discounted_future_rewards[T - 1] = rewards[T - 1]\n",
    "        for t in reversed(range(0, T - 1)):\n",
    "            discounted_future_rewards[t] = (\n",
    "                rewards[t]\n",
    "                + discounted_future_rewards[t + 1] * self.mdp.get_discount_factor()\n",
    "            )\n",
    "        deltas = []\n",
    "        for t in range(len(discounted_future_rewards)):\n",
    "            deltas += [\n",
    "                (self.mdp.get_discount_factor() ** t)\n",
    "                * discounted_future_rewards[t]\n",
    "            ]\n",
    "        return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def select_action(self, state, action):\n",
    "        pass\n",
    "\n",
    "\n",
    "class StochasticPolicy(Policy):\n",
    "    def update(self, states, actions, rewards):\n",
    "        pass\n",
    "\n",
    "\n",
    "class LogisticRegressionPolicy(StochasticPolicy):\n",
    "    def __init__(self, actions, num_params, alpha=0.1, theta=None):\n",
    "        self.actions = actions\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        if theta is None:\n",
    "            theta = [0.0 for _ in range(num_params)]\n",
    "        self.theta = theta\n",
    "    \n",
    "    def select_action(self, state, action):\n",
    "        probability = self.get_probability(state, self.actions[0])\n",
    "        if random.random() < probability:\n",
    "            return self.actions[0]\n",
    "        else:\n",
    "            return self.actions[1]\n",
    "\n",
    "    \"\"\" Update our policy parameters according using the gradient descent formula:\n",
    "          theta <- theta + alpha * G * nabla J(theta), \n",
    "          where G is the future discounted reward\n",
    "    \"\"\"\n",
    "    \n",
    "    def update(self, states, actions, deltas):\n",
    "        for t in range(len(states)):\n",
    "            gradient_log_pi = self.gradient_log_pi(states[t], actions[t])\n",
    "            # Update each parameter\n",
    "            for i in range(len(self.theta)):\n",
    "                self.theta[i] += self.alpha * deltas[t] * gradient_log_pi[i]\n",
    "    \n",
    "    def get_probability(self, state, action):\n",
    "        # Calculate y as the linearly weitht product of the\n",
    "        # policy parameters (theta) and the state\n",
    "        y = self.dot_product(state, self.theta)\n",
    "        \n",
    "        # Pass y through the logistic regression function to convert it to a probability\n",
    "        probability = self.logistic_function(y)\n",
    "        \n",
    "        if action == self.actions[0]:\n",
    "            return probability\n",
    "        return 1 - probability\n",
    "    \n",
    "    \"\"\" Computes the gradient of the log of the policy (pi),\n",
    "    which is needed to get the gradient of objective (J).\n",
    "    Because the policy is a logistic regerssion, using the policy papameters (theta).\n",
    "        pi(actions[0] | state)= 1 / (1 + e^(-theta * state))\n",
    "        pi(actions[1] | state) = 1 / (1 + e^(theta * state))\n",
    "    When we apply a logarithmic transformation and take the gradient we end up with:\n",
    "        grad_log_pi(left | state) = state - state * pi(left | state)\n",
    "        grad_log_pi(right | state) = - state * pi(right | state)\n",
    "    \"\"\"\n",
    "    \n",
    "    def gradient_log_pi(self, state, action):\n",
    "        y = self.dot_product(state, self.theta)\n",
    "        if action == self. actions[0]:\n",
    "            return [s_i - s_i * self.logistic_function(y) for s_i in state]\n",
    "        return [-s_i * self.logistic_function(y) for s_i in state]\n",
    "    \n",
    "    @staticmethod\n",
    "    def logistic_function(y):\n",
    "        return 1 / (1 + math.exp(-y))\n",
    "    \n",
    "    @staticmethod\n",
    "    def dot_product(vec1, vec2):\n",
    "        return sum([v1 * v2 for v1, v2 in zip(vec1, vec2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gridworld = GridWorld(\n",
    "    height=1, width=11, initial_state=(5, 0), goals=[((0, 0), -1), ((10, 0), 1)]\n",
    ")\n",
    "gridworld_image = gridworld.visualise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/0AAABvCAYAAAC+eaC1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAiSklEQVR4nO3de1xU9b7/8fcwyFVAvCCgCFiKaQo/LZStZv7kSKWlu62mlta2y/bULk0j29vUzq7TFq1dlqVtU7OOmZfUXWaaqaXbtPKCtxRMvCEieAMBhYFZ5w9jYuQ2chltzuv5eMzDh2u+3++sea81i/nMupkMwzAEAAAAAABcjtv1ngEAAAAAAFA/KPoBAAAAAHBRFP0AAAAAALgoin4AAAAAAFwURT8AAAAAAC6Koh8AAAAAABdF0Q8AAAAAgIui6AcAAAAAwEW5O9LIarUqIyNDfn5+MplM9T1PAAAAAACgEoZh6OLFiwoNDZWbW9X78h0q+jMyMhQWFlYnMwcAAAAAAGrvxIkTatmyZZVtHCr6/fz8JEmfdeki3+Li2s8ZKufpKZ8ZMxQVFSWz2Xy958allZSUKCUlhaydgKydh6ydh6ydh6ydh6ydh6ydh6ydpzTrt396Wxar5XrPjkuzXLJo5TMrbbV6VRwq+ksP6fc1m+VrGLWbO1TN3V2+DRvK39+fjVI9KykpUUOydgqydh6ydh6ydh6ydh6ydh6ydh6ydp7SrD18PCTr9Z6b/xscOf2eC/kBAAAAAOCiKPoBAAAAAHBRFP0AAAAAALgoin4AAAAAAFwURT8AAAAAAC6Koh8AAAAAABdF0Q8AAAAAgIui6AcAAAAAwEVR9AMAAAAA4KIo+gEAAAAAcFEU/QAAAAAAuCiKfgAAAAAAXBRFPwAAAAAALoqiHwAAAAAAF0XRDwAAAACAi6LoBwAAAADARVH0AwAAAADgoij6AQAAAABwURT9AAAAAAC4KIp+AAAAAABcFEU/AAAAAAAuiqIfAAAAAAAXRdEPAAAAAICLougHAAAAAMBFUfQDAAAAAOCiKPoBAAAAAHBRFP0AAAAAALgoin4AAAAAAFwURT8AAAAAAC6Koh8AAAAAABdF0Q8AAAAAgIui6AcAAAAAwEVR9AMAAAAA4KIo+gEAAAAAcFHu13sGKrM/P1/fXrig1EuXlFpQoHPFxWrWoIE+79ixRuNdtlr1UWam1p0/r8yiIvmazercsKEeCwlRpLd3hX1yios179QpfZuTo7MWiwLc3dXN319PhIQoyMOjNm8PAAAAAFDHrMVWpX6dqgvHLujc0XPKPZkra4lVsY/F6ubeN9dozOzUbO1buU9nfz6rkqIS+QX7qXWv1mqb0FZubhXvRz+586QOrD6g80fPy7AaCmgZoDbxbdT6jta1eXs1csMW/V+dO6fF2dlyN5kU6eWlc8XFNR6ryGrVM4cOaU9+vm7x8dGQoCBlFRVp/fnz2pKbq5lt2uhWX1+7PjnFxXo8JUXHCwt1m5+f/iMwUMcuX9aqs2e1JSdH70dFqYWnZ23fJgAAAACgjhQXFmvnRzslSV4BXvJq5KWCswU1Hi99e7o2z9gscwOzWnVrJc+Gnjq586R2/s9OZadmq+eYnuX6pHyVoh0LdsizoaciukfIzd1NJ344oW3vbdOFExfU+cHONZ6fmrhhi/5+TZroniZN1NrLSw3c3NRt584aj7UoK0t78vP1/xs10iuRkXIzmSRJ8YGBej4tTf997JgW3nKLbbokzcrI0PHCQg0LCtKYli1t0xdnZemN9HRNP3FCb95cs1+KAAAAAACOS/s2Tdv+uU19JvZR8/bNK21n9jTrzsQ7FRgeKO9Ab+35dI/2Ld9Xo9e0FFj0/dzvZXIzqc+LfdSkdRNJUqdBnbT+1fU68cMJHd16VBFxEbY+edl52vXxLnk09FDCKwlq2KyhJKnj/R21ZtIaHVx9UGGxYWrWplmN5qkmbthz+tv6+CjKx0cNKjlcwlGGYWjFmTOSpD+3aGFX2N/RqJFiGjbUkcuXtSsvzza9oKREX549K283Nz0WEmI33uBmzRTs4aFtubk6WVhYq3lz1DvvvKOIiAh5eXmpa9eu+uGHH6psP2fOHPXs2VOBgYEKDAxUfHx8hX0OHDig++67TwEBAfL19dXtt9+u48eP19fbqNSePXvUs2dPeXl5KSwsTNOmTauy/QcffCCTyVThIysrq1z7LVu2yN3dXTExMdXOy7VmvX//fv3hD39QRESETCaT3nzzzXJtNm3apHvvvVehoaEymUxauXJltfNRX44fP65+/frJx8dHQUFBSkxMVHE1R9GkpqZqwIABatq0qfz9/dWjRw9t3LjRrs369ev1u9/9Tn5+fgoODtaECROqHfdas5akpUuXql27dvLy8lLHjh21evXqStuOHj260mXiDN988406d+4sT09P3Xzzzfrggw+q7bNkyRLFxMTIx8dH4eHhmj59erk2hYWFmjhxosLDw+Xp6amIiAjNmzevynHrOmuLxaIJEyaoY8eO8vX1VWhoqEaOHKmMjIxqx60P9ZX1woULFR0dLR8fH4WEhGjUqFE6e/ZslePWx3q9fPly9e3bV02aNJHJZFJycnK1Y9aXa91eS45tHxxZHlerj6wNw9DkyZMVEhIib29vxcfH69ChQ9WOW9dOnTql4cOHq23bK4eNjh071qF+jmzja/J5IevyyPrakbXz3GhZO5vZ3azQmFB5B1Z8Cve1OP7DcRXmFio8LtxW8EuS2cOsToM7SZJ+/vpnuz5p36bJarGq7X+0tRX8kuTh66EO93W40me9fZ/6dsMW/XUlvbBQmUVFauXpqdAKDseP8/eXJG2/eNE2bV9+vgoNQ518feVrNtu1dzOZ1O2XPjvK9Kkvixcv1rhx4zRlyhTt3LlT0dHRSkhIqLC4LfXNN99o2LBh2rhxo7Zu3aqwsDD17dtXJ0+etLU5fPiwevTooXbt2umbb77Rnj17NGnSJHl5edX7eyorNzdXffv2VXh4uHbs2KHp06frpZde0j//+c9K+zzwwAM6deqU3SMhIUG9evVSUFCQXdsLFy5o5MiR6tOnT7XzUpOsCwoK1Lp1a02dOlXBwcEVtsnPz1d0dLTeeeedauehPpWUlKhfv34qKirSd999pwULFuiDDz7Q5MmTq+zXv39/FRcXa8OGDdqxY4eio6PVv39/ZWZmSpJ2796te+65R3fddZd27dqlxYsX67PPPtMLL7xQ6Zg1yfq7777TsGHD9Oijj2rXrl0aOHCgBg4cqH37yv9yu2LFCm3btk2hoaEOplO3jhw5on79+ql3795KTk7W2LFj9dhjj2nt2rWV9vnyyy/14IMPavTo0dq3b5/effddvfHGG5o5c6ZduyFDhmj9+vWaO3euUlJStGjRIkVFRVU6bn1kXVBQoJ07d2rSpEnauXOnli9frpSUFN13333XmFTt1VfWW7Zs0ciRI/Xoo49q//79Wrp0qX744Qc9/vjjlY5bX+t1fn6+evTooaSkpGtMp27VZHvtyPbB0XW/rPrKetq0aXrrrbc0e/Zsff/99/L19VVCQoIuX758jWnVTmFhoZo1a6YXX3xR0dHRDvVxZBtfk88LWZdH1jVD1s5zI2X9W3f6p9OSpJBOIeWeC2oXJLOnWdmHslViKfm1z/5f+kSX7xMaHWrXxllMhmEY1TXKzc1VQECA1sfGyrcW59bXRredO2t0Ib8tOTkaf/iwuvv76/UKDsffcP68/nrkiPo0aqT/bn3logrLsrP12okTGtSsmZ4LCyvXZ+Hp03r75Ek91Ly5/tyiRc3eUGW8vOT73nu65ZZbZDab1bVrV91+++22Lz9Wq1VhYWF6+umnqyyqyiopKVFgYKBmzpypkSNHSpKGDh2qBg0a6KOPPqrV7BYVFcmjzEUNi4uLZTabZSpzREVVZs2apYkTJyozM9M2zgsvvKCVK1fq4MGDDo2RnZ2tFi1aaO7cuRoxYoTdc0OHDlWbNm1kNpu1cuVKuz1kJSUlOnDgQJ1lHRERobFjx1b5a6rJZNKKFSs0cOBAh95bWVdnbbVaZRiGzFf9MFWZL7/8Uv3791dGRoaaN79ySNTs2bM1YcIEZWdn241d6syZM2rWrJk2bdqknj2vnK908eJF+fv7a926dYqPj9df//pXrVu3Tj/++KOt3+eff64hQ4YoKytLfn5+dZL1Aw88oPz8fK1atco2rVu3boqJidHs2bNt006ePKmuXbtq7dq16tevX7XLpCIWi0UNGjSo9P/VmTBhgr744gu7LwJDhw7VhQsXtGbNmgr7DB8+XBaLRUuXLrVNe/vttzVt2jQdP35cJpNJa9as0dChQ5WWlqbGjRtXOI4zsy7rxx9/VGxsrI4dO6ZWrVpVHVAZN2rWr732mmbNmqXDhw/btUlKSlJ6erok52d99OhRRUZGateuXQ4duXS167G9dmT74MjycEbWhmEoNDRU48eP13PPPSdJysnJUfPmzfXBBx9o6NChDuUk1X69LuvOO+9UTExMtUctObKNd+TzQtZkXRmyJutryfr1fa+ryFpUo3mujKOH91+t9PD+mlzIb82kNTqXdk53vXKXGkeW/+71xYQvlJOeo37T+imgRYAk6dPRn6rwYqH+MPsP8vQrv9N5yaglKi4s1pB5Q+TuWfOz7S0FFi19fKlycnLk/8tO6cq4/J7+vJIrv7o0rKQwKt2TX9qupn3qQ1FRkXbs2KH4+HjbNDc3N8XHx2vr1q0Oj1NQUCCLxWIrEqxWq7744gu1bdtWCQkJCgoKUteuXR0+7Hzv3r2aMmWK2rdvr48//tjuueTkZLVq1Upjx47Vli1bVN1vSlu3btUdd9xh90U0ISFBKSkpOn/+vEPz8+GHH8rHx0eDBg2ymz5//nylpaVpypQp1Y5RV1nXtcOHDyspKUm33367/v73v9s9d+7cOQUHB+uJJ57QV199Ve3h9Fu3blXHjh1tG3LpSta5ubnav39/hX2aNGmiqKgoffjhh8rPz1dxcbHee+89BQUFqUuXLpKu/Jp89REi3t7eunz5snbs2FFuzJpmvXXrVrs+pfNfto/VatWIESOUmJioDh06VJGGPcMwtG3bNj333HOKjIzUli1b7J7/17/+paioKE2cOFG7du2qdjxH5vVqleWYnp6uY8eOSZI+++wz3XbbbZo2bZpatGihtm3b6rnnntOlS5cqHLM+s75aTk6OTCaTGjVqVGkb6beTdVxcnE6cOKHVq1fLMAydPn1ay5Yt0z333FPhmM7M+lpc7+21I9sHR5ZHWfWV9ZEjR5SZmWnXJiAgQF27dq12edT1el0Tjmzjr3V9I+uKkfUVZE3WpW2u53fl+mYpsEiSGnhX/KNL6fSigqLyfXwq6fPL9NJ2znDdLuQ3p4JzP/s1aVLhIfj/V505c0YlJSV2Hz5Jat68ucN7waUre8JCQ0NtH9KsrCzl5eVp6tSpeuWVV5SUlKQ1a9bo/vvv18aNG9WrV69yYyQnJ2vZsmVaunSpUlNTq3y99PR0zZgxQzNmzFBoaKjuv/9+DR48WD169Ch3S4vMzExFRkaWe3+lzwUGBlb7/ubOnavhw4fLu8ytFw8dOqQXXnhBmzdvlrt79at5XWVdF1JTU7Vs2TItW7bMbsPer1+/cm3PnDmjOXPmaM6cOWrcuLEGDhyoQYMGKT4+vtwvwpmZmRW+v9LnKmIymfT1119r4MCB8vPzk5ubm4KCgrRmzRrbsklISNCbb76pRYsWaciQIcrMzNTf/vY3SVfOKatonmuSdWXzX3bek5KS5O7urmeeeabScUoZhqHvvvtOy5Yt06effqoTJ05U2T41NVWvvvqqXn31Vd10000aNGiQBg0apNtuu83hec3NzdWlS5fs1tVSCQkJevbZZ/XII4+od+/e+vnnn/X6669LupJjRESE0tLS9O9//1teXl5asWKFzpw5oyeffFJnz57V/Pnzy41Zn1mXdfnyZU2YMEHDhg2r8Jfm32LW3bt318KFC/XAAw/o8uXLKi4u1r333lvpaTrOytoRN9L22pHtgyPLo6z6yrr0X0eXR32u1zXhyDb+Wj8vZF0xsv4VWZN1dX9za+pfY/6l/DP5FT63/r/Xl5sW2TNScaPj6uz1Xc11K/rnVrDyd/bzq/Oiv2E1e+XzK9irX5M+N6qpU6fqk08+0TfffGPbk2K1WiVJAwYM0LPPPitJiomJ0XfffafZs2fbiv4dO3bYvjiWPcT1WmRkZGjmzJmaOXOmgoOD9fvf/16DBw/WHXfc4fBh6VXZunWrDhw4YHeaQklJiYYPH67/+q//Utu2bWv9Gs5w4MABW6G/Z8+eGo1x7tw5zZs3T/PmzVOjRo00YMAADRo0SH379q3w0H1HGIahp556SkFBQdq8ebO8vb31/vvv695779WPP/6okJAQ9e3bV9OnT9fo0aM1YsQIeXp6atKkSdq8eXOl9y2tDzt27NCMGTO0c+fOSg9Xtlqt+ve//61ly5Zp+fLldte5uBalR2AkJSUpIiLC9oc3NjbW4UOlr/b444/r8OHD6t+/vywWi/z9/TVmzBi99NJLthytVqtMJpMWLlyogIArh5D94x//0KBBg/Tuu+/W6R9bR1ksFg0ZMkSGYWjWrFm26b/1rH/66SeNGTNGkydPVkJCgk6dOqXExESNHj1ac+fOrdHr1qcbdXvtyPbBkeVxo7iR12tXQ9bOQ9bOQ9aOi7orym7vuSRdOHZB6TvSFdkzUr7N7G+3Hhhe/Y7CmrDtlb9U8V750ukePh52fQovFspSYKnw8P7qjgSoD9ftr+m2zp3LPbr4+dX567T65UeEE5Vcab90eqsyhxba+lRy0Q1bn3o+KqFp06Yym806fdr+Qg+nT5+u9KJxZb322muaOnWqvvrqK3Xq1MluXHd3d7Vv396u/S233KLjx4+rpKRE06ZN0z333KOpU6fW+Avk1TIzMzVr1iwNGDBA48ePV35+voKDgyt8f5Iceo/vv/++YmJibIeaS1fOOd++fbv+/Oc/y93dXe7u7vrb3/6m3bt3y93dXRs2bCg3Tm2zro158+bp7rvv1uTJk2tc8F/twoULWrBggQYPHqzHHntM2dnZNcp6w4YNWrVqlT755BN1795dnTt3thWXCxYssLUbN26cLly4oOPHj+vMmTMaMGCAJKn1L9fJKKumWVc2/6V9Nm/erKysLLVq1cq23I8dO6bx48crIiJCly5d0l/+8hfdd999evvtt2v8h/ZqR48e1Wuvvab+/fvr5ZdflsViqXRe/f39Ky3MTSaTkpKSlJeXp2PHjikzM1OxsbGSfs0xJCRELVq0sBX80pXPrWEYtvPMy6qvrEuVFvzHjh3TunXrbHv5XSHrv//97+revbsSExPVqVMnJSQk6N1339W8efMqPIKlvrOuzG9he13d9sGR5VFWfWVd+m9lbZy5XteEI8vnWj8vZF0xsq4eWZdv838x69pod3c7dfpDJ7tHyy5XbqPe+o7W5Z4Lu638ddjqgn/Ile82uadyyz1nLbEqLztPJrNJDYMalu+TWb7PpfOXVFxYLJ/GPrU6n/9a3Vg/odeDlp6eCvbw0PHCQmVUUPhvzb2yMG4r84PDrb6+8jSZtCc/37ZXv5TVMPT9L33q40eKsjw8PNSlSxetX//rISxWq1Xr169XXFzVh69MmzZNL7/8stasWVPusB8PDw/dfvvtSklJsZuempqq8PBwmc1mPf/888rIyNCGDRv05JNPKuSqWxdeq4CAAD300ENauXKlsrOz9eabb8rX11dxcXHatGmT3QZq3bp1ioqKqvbQ/ry8PC1ZskSPPvqo3XR/f3/t3btXycnJtsfo0aMVFRWl5ORkde3atdxYtcm6tkaNGqWjR4/q+++/V2JiYoVfcq+Ft7e37r//fn388cfKysrShx9+qGbNmikuLk579+61uzJtaaF29Q9ApQoKCiSp3N42Nzc32xEjpUwmk0JDQ+Xt7a1FixYpLCxMnTt3LjdmTbOOi4uz61M6/6V9RowYoT179tgt99DQUCUmJmrt2rXy9vZWUlKSsrKytGrVKj3yyCMOnT5SlaCgIP3pT3/SunXrlJmZqcmTJ6tBgwbVzmtVzGazWrRoIQ8PDy1atEhxcXFq1uzKfVy7d++ujIwM5ZW5xWhqaqrc3NzUsmXLcmPVV9bSrwX/oUOH9PXXX6tJk19vY+MKWRcUFJRb70v3dld07nt9Zl3de/gtbK8d2T5UtTzKqq+sIyMjFRwcbNcmNzdX33//veLi4py6XteEI9v4a13fyLpiZF01siZrV1J6wcBTe8r/4J91MEslhSVq1qaZzA1+PSKueYdf+uwu3ydjd4ZdG2dxqaI/vbBQRy9fVnGZL2Qmk0m/b9pUkjTz5ElZyzy36cIFJeflKdLLS/+v4a+/zviYzbq7SRNdslr1/lV7dJZmZ+tUUZG6+furhROuPzBu3DjNmTNHCxYs0IEDB/Sf//mfys/P1x//+Edbm5EjR+ovf/mL7f9JSUmaNGmS5s2bp4iICGVmZiozM9OuUEhMTNTixYs1Z84c/fzzz5o5c6Y+//xzPfnkk7Y2ZrNZvXv31jvvvKP09HRt2rRJTz/9tFo4eMeCwMBAPfzww1q1apWysrL00UcfacCAAfIsk9vw4cPl4eFhuy3W4sWLNWPGDI0bN87WZsWKFWrXrl258RcvXqzi4mI99NBDdtPd3Nx066232j2CgoLk5eWlW2+9Vb6+vuXGqmnWRUVFtgKzqKhIJ0+eVHJysn7++dd7b+bl5dnaSFcu9JKcnKzjx4/bvX5sbKymTZumw4cPa/v27XrhhRfUpk0bB5KWfH19NXjwYC1evFjZ2dn69NNPNWzYMPmV+WGqb9++at++vUaMGKHdu3dr7dq1evHFF/XUU0/ZlskPP/ygdu3a2X59jouLsy3H3bt3KzU1VYmJibZbtpSaPn269u7dq/379+vll1/W1KlT9dZbb1V6SHBNsh4zZozWrFmj119/XQcPHtRLL71kO6JDunLRwauXe4MGDRQcHGx3SzsPDw/169dP8+fP1+nTp/Xll1/q0UcftStaqxIcHKwnn3xSGzZsUEZGhmbPnq34+Hi79zp69GilpaXp+eef18GDB/Xuu+9qyZIlttNpJGnmzJl2t5I8c+aMZs+erYMHDyo5OVljxozR0qVL7a62O3z4cDVp0kR//OMf9dNPP2nTpk1KTEzUqFGjKv2FvT6ytlgsGjRokLZv366FCxeqpKTEtp0pKvr1ULzfctb33nuvli9frlmzZiktLU1btmzRM888o9jY2EpvBVkfWUtXTt1JTk7WTz/9JElKSUlRcnKy3TmiN/L2urrtgyPLwxlZm0wmjR07Vq+88oo+++wz7d27VyNHjlRoaKjdHVecsV5Lsv3dyMvLU3Z2tt06UFHWjmzjHfm8kDVZkzVZ12XWvxVFBUXKycjRpfP2F0duFdtKnn6eOrb1mM6mnbVNLykq0Z6lV47QvTne/o4Are9oLbcGbkpdl6q87F/rr6L8Iu3/7MpFEW/uc213Eag1wwE5OTmGJGN9bKyxrXNnpzw+ad/euKdxY9tDkuHl5mY3bU2nTnZ9gj08DEnG8g4d7KZviokxOvr6GpKMW3x8jBHNmxt9AwMN8y9jvh8VVe7113bqZLTy9DQkGbf5+Rkjmzc37ggIMCQZge7uxrKrXqPOHr/7nbF3716juLjYlv/bb79ttGrVyvDw8DBiY2ONbdu22S2fXr16GQ8//LDt/+Hh4Yakco8pU6bY9Zs7d65x8803G15eXkZ0dLSxcuVKR1YHw2q1Glu2bDHGjh1rrFq1yu65/fv3G6NGjTJWr15tFBUVOTTe7t27jR49ehienp5GixYtjKlTp9o9P3/+fKOiVTUuLs4YPny4Q68xZcoUIzo62m5acXFxrbM+cuRIhVn36tXL1mbjxo0Vtik7TlV27dplTJw40Zg3b57d9PPnzxtDhw41li1bZhQUFDg01tGjR427777b8Pb2Npo2bWqMHz/esFgs5eb1yJEjtmk//vij0bdvX6Nx48aGn5+f0a1bN2P16tV24/bu3dsICAgwvLy8jK5du5Z7vi6yNgzDWLJkidG2bVvDw8PD6NChg/HFF19U+X7Dw8ONN954w4FkDMNisRhfffWV8cQTTxjbt2+3e279+vXG008/bXz77bdGSUmJQ+Nt3LjRiImJMTw8PIzWrVsb8+fPt3t+ypQpRnh4uO3/2dnZRrdu3QxfX1/Dx8fH6NOnT7lMDMMwDhw4YMTHxxve3t5Gy5YtjXHjxtktf2dkXdl6L8nYuHFjtdn8VrJ+6623jPbt2xve3t5GSEiI8eCDDxrp6em25521XpduA6vbplfkRtheV7d9cGR5OCtrq9VqTJo0yWjevLnh6elp9OnTx0hJSXEom7peryta5mXX44qyrm4bbxjVf17ImqzLIuuH7aaRteNZP7LoEWP4wuF1+uj2RDdDktFnYp9q20Y/EG1E9ow0IntGGo1aNTIkGU3bNLVNi30stsKxI3tGlhur57M9DZObyXD3dDduuvMm45Z+txj+If6GJCMsNswY9j/DyvXpMrKLIcnwbOhptIlvY0TdFWX4NPYxJBnt7mlXJ3kMnjPYkGTk5ORUuz6YflkpqpSbm6uAgACtj42VbzW3BasrOy5e1FOHDlXZZnmHDnYX/hu4b58yi4rKTZeky1arPszM1Ffnz+t0UZF8zWZ1bthQj4eEKLKSPWQ5xcWae+qUNuXk6IzFogCzWXEBAXoiJERBNbwwWrW8vOT73nu2+4ii/lx9z1bUH7J2HrJ2HrJ2HrJ2HrJ2HrJ2HrJ2ntKsX9/3uoqsRdV3uAZp36Zp2z+3qc/EPrbD7ivz9StfK+tAVqXPX321/9KxK7sLQHZKtvb9a5/OHDojq8Wqhs0b6qY7b1LbhLaVXnQ2fWe6Dn5xUOeOnpNhGApoEaC2/9FWre+o3am8pSwFFi19fKlycnIqvHtSWdft6v3V6eLnp20VnA9clZW33lrpc15ubnoiNFRPVHJYZkUC3N01LixM48Lq58IQAAAAAIDqte7VWq17OVYwx78YX6djN4tqpt7P976mMVt2bqmWnctfc+l6cKlz+gEAAAAAwK8o+gEAAAAAcFEU/QAAAAAAuCiKfgAAAAAAXBRFPwAAAAAALoqiHwAAAAAAF0XRDwAAAACAi6LoBwAAAADARVH0AwAAAADgoij6AQAAAABwURT9AAAAAAC4KIp+AAAAAABcFEU/AAAAAAAuiqIfAAAAAAAXRdEPAAAAAICLougHAAAAAMBFUfQDAAAAAOCiKPoBAAAAAHBRFP0AAAAAALgoin4AAAAAAFwURT8AAAAAAC6Koh8AAAAAABdF0Q8AAAAAgIui6AcAAAAAwEVR9AMAAAAA4KIo+gEAAAAAcFEU/QAAAAAAuCiKfgAAAAAAXBRFPwAAAAAALoqiHwAAAAAAF0XRDwAAAACAi6LoBwAAAADARVH0AwAAAADgoij6AQAAAABwURT9AAAAAAC4KIp+AAAAAABcFEU/AAAAAAAuiqIfAAAAAAAX5e5II8MwJEn57g41R224u8vIy9P58+dlNpuv99y4tJKSEuWRtVOQtfOQtfOQtfOQtfOQtfOQtfOQtfOUZq3LkqzXe25cXOGVf0pr9aqYDAdapaenKywsrNbzBQAAAAAA6saJEyfUsmXLKts4VPRbrVZlZGTIz89PJpOpzmYQAAAAAABcG8MwdPHiRYWGhsrNreqz9h0q+gEAAAAAwG8PF/IDAAAAAMBFUfQDAAAAAOCiKPoBAAAAAHBRFP0AAAAAALgoin4AAAAAAFwURT8AAAAAAC6Koh8AAAAAABf1vyk0Go/KE79hAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy = LogisticRegressionPolicy(\n",
    "    actions=[GridWorld.LEFT, GridWorld.RIGHT],\n",
    "    num_params=len(gridworld.get_initial_state()),\n",
    ")\n",
    "PolicyGradient(gridworld, policy).execute(episodes=100)\n",
    "policy_image = gridworld.visualise_stochastic_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetworkPolicy(StochasticPolicy):\n",
    "    \"\"\"\n",
    "    An implementation of a policy that uses a PyTorch deep neural network \n",
    "    to represent the underlying policy.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_space, action_space, hidden_dim=64, alpha=0.001, stochastic=True):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.temperature = 6.0\n",
    "        \n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(in_features=self.state_space, out_features=hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=self.action_space)\n",
    "        )\n",
    "        \n",
    "        self.optimiser = Adam(self.policy_network.parameters(), lr=alpha)\n",
    "        self.stochastic = stochastic\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for layer in self.policy_network:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "        \n",
    "        last_layer = self.policy_network[-1]\n",
    "        if isinstance():\n",
    "            with torch.no_grad():\n",
    "                last_layer.weight.fill_(0)\n",
    "                last_layer.bias.fill_(0)\n",
    "    \n",
    "    def select_action(self, state, actions):\n",
    "        state = torch.as_tensor(state, dtype=torch.float32)\n",
    "        action_logits = self.policy_network(state)\n",
    "        mask = torch.full_like(action_logits, float('-inf'))\n",
    "        mask[actions]\n",
    "    \n",
    "    def get_probability(self, state, action):\n",
    "        pass\n",
    "    \n",
    "    def evaluate_actions(self, states, actions):\n",
    "        pass\n",
    "    \n",
    "    def update(self, states, actions, deltas):\n",
    "        pass\n",
    "    \n",
    "    def save(self, filename):\n",
    "        torch.save(self.policy_network.state_dict(), filename)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, state_space, action_space, filename):\n",
    "        poliocy = cls(state_space, action_space)\n",
    "        policy.policy_networl.load_state_dict(torch.load(filename))\n",
    "        return policy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
