{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "from rendering_utils import *\n",
    "from plot import Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    \"\"\"Return all states of this MDP\"\"\"\n",
    "\n",
    "    def get_states(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return all actions with non-zero probability from this state \"\"\"\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return all non-zero probability transitions for this action\n",
    "        from this state, as a list of (state, probability) pairs\n",
    "    \"\"\"\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return the reward for transitioning from state to\n",
    "        nextState via action\n",
    "    \"\"\"\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return true if and only if state is a terminal state of this MDP \"\"\"\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return the discount factor for this MDP \"\"\"\n",
    "\n",
    "    def get_discount_factor(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return the initial state of this MDP \"\"\"\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return all goal states of this MDP \"\"\"\n",
    "\n",
    "    def get_goal_states(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Return a new state and a reward for executing action in state,\n",
    "    based on the underlying probability. This can be used for\n",
    "    model-free learning methods, but requires a model to operate.\n",
    "    Override for simulation-based learning\n",
    "    \"\"\"\n",
    "\n",
    "    def execute(self, state, action):\n",
    "        rand = random.random()\n",
    "        cumulative_probability = 0.0\n",
    "        for (new_state, probability) in self.get_transitions(state, action):\n",
    "            if cumulative_probability <= rand <= probability + cumulative_probability:\n",
    "                reward = self.get_reward(state, action, new_state)\n",
    "                return (new_state, reward, self.is_terminal(new_state))\n",
    "            cumulative_probability += probability\n",
    "            if cumulative_probability >= 1.0:\n",
    "                raise (\n",
    "                    \"Cumulative probability >= 1.0 for action \"\n",
    "                    + str(action)\n",
    "                    + \" from \"\n",
    "                    + str(state)\n",
    "                )\n",
    "\n",
    "        raise BaseException(\n",
    "            \"No outcome state in simulation for action \"\n",
    "            + str(action)\n",
    "            + \" from \"\n",
    "            + str(state)\n",
    "        )\n",
    "\n",
    "    \"\"\" \n",
    "    Execute a policy on this mdp for a number of episodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def execute_policy(self, policy, episodes=100, max_step=100):\n",
    "        cumulative_rewards = []\n",
    "        states = set()\n",
    "        for _ in range(episodes):\n",
    "            cumulative_reward = 0.0\n",
    "            state = self.get_initial_state()\n",
    "            step = 0\n",
    "            while not self.is_terminal(state):\n",
    "                actions = self.get_actions(state)\n",
    "                action = policy.select_action(state, actions)\n",
    "                (next_state, reward, done) = self.execute(state, action)\n",
    "                cumulative_reward += reward * (self.discount_factor ** step)\n",
    "                state = next_state\n",
    "                step += 1\n",
    "                if step > max_step:\n",
    "                    break\n",
    "            cumulative_rewards += [cumulative_reward]\n",
    "        return cumulative_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(MDP):\n",
    "    # labels for terminate action and terminal state\n",
    "    TERMINAL = (-1, -1)\n",
    "    TERMINATE = 0\n",
    "    LEFT = 1\n",
    "    UP = 2\n",
    "    RIGHT = 3\n",
    "    DOWN = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        noise=0.1,\n",
    "        width=4,\n",
    "        height=3,\n",
    "        discount_factor=0.9,\n",
    "        blocked_states=[(1, 1)],\n",
    "        action_cost=0.0,\n",
    "        initial_state=(0, 0),\n",
    "        goals=None,\n",
    "    ):\n",
    "        self.noise = noise\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.blocked_states = blocked_states\n",
    "        self.discount_factor = discount_factor\n",
    "        self.action_cost = action_cost\n",
    "        self.initial_state = initial_state\n",
    "        if goals is None:\n",
    "            self.goal_states = dict(\n",
    "                [((width - 1, height - 1), 1), ((width - 1, height - 2), -1)]\n",
    "            )\n",
    "        else:\n",
    "            self.goal_states = dict(goals)\n",
    "\n",
    "        # A list of lists that records all rewards given at each step\n",
    "        # for each episode of a simulated gridworld\n",
    "        self.rewards = []\n",
    "\n",
    "        # A list of cumulative rewards for each episode\n",
    "        self.cumulative_rewards = []\n",
    "    \n",
    "        # The rewards for the current episode\n",
    "        self.episode_rewards = []\n",
    "\n",
    "\n",
    "    def get_states(self):\n",
    "        states = [self.TERMINAL]\n",
    "        for x in range(self.width):\n",
    "            for y in range(self.height):\n",
    "                if not (x, y) in self.blocked_states:\n",
    "                    states.append((x, y))\n",
    "        return states\n",
    "\n",
    "    def get_actions(self, state=None):\n",
    "\n",
    "        actions = [self.TERMINATE, self.LEFT, self.UP, self.RIGHT, self.DOWN]\n",
    "        if state is None:\n",
    "            return actions\n",
    "\n",
    "        valid_actions = []\n",
    "        for action in actions:\n",
    "            for (new_state, probability) in self.get_transitions(state, action):\n",
    "                if probability > 0:\n",
    "                    valid_actions.append(action)\n",
    "                    break\n",
    "        return valid_actions\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        self.episode_rewards = []\n",
    "        return self.initial_state\n",
    "\n",
    "    def get_goal_states(self):\n",
    "        return self.goal_states\n",
    "\n",
    "    def valid_add(self, state, new_state, probability):\n",
    "        # If the next state is blocked, stay in the same state\n",
    "        if probability == 0.0:\n",
    "            return []\n",
    "\n",
    "        if new_state in self.blocked_states:\n",
    "            return [(state, probability)]\n",
    "\n",
    "        # Move to the next space if it is not off the grid\n",
    "        (x, y) = new_state\n",
    "        if x >= 0 and x < self.width and y >= 0 and y < self.height:\n",
    "            return [((x, y), probability)]\n",
    "\n",
    "        # If off the grid, state in the same state\n",
    "        return [(state, probability)]\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        transitions = []\n",
    "\n",
    "        if state == self.TERMINAL:\n",
    "            if action == self.TERMINATE:\n",
    "                return [(self.TERMINAL, 1.0)]\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        # Probability of not slipping left or right\n",
    "        straight = 1 - (2 * self.noise)\n",
    "\n",
    "        (x, y) = state\n",
    "        if state in self.get_goal_states().keys():\n",
    "            if action == self.TERMINATE:\n",
    "                transitions += [(self.TERMINAL, 1.0)]\n",
    "\n",
    "        elif action == self.UP:\n",
    "            transitions += self.valid_add(state, (x, y + 1), straight)\n",
    "            transitions += self.valid_add(state, (x - 1, y), self.noise)\n",
    "            transitions += self.valid_add(state, (x + 1, y), self.noise)\n",
    "\n",
    "        elif action == self.DOWN:\n",
    "            transitions += self.valid_add(state, (x, y - 1), straight)\n",
    "            transitions += self.valid_add(state, (x - 1, y), self.noise)\n",
    "            transitions += self.valid_add(state, (x + 1, y), self.noise)\n",
    "\n",
    "        elif action == self.RIGHT:\n",
    "            transitions += self.valid_add(state, (x + 1, y), straight)\n",
    "            transitions += self.valid_add(state, (x, y - 1), self.noise)\n",
    "            transitions += self.valid_add(state, (x, y + 1), self.noise)\n",
    "\n",
    "        elif action == self.LEFT:\n",
    "            transitions += self.valid_add(state, (x - 1, y), straight)\n",
    "            transitions += self.valid_add(state, (x, y - 1), self.noise)\n",
    "            transitions += self.valid_add(state, (x, y + 1), self.noise)\n",
    "\n",
    "        # Merge any duplicate outcomes\n",
    "        merged = defaultdict(lambda: 0.0)\n",
    "        for (state, probability) in transitions:\n",
    "            merged[state] = merged[state] + probability\n",
    "\n",
    "        transitions = []\n",
    "        for outcome in merged.keys():\n",
    "            transitions += [(outcome, merged[outcome])]\n",
    "\n",
    "        return transitions\n",
    "\n",
    "    def get_reward(self, state, action, new_state):\n",
    "        reward = 0.0\n",
    "        if state in self.get_goal_states().keys() and new_state == self.TERMINAL:\n",
    "            reward = self.get_goal_states().get(state)\n",
    "        else:\n",
    "            reward = self.action_cost\n",
    "        step = len(self.episode_rewards)\n",
    "        self.episode_rewards += [reward * (self.discount_factor ** step)]\n",
    "        return reward\n",
    "\n",
    "    def get_discount_factor(self):\n",
    "        return self.discount_factor\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        if state == self.TERMINAL:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    \"\"\"\n",
    "        Returns a list of lists, which records all rewards given at each step\n",
    "        for each episode of a simulated gridworld\n",
    "    \"\"\"\n",
    "\n",
    "    def get_rewards(self):\n",
    "        return self.rewards\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "        Returns a list of all cumulative rewards\n",
    "        for each episode of a simulated gridworld\n",
    "    \"\"\"\n",
    "\n",
    "    def get_cumulative_rewards(self):\n",
    "        return self.cumulative_rewards\n",
    "\n",
    "    \"\"\"\n",
    "        Create a gridworld from an array of strings: one for each line\n",
    "        - First line is rewards as a dictionary from cell to value: {'A': 1, ...}\n",
    "        - space is an empty cell\n",
    "        - # is a blocked cell\n",
    "        - @ is the agent (initial state)\n",
    "        - new 'line' is a new row\n",
    "        - a letter is a cell with a reward for transitioning\n",
    "          into that cell. The reward defined by the first line.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create(string):\n",
    "        # Parse the reward on the first line\n",
    "        import ast\n",
    "\n",
    "        rewards = ast.literal_eval(string[0])\n",
    "\n",
    "        width = 0\n",
    "        height = len(string) - 1\n",
    "\n",
    "        blocked_cells = []\n",
    "        initial_state = (0, 0)\n",
    "        goals = []\n",
    "        row = 0\n",
    "        for next_row in string[1:]:\n",
    "            column = 0\n",
    "            for cell in next_row:\n",
    "                if cell == \"#\":\n",
    "                    blocked_cells += [(column, row)]\n",
    "                elif cell == \"@\":\n",
    "                    initial_state = (column, row)\n",
    "                elif cell.isalpha():\n",
    "                    goals += [((column, row), rewards[cell])]\n",
    "                column += 1\n",
    "            width = max(width, column)\n",
    "            row += 1\n",
    "        return GridWorld(\n",
    "            width=width,\n",
    "            height=height,\n",
    "            blocked_states=blocked_cells,\n",
    "            initial_state=initial_state,\n",
    "            goals=goals,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def open(file):\n",
    "        file = open(file, \"r\")\n",
    "        string = file.read().splitlines()\n",
    "        file.close()\n",
    "        return GridWorld.create(string)\n",
    "\n",
    "    @staticmethod\n",
    "    def matplotlib_installed():\n",
    "        try:\n",
    "            import matplotlib as mpl\n",
    "            import matplotlib.pyplot as plt\n",
    "            return True\n",
    "        except ModuleNotFoundError:\n",
    "            return False\n",
    "\n",
    "    \"\"\" Visualise a Grid World problem \"\"\"\n",
    "\n",
    "    def visualise(self, agent_position=None, title=\"\", grid_size=1.0, gif=False):\n",
    "        if self.matplotlib_installed():\n",
    "            return self.visualise_as_image(agent_position=agent_position, title=title, grid_size=grid_size, gif=gif)\n",
    "        else:\n",
    "            print(self.to_string(title=title))\n",
    "\n",
    "    \"\"\" Visualise a Grid World value function \"\"\"\n",
    "    def visualise_value_function(self, value_function, title=\"\", grid_size=1.0, gif=False):\n",
    "        if self.matplotlib_installed():\n",
    "            return self.visualise_value_function_as_image(value_function, title=title, grid_size=grid_size, gif=gif)\n",
    "        else:\n",
    "            print(self.value_function_to_string(value_function, title=title))\n",
    "\n",
    "    def visualise_q_function(self, qfunction, title=\"\", grid_size=1.5, gif=False):\n",
    "        if self.matplotlib_installed():\n",
    "            return self.visualise_q_function_as_image(qfunction, title=title, grid_size=grid_size, gif=gif)\n",
    "        else:\n",
    "            print(self.q_function_to_string(qfunction, title=title))\n",
    "\n",
    "    def visualise_policy(self, policy, title=\"\", grid_size=1.0, gif=False):\n",
    "        if self.matplotlib_installed():\n",
    "            return self.visualise_policy_as_image(policy, title=title, grid_size=grid_size, gif=gif)\n",
    "        else:\n",
    "            print(self.policy_to_string(policy, title=title))\n",
    "\n",
    "    def visualise_stochastic_policy(self, policy, title=\"\", grid_size=1.0, gif=False):\n",
    "        if self.matplotlib_installed():\n",
    "            return self.visualise_stochastic_policy_as_image(policy, title=title, grid_size=grid_size, gif=gif)\n",
    "        else:\n",
    "            # TODO make a stochastic policy to string\n",
    "            pass\n",
    "\n",
    "    \"\"\" Visualise a grid world problem as a formatted string \"\"\"\n",
    "    def to_string(self, title=\"\"):\n",
    "        left_arrow = \"\\u25C4\"\n",
    "        up_arrow = \"\\u25B2\"\n",
    "        right_arrow = \"\\u25BA\"\n",
    "        down_arrow = \"\\u25BC\"\n",
    "\n",
    "\n",
    "        space = \" |              \"\n",
    "        block = \" | #############\"\n",
    "\n",
    "        line = \"  \"\n",
    "        for x in range(self.width):\n",
    "            line += \"--------------- \"\n",
    "        line += \"\\n\"\n",
    "\n",
    "        result = \" \" + title + \"\\n\"\n",
    "        result += line\n",
    "        for y in range(self.height - 1, -1, -1):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.get_goal_states().keys():\n",
    "                    result += space\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                else:\n",
    "                    result += \" |       {}      \".format(up_arrow)\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) == self.get_initial_state():\n",
    "                    result += \" |     _____    \"\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                else:\n",
    "                    result += space\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) == self.get_initial_state():\n",
    "                    result += \" |    ||o  o|   \"\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                else:\n",
    "                    result += space\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) == self.get_initial_state():\n",
    "                    result += \" | {}  ||  * |  {}\".format(left_arrow, right_arrow)\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                elif (x, y) in self.get_goal_states().keys():\n",
    "                    result += \" |     {:+0.2f}    \".format(\n",
    "                        self.get_goal_states()[(x, y)]\n",
    "                    )\n",
    "                else:\n",
    "                    result += \" | {}           {}\".format(left_arrow, right_arrow)\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) == self.get_initial_state():\n",
    "                    result += \" |    ||====|   \".format(left_arrow, right_arrow)\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                else:\n",
    "                    result += space\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) == self.get_initial_state():\n",
    "                    result += \" |     -----    \"\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                else:\n",
    "                    result += space\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.get_goal_states().keys():\n",
    "                    result += space\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    result += block\n",
    "                else:\n",
    "                    result += \" |       {}      \".format(down_arrow)\n",
    "            result += \" |\\n\"\n",
    "            result += line\n",
    "        return result\n",
    "\n",
    "    \"\"\" Convert a grid world value function to a formatted string \"\"\"\n",
    "\n",
    "    def value_function_to_string(self, values, title=\"\"):\n",
    "        line = \" {:-^{n}}\\n\".format(\"\", n=len(\" | +0.00\") * self.width + 1)\n",
    "        result = \" \" + title + \"\\n\"\n",
    "        result += line\n",
    "        for y in range(self.height - 1, -1, -1):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states:\n",
    "                    result += \" | #####\"\n",
    "                else:\n",
    "                    result += \" | {:+0.2f}\".format(values.get_value((x, y)))\n",
    "            result += \" |\\n\"\n",
    "            result += line\n",
    "\n",
    "        return result\n",
    "\n",
    "    \"\"\" Convert a grid world Q function to a formatted string \"\"\"\n",
    "\n",
    "    def q_function_to_string(self, qfunction, title=\"\"):\n",
    "        left_arrow = \"\\u25C4\"\n",
    "        up_arrow = \"\\u25B2\"\n",
    "        right_arrow = \"\\u25BA\"\n",
    "        down_arrow = \"\\u25BC\"\n",
    "\n",
    "        space = \" |               \"\n",
    "\n",
    "        line = \"  \"\n",
    "        for x in range(self.width):\n",
    "            line += \"---------------- \"\n",
    "        line += \"\\n\"\n",
    "\n",
    "        result = \" \" + title + \"\\n\"\n",
    "        result += line\n",
    "        for y in range(self.height - 1, -1, -1):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states or (\n",
    "                    x,\n",
    "                    y,\n",
    "                ) in self.get_goal_states().keys():\n",
    "                    result += space\n",
    "                else:\n",
    "                    result += \" |       {}       \".format(up_arrow)\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states or (\n",
    "                    x,\n",
    "                    y,\n",
    "                ) in self.get_goal_states().keys():\n",
    "                    result += space\n",
    "                else:\n",
    "                    result += \" |     {:+0.2f}     \".format(\n",
    "                        qfunction.get_q_value((x, y), self.UP)\n",
    "                    )\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                result += space\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states:\n",
    "                    result += \" |     #####     \"\n",
    "                elif (x, y) in self.get_goal_states().keys():\n",
    "                    result += \" |     {:+0.2f}     \".format(\n",
    "                        self.get_goal_states()[(x, y)]\n",
    "                    )\n",
    "                else:\n",
    "                    result += \" | {}{:+0.2f}  {:+0.2f}{}\".format(\n",
    "                        left_arrow,\n",
    "                        qfunction.get_q_value((x, y), self.LEFT),\n",
    "                        qfunction.get_q_value((x, y), self.RIGHT),\n",
    "                        right_arrow,\n",
    "                    )\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                result += space\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states or (\n",
    "                    x,\n",
    "                    y,\n",
    "                ) in self.get_goal_states().keys():\n",
    "                    result += space\n",
    "                else:\n",
    "                    result += \" |     {:+0.2f}     \".format(\n",
    "                        qfunction.get_q_value((x, y), self.DOWN)\n",
    "                    )\n",
    "            result += \" |\\n\"\n",
    "\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states or (\n",
    "                    x,\n",
    "                    y,\n",
    "                ) in self.get_goal_states().keys():\n",
    "                    result += space\n",
    "                else:\n",
    "                    result += \" |       {}       \".format(down_arrow)\n",
    "            result += \" |\\n\"\n",
    "            result += line\n",
    "        return result\n",
    "\n",
    "    \"\"\" Convert a grid world policy to a formatted string \"\"\"\n",
    "    def policy_to_string(self, policy, title=\"\"):\n",
    "        arrow_map = {self.UP:'\\u25B2',\n",
    "                     self.DOWN:'\\u25BC',\n",
    "                     self.LEFT:'\\u25C4',\n",
    "                     self.RIGHT:'\\u25BA',\n",
    "                    }\n",
    "        line = \" {:-^{n}}\\n\".format(\"\", n=len(\" |  N \") * self.width + 1)\n",
    "        result = \" \" + title + \"\\n\"\n",
    "        result += line\n",
    "        for y in range(self.height - 1, -1, -1):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states:\n",
    "                    result += \" | ###\"\n",
    "                elif policy.select_action((x, y), self.get_actions((x, y))) == self.TERMINATE:\n",
    "                    result += \" | {:+0d} \".format(self.goal_states[(x, y)])\n",
    "                else:\n",
    "                    result += \" |  \" + arrow_map[policy.select_action((x, y), self.get_actions((x, y)))] + \" \"\n",
    "            result += \" |\\n\"\n",
    "            result += line\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    \"\"\" Initialise a gridworld grid \"\"\"\n",
    "    def initialise_grid(self, grid_size=1.0):\n",
    "        fig = plt.figure(figsize=(self.width * grid_size, self.height * grid_size))\n",
    "\n",
    "        # Trim whitespace \n",
    "        plt.subplots_adjust(top=0.92, bottom=0.01, right=1, left=0, hspace=0, wspace=0)\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "        # Initialise the map to all white\n",
    "        img = [[COLOURS['white'] for _ in range(self.width)] for _ in range(self.height)]\n",
    "\n",
    "        # Render the grid\n",
    "        for y in range(0, self.height):\n",
    "            for x in range(0, self.width):\n",
    "                if (x, y) in self.goal_states:\n",
    "                    img[y][x] = COLOURS['red'] if self.goal_states[(x, y)] < 0 else COLOURS['green']\n",
    "                elif (x, y) in self.blocked_states:\n",
    "                    img[y][x] = COLOURS['grey']\n",
    "\n",
    "        ax.xaxis.set_ticklabels([])  # clear x tick labels\n",
    "        ax.axes.yaxis.set_ticklabels([])  # clear y tick labels\n",
    "        ax.tick_params(which='both', top=False, left=False, right=False, bottom=False)\n",
    "        ax.set_xticks([w - 0.5 for w in range(0, self.width, 1)])\n",
    "        ax.set_yticks([h - 0.5 for h in range(0, self.height, 1)])\n",
    "        ax.grid(color='lightgrey')\n",
    "        return fig, ax, img\n",
    "\n",
    "    \"\"\" visualise the gridworld problem as a matplotlib image \"\"\"\n",
    "\n",
    "    def visualise_as_image(self, agent_position=None, title=\"\", grid_size=1.0, gif=False):\n",
    "        fig, ax, img = self.initialise_grid(grid_size=grid_size)\n",
    "        current_position = (\n",
    "            self.get_initial_state() if agent_position is None else agent_position\n",
    "        )\n",
    "\n",
    "        # Render the grid\n",
    "        for y in range(0, self.height):\n",
    "            for x in range(0, self.width):\n",
    "                if (x, y) == current_position:\n",
    "                    ax.scatter(x, y, s=2000, marker='o', edgecolors='none')\n",
    "                elif (x, y) in self.goal_states:\n",
    "                    plt.text(\n",
    "                        x,\n",
    "                        y,\n",
    "                        f\"{self.get_goal_states()[(x, y)]:+0.2f}\",\n",
    "                        fontsize=\"x-large\",\n",
    "                        horizontalalignment=\"center\",\n",
    "                        verticalalignment=\"center\",\n",
    "                    )\n",
    "        im = plt.imshow(img, origin=\"lower\")\n",
    "        plt.title(title)\n",
    "        if gif:\n",
    "            return fig, ax, im\n",
    "        else:\n",
    "            return fig\n",
    "\n",
    "    \"\"\"Render each tile individually depending on the current state of the cell\"\"\"\n",
    "\n",
    "    def render_tile(self, x, y, tile_size, img, tile_type=None):\n",
    "        ymin = y * tile_size\n",
    "        ymax = (y + 1) * tile_size\n",
    "        xmin = x * tile_size\n",
    "        xmax = (x + 1) * tile_size\n",
    "\n",
    "        for i in range(ymin, ymax):\n",
    "            for j in range(xmin, xmax):\n",
    "                if i == ymin or i == ymax - 1 or j == xmin or j == xmax + 1:\n",
    "                    draw_grid_lines(i, j, img)\n",
    "                else:\n",
    "                    if tile_type == \"goal\":\n",
    "                        render_goal(\n",
    "                            i,\n",
    "                            j,\n",
    "                            img,\n",
    "                            reward=self.goal_states[(x, y)],\n",
    "                            reward_max=max(self.get_goal_states().values()),\n",
    "                            reward_min=min(self.get_goal_states().values()),\n",
    "                        )\n",
    "                    elif tile_type == \"blocked\":\n",
    "                        render_blocked_tile(i, j, img)\n",
    "                    elif tile_type == \"agent\":\n",
    "                        render_agent(\n",
    "                            i,\n",
    "                            j,\n",
    "                            img,\n",
    "                            center_x=xmin + tile_size / 2,\n",
    "                            center_y=ymin + tile_size / 2,\n",
    "                            radius=tile_size / 4,\n",
    "                        )\n",
    "                    elif tile_type == \"empty\":\n",
    "                        img[i][j] = [255, 255, 255]\n",
    "                    else:\n",
    "                        raise ValueError(\"Invalid tile type\")\n",
    "\n",
    "    \"\"\" Visualise the value function \"\"\"\n",
    "\n",
    "    def visualise_value_function_as_image(self, value_function, title=\"\", grid_size=1.0, gif=False):\n",
    "        if not gif:\n",
    "            fig, ax, img = self.initialise_grid(grid_size=grid_size)\n",
    "        texts = []\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                value = value_function.get_value((x, y))\n",
    "                if (x, y) not in self.blocked_states:\n",
    "                    text = plt.text(\n",
    "                        x,\n",
    "                        y,\n",
    "                        f\"{float(value):+0.2f}\",\n",
    "                        fontsize=\"medium\",\n",
    "                        horizontalalignment=\"center\",\n",
    "                        verticalalignment=\"center\",\n",
    "                        color='lightgrey' if value == 0.0 else 'black',\n",
    "                    )\n",
    "                    texts.append(text)\n",
    "        if gif:\n",
    "            return texts\n",
    "        else:\n",
    "            ax.imshow(img, origin=\"lower\")\n",
    "            plt.title(title, fontsize=\"large\")\n",
    "            plt.show()\n",
    "\n",
    "    \"\"\" Visualise the value function using a heat-map where green is high value and\n",
    "    red is low value\n",
    "    \"\"\"\n",
    "\n",
    "    def visualise_value_function_as_heatmap(self, value_function, title=\"\"):\n",
    "        values = [[0 for _ in range(self.width)] for _ in range(self.height)]\n",
    "        fig, ax = self.initialise_grid()\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.blocked_states:\n",
    "                    plt.text(\n",
    "                        x,\n",
    "                        y,\n",
    "                        \"#\",\n",
    "                        horizontalalignment=\"center\",\n",
    "                        verticalalignment=\"center\",\n",
    "                    )\n",
    "                else:\n",
    "                    values[y][x] = value_function.get_value((x, y))\n",
    "                    plt.text(\n",
    "                        x,\n",
    "                        y,\n",
    "                        f\"{values[y][x]:.2f}\",\n",
    "                        horizontalalignment=\"center\",\n",
    "                        verticalalignment=\"center\",\n",
    "                    )\n",
    "        plt.imshow(values, origin=\"lower\", cmap=make_red_white_green_cmap())\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    \"\"\" Visualise the Q-function with matplotlib \"\"\"\n",
    "\n",
    "    def visualise_q_function_as_image(self, qfunction, title=\"\", grid_size=1.5, gif=False):\n",
    "        if not gif:\n",
    "            fig, ax, img = self.initialise_grid(grid_size=grid_size)\n",
    "        texts = []\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) in self.goal_states:\n",
    "                    # gif player handles goal state rendering\n",
    "                    if not gif:\n",
    "                        texts.append(plt.text(\n",
    "                            x,\n",
    "                            y,\n",
    "                            f\"{self.get_goal_states()[(x,y)]:+0.2f}\",\n",
    "                            fontsize=\"large\",\n",
    "                            horizontalalignment=\"center\",\n",
    "                            verticalalignment=\"center\",\n",
    "                        ))\n",
    "                elif (x, y) not in self.blocked_states:\n",
    "                    up_value = qfunction.get_q_value((x, y), self.UP)\n",
    "                    down_value = qfunction.get_q_value((x, y), self.DOWN)\n",
    "                    left_value = qfunction.get_q_value((x, y), self.LEFT)\n",
    "                    right_value = qfunction.get_q_value((x, y), self.RIGHT)\n",
    "                    texts.append(plt.text(\n",
    "                        x,\n",
    "                        y + 0.35,\n",
    "                        f\"{up_value:+0.2f}\",\n",
    "                        fontsize=\"medium\",\n",
    "                        horizontalalignment=\"center\",\n",
    "                        verticalalignment=\"top\",\n",
    "                        color='lightgrey' if up_value == 0.0 else 'black',\n",
    "                    ))\n",
    "                    texts.append(plt.text(\n",
    "                        x,\n",
    "                        y - 0.35,\n",
    "                        f\"{down_value:+0.2f}\",\n",
    "                        fontsize=\"medium\",\n",
    "                        horizontalalignment=\"center\",\n",
    "                        verticalalignment=\"bottom\",\n",
    "                        color='lightgrey' if down_value == 0.0 else 'black',\n",
    "                    ))\n",
    "                    texts.append(plt.text(\n",
    "                        x - 0.45,\n",
    "                        y,\n",
    "                        f\"{left_value:+0.2f}\",\n",
    "                        fontsize=\"medium\",\n",
    "                        horizontalalignment=\"left\",\n",
    "                        verticalalignment=\"center\",\n",
    "                        color='lightgrey' if left_value == 0.0 else 'black'\n",
    "                    ))\n",
    "                    texts.append(plt.text(\n",
    "                        x + 0.45,\n",
    "                        y,\n",
    "                        f\"{right_value:+0.2f}\",\n",
    "                        fontsize=\"medium\",\n",
    "                        horizontalalignment=\"right\",\n",
    "                        verticalalignment=\"center\",\n",
    "                        color='lightgrey' if right_value == 0.0 else 'black'\n",
    "                    ))\n",
    "                    plt.plot([x-0.5, x+0.5], [y-0.5, y+0.5], ls='-', lw=1, color='lightgrey')\n",
    "                    plt.plot([x + 0.5, x - 0.5], [y - 0.5, y + 0.5], ls='-', lw=1, color='lightgrey')\n",
    "        if gif:\n",
    "            return texts\n",
    "        ax.imshow(img, origin=\"lower\")\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    \"\"\" Visualise the Q-function with a matplotlib visual\"\"\"\n",
    "\n",
    "    def visualise_q_function_rendered(self, q_values, title=\"\", tile_size=32, show_text=False):\n",
    "        width_px = self.width * tile_size\n",
    "        height_px = self.height * tile_size\n",
    "        img = [[[0, 0, 0] for _ in range(width_px)] for _ in range(height_px)]\n",
    "\n",
    "        # provide these to scale the colours between the highest and lowest value\n",
    "        reward_max = max(self.get_goal_states().values())\n",
    "        reward_min = min(self.get_goal_states().values())\n",
    "        # Render the grid\n",
    "        for y in range(0, self.height):\n",
    "            for x in range(0, self.width):\n",
    "                # Draw in the blocked states as a black and white mesh\n",
    "                if (x, y) in self.blocked_states:\n",
    "                    render_full_blocked_tile(\n",
    "                        x * tile_size, y * tile_size, tile_size, img\n",
    "                    )\n",
    "                    continue\n",
    "                # Draw goal states\n",
    "                if (x, y) in self.goal_states:\n",
    "                    render_full_goal_tile(\n",
    "                        x * tile_size,\n",
    "                        y * tile_size,\n",
    "                        tile_size,\n",
    "                        img,\n",
    "                        reward=self.goal_states[(x, y)],\n",
    "                        rewardMax=reward_max,\n",
    "                        rewardMin=reward_min,\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                # Draw the action value for action available in each cell\n",
    "                # Break the grid up into 4 sections, using triangles that meet\n",
    "                # in the middle. The base of the triangle points toward the\n",
    "                # direction of the action\n",
    "                render_action_q_value(\n",
    "                    tile_size,\n",
    "                    x,\n",
    "                    y,\n",
    "                    self.UP,\n",
    "                    q_values,\n",
    "                    img,\n",
    "                    show_text,\n",
    "                    v_text_offset=8,\n",
    "                    rewardMax=reward_max,\n",
    "                    rewardMin=reward_min,\n",
    "                )\n",
    "                render_action_q_value(\n",
    "                    tile_size,\n",
    "                    x,\n",
    "                    y,\n",
    "                    self.DOWN,\n",
    "                    q_values,\n",
    "                    img,\n",
    "                    show_text,\n",
    "                    v_text_offset=-8,\n",
    "                    rewardMax=reward_max,\n",
    "                    rewardMin=reward_min,\n",
    "                )\n",
    "                render_action_q_value(\n",
    "                    tile_size,\n",
    "                    x,\n",
    "                    y,\n",
    "                    self.LEFT,\n",
    "                    q_values,\n",
    "                    img,\n",
    "                    show_text,\n",
    "                    h_text_offset=-8,\n",
    "                    rewardMax=reward_max,\n",
    "                    rewardMin=reward_min,\n",
    "                )\n",
    "                render_action_q_value(\n",
    "                    tile_size,\n",
    "                    x,\n",
    "                    y,\n",
    "                    self.RIGHT,\n",
    "                    q_values,\n",
    "                    img,\n",
    "                    show_text,\n",
    "                    h_text_offset=8,\n",
    "                    rewardMax=reward_max,\n",
    "                    rewardMin=reward_min,\n",
    "                )\n",
    "\n",
    "        ax.imshow(img, origin=\"lower\", interpolation=\"bilinear\")\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    \"\"\" Visualise the policy of the agent with a matplotlib visual \"\"\"\n",
    "\n",
    "    def visualise_policy_as_image(self, policy, title=\"\", grid_size=1.0, gif=False):\n",
    "        # Map from action names to prettier arrows\n",
    "        arrow_map = {self.UP:'\\u2191',\n",
    "                     self.DOWN:'\\u2193',\n",
    "                     self.LEFT:'\\u2190',\n",
    "                     self.RIGHT:'\\u2192',\n",
    "                    }\n",
    "        if not gif:\n",
    "            fig, ax, img = self.initialise_grid(grid_size=grid_size)\n",
    "        texts = []\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                if (x, y) not in self.blocked_states and (x, y) not in self.goal_states:\n",
    "                    if policy.select_action((x, y), self.get_actions((x, y))) != self.TERMINATE:\n",
    "                        action = arrow_map[policy.select_action((x, y), self.get_actions((x, y)))]\n",
    "                        fontsize = \"xx-large\"\n",
    "                    texts.append(plt.text(\n",
    "                                x,\n",
    "                                y,\n",
    "                                action,\n",
    "                                fontsize=fontsize,\n",
    "                                horizontalalignment=\"center\",\n",
    "                                verticalalignment=\"center\",\n",
    "                            ))\n",
    "                elif (x, y) in self.goal_states:\n",
    "                    # gif player handles goal state rendering\n",
    "                    if not gif:\n",
    "                        plt.text(\n",
    "                            x,\n",
    "                            y,\n",
    "                            f\"{self.get_goal_states()[(x, y)]:+0.2f}\",\n",
    "                            fontsize=\"x-large\",\n",
    "                            horizontalalignment=\"center\",\n",
    "                            verticalalignment=\"center\",\n",
    "                        )\n",
    "        if gif:\n",
    "            return texts\n",
    "        ax.imshow(img, origin=\"lower\")\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    def execute(self, state, action):\n",
    "        if state in self.goal_states:\n",
    "            self.rewards += [self.episode_rewards]\n",
    "            self.cumulative_rewards += [sum(self.episode_rewards)]\n",
    "            return MDP.execute(self, state=state, action=self.TERMINATE)\n",
    "        return super().execute(state, action)\n",
    "\n",
    "    def visualise_stochastic_policy_as_image(self, policy, title=\"\", grid_size=1.0, gif=False):\n",
    "        if not gif:\n",
    "            fig, ax, img = self.initialise_grid(grid_size=grid_size)\n",
    "        texts = []\n",
    "\n",
    "        # Render the grid\n",
    "        for y in range(0, self.height):\n",
    "            for x in range(0, self.width):\n",
    "                prob_up = 0.0\n",
    "                prob_down = 0.0\n",
    "                prob_left = policy.get_probability((x, y), self.LEFT)\n",
    "                prob_right = policy.get_probability((x, y), self.RIGHT)\n",
    "                if self.height > 1:\n",
    "                    prob_up = policy.get_probability((x, y), self.UP)\n",
    "                    prob_down = policy.get_probability((x, y), self.DOWN)\n",
    "                # Normalise to account for the 'terminate' action that is not visualised\n",
    "                total = prob_left + prob_right + prob_down + prob_up\n",
    "                if total != 0:\n",
    "                    prob_left = prob_left / total\n",
    "                    prob_right = prob_right / total\n",
    "                    prob_down = prob_down / total\n",
    "                    prob_up = prob_up / total\n",
    "                if (x, y) in self.goal_states:\n",
    "                    # gif player handles goal state rendering\n",
    "                    if not gif:\n",
    "                        plt.text(\n",
    "                            x,\n",
    "                            y,\n",
    "                            f\"{self.get_goal_states()[(x, y)]:+0.2f}\",\n",
    "                            fontsize=\"x-large\",\n",
    "                            horizontalalignment=\"center\",\n",
    "                            verticalalignment=\"center\",\n",
    "                        )\n",
    "                elif (x, y) not in self.blocked_states:\n",
    "                    left_triangle = '\\u25C4'\n",
    "                    up_triangle = '\\u25B2'\n",
    "                    right_triangle = '\\u25BA'\n",
    "                    down_triangle = '\\u25BC'\n",
    "                    if self.height > 1:\n",
    "                        texts.append(plt.text(\n",
    "                            x,\n",
    "                            y,\n",
    "                            f\"{prob_up:0.2f}\\n{up_triangle}\\n{prob_left:0.2f}{left_triangle} {right_triangle}{prob_right:0.2f}\\n{down_triangle}\\n{prob_down:0.2f}\",\n",
    "                            fontsize=\"medium\",\n",
    "                            horizontalalignment=\"center\",\n",
    "                            verticalalignment=\"center\",\n",
    "                        ))\n",
    "                    else:\n",
    "                        texts.append(plt.text(\n",
    "                            x,\n",
    "                            y,\n",
    "                            f\"{prob_left:0.2f}{left_triangle} {right_triangle}{prob_right:0.2f}\",\n",
    "                            fontsize=\"medium\",\n",
    "                            horizontalalignment=\"center\",\n",
    "                            verticalalignment=\"center\",\n",
    "                        ))\n",
    "        if gif:\n",
    "            return texts\n",
    "        ax.imshow(img, origin=\"lower\")\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REDINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient:\n",
    "    def __init__(self, mdp, policy):\n",
    "        super().__init__()\n",
    "        self.mdp = mdp\n",
    "        self.policy = policy\n",
    "    \n",
    "    \"\"\" Generate and store an entire episode trajectory to use to updsate the policy \"\"\"\n",
    "    \n",
    "    def execute(self, episodes=100, max_episode_length=float('inf')):\n",
    "        total_steps = 0\n",
    "        random_steps = 50\n",
    "        episode_rewards = []\n",
    "        for episode in range(episodes):\n",
    "            actions = []\n",
    "            states = []\n",
    "            rewards = []\n",
    "            \n",
    "            state = self.mdp.get_initial_state()\n",
    "            episode_reward = 0.0\n",
    "            for step in count():\n",
    "                if total_steps < random_steps:\n",
    "                    action = random.choice(self.mdp.get_actions(state))\n",
    "                else:\n",
    "                    action = self.policy.select_action(state, self.mdp.get_actions(state))\n",
    "                (next_state, reward, done) = self.mdp.execute(state, action)\n",
    "                \n",
    "                # Store the information from this step of trajectory\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                state = next_state\n",
    "                episode_reward += reward * (self.mdp.discount_factor ** step)\n",
    "                total_steps += 1\n",
    "                \n",
    "                if done or step == max_episode_length:\n",
    "                    break\n",
    "            \n",
    "            deltas = self.calculate_deltas(rewards)\n",
    "            \n",
    "            self.policy.update(states, actions, deltas)\n",
    "            episode_rewards.append(episode_rewards)\n",
    "        \n",
    "        return episode_rewards\n",
    "    \n",
    "    def calculate_deltas(self, rewards):\n",
    "        \"\"\"\n",
    "        Generate a list of the discounted future rewards at each step of an episode\n",
    "        Note that discounted_reward[T-2] = rewards[T-1] + discounted_reward[T-1] * gamma.\n",
    "        We can use that pattern to populate the discounted_rewards array.\n",
    "        \"\"\"\n",
    "        T = len(rewards)\n",
    "        discounted_future_rewards = [0 for _ in range(T)]\n",
    "\n",
    "        # The final discounted reward is the reward you get at that step\n",
    "        discounted_future_rewards[T - 1] = rewards[T - 1]\n",
    "        for t in reversed(range(0, T - 1)):\n",
    "            discounted_future_rewards[t] = (\n",
    "                rewards[t]\n",
    "                + discounted_future_rewards[t + 1] * self.mdp.get_discount_factor()\n",
    "            )\n",
    "        deltas = []\n",
    "        for t in range(len(discounted_future_rewards)):\n",
    "            deltas += [\n",
    "                (self.mdp.get_discount_factor() ** t)\n",
    "                * discounted_future_rewards[t]\n",
    "            ]\n",
    "        return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def select_action(self, state, action):\n",
    "        pass\n",
    "\n",
    "\n",
    "class StochasticPolicy(Policy):\n",
    "    def update(self, states, actions, rewards):\n",
    "        pass\n",
    "\n",
    "\n",
    "class LogisticRegressionPolicy(StochasticPolicy):\n",
    "    def __init__(self, actions, num_params, alpha=0.1, theta=None):\n",
    "        self.actions = actions\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        if theta is None:\n",
    "            theta = [0.0 for _ in range(num_params)]\n",
    "        self.theta = theta\n",
    "    \n",
    "    def select_action(self, state, action):\n",
    "        probability = self.get_probability(state, self.actions[0])\n",
    "        if random.random() < probability:\n",
    "            return self.actions[0]\n",
    "        else:\n",
    "            return self.actions[1]\n",
    "\n",
    "    \"\"\" Update our policy parameters according using the gradient descent formula:\n",
    "          theta <- theta + alpha * G * nabla J(theta), \n",
    "          where G is the future discounted reward\n",
    "    \"\"\"\n",
    "    \n",
    "    def update(self, states, actions, deltas):\n",
    "        for t in range(len(states)):\n",
    "            gradient_log_pi = self.gradient_log_pi(states[t], actions[t])\n",
    "            # Update each parameter\n",
    "            for i in range(len(self.theta)):\n",
    "                self.theta[i] += self.alpha * deltas[t] * gradient_log_pi[i]\n",
    "    \n",
    "    def get_probability(self, state, action):\n",
    "        # Calculate y as the linearly weitht product of the\n",
    "        # policy parameters (theta) and the state\n",
    "        y = self.dot_product(state, self.theta)\n",
    "        \n",
    "        # Pass y through the logistic regression function to convert it to a probability\n",
    "        probability = self.logistic_function(y)\n",
    "        \n",
    "        if action == self.actions[0]:\n",
    "            return probability\n",
    "        return 1 - probability\n",
    "    \n",
    "    \"\"\" Computes the gradient of the log of the policy (pi),\n",
    "    which is needed to get the gradient of objective (J).\n",
    "    Because the policy is a logistic regerssion, using the policy papameters (theta).\n",
    "        pi(actions[0] | state)= 1 / (1 + e^(-theta * state))\n",
    "        pi(actions[1] | state) = 1 / (1 + e^(theta * state))\n",
    "    When we apply a logarithmic transformation and take the gradient we end up with:\n",
    "        grad_log_pi(left | state) = state - state * pi(left | state)\n",
    "        grad_log_pi(right | state) = - state * pi(right | state)\n",
    "    \"\"\"\n",
    "    \n",
    "    def gradient_log_pi(self, state, action):\n",
    "        y = self.dot_product(state, self.theta)\n",
    "        if action == self. actions[0]:\n",
    "            return [s_i - s_i * self.logistic_function(y) for s_i in state]\n",
    "        return [-s_i * self.logistic_function(y) for s_i in state]\n",
    "    \n",
    "    @staticmethod\n",
    "    def logistic_function(y):\n",
    "        return 1 / (1 + math.exp(-y))\n",
    "    \n",
    "    @staticmethod\n",
    "    def dot_product(vec1, vec2):\n",
    "        return sum([v1 * v2 for v1, v2 in zip(vec1, vec2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gridworld = GridWorld(\n",
    "    height=1, width=11, initial_state=(5, 0), goals=[((0, 0), -1), ((10, 0), 1)]\n",
    ")\n",
    "gridworld_image = gridworld.visualise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy = LogisticRegressionPolicy(\n",
    "    actions=[GridWorld.LEFT, GridWorld.RIGHT],\n",
    "    num_params=len(gridworld.get_initial_state()),\n",
    ")\n",
    "PolicyGradient(gridworld, policy).execute(episodes=100)\n",
    "policy_image = gridworld.visualise_stochastic_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetworkPolicy(StochasticPolicy):\n",
    "    \"\"\"\n",
    "    An implementation of a policy that uses a PyTorch deep neural network \n",
    "    to represent the underlying policy.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_space, action_space, hidden_dim=64, alpha=0.001, stochastic=True):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.temperature = 6.0\n",
    "        \n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(in_features=self.state_space, out_features=hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=self.action_space)\n",
    "        )\n",
    "        \n",
    "        self.optimiser = Adam(self.policy_network.parameters(), lr=alpha)\n",
    "        self.stochastic = stochastic\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for layer in self.policy_network:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "        \n",
    "        last_layer = self.policy_network[-1]\n",
    "        if isinstance():\n",
    "            with torch.no_grad():\n",
    "                last_layer.weight.fill_(0)\n",
    "                last_layer.bias.fill_(0)\n",
    "    \n",
    "    def select_action(self, state, actions):\n",
    "        state = torch.as_tensor(state, dtype=torch.float32)\n",
    "        action_logits = self.policy_network(state)\n",
    "        mask = torch.full_like(action_logits, float('-inf'))\n",
    "        mask[actions] = 0\n",
    "        masked_logits = action_logits + mask\n",
    "\n",
    "        action_distribution = Categorical(logits=masked_logits)\n",
    "        if self.stochastic:\n",
    "            # Sample an action according to the probabolity distribution\n",
    "            action = action_distribution.sample()\n",
    "\n",
    "    \n",
    "    def get_probability(self, state, action):\n",
    "        pass\n",
    "    \n",
    "    def evaluate_actions(self, states, actions):\n",
    "        pass\n",
    "    \n",
    "    def update(self, states, actions, deltas):\n",
    "        pass\n",
    "    \n",
    "    def save(self, filename):\n",
    "        torch.save(self.policy_network.state_dict(), filename)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, state_space, action_space, filename):\n",
    "        poliocy = cls(state_space, action_space)\n",
    "        policy.policy_networl.load_state_dict(torch.load(filename))\n",
    "        return policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
